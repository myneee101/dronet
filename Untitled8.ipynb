{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e343635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier as ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e420cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('processed.cleveland.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2474d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e028833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:13]\n",
    "y = array[:,13]\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16eb5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train_scaled = scale.transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "666b626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "518b0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f88660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "        'act' : {'relu', 'logistic', 'tanh'},\n",
    "        'lr' : {0.0001,0.001,0.01,.1},\n",
    "        'max_iter' : {200,500,1000}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de243d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82621843\n",
      "Iteration 2, loss = 1.63191769\n",
      "Iteration 3, loss = 1.47991491\n",
      "Iteration 4, loss = 1.38043711\n",
      "Iteration 5, loss = 1.33419624\n",
      "Iteration 6, loss = 1.30847119\n",
      "Iteration 7, loss = 1.29306312\n",
      "Iteration 8, loss = 1.28376615\n",
      "Iteration 9, loss = 1.27764840\n",
      "Iteration 10, loss = 1.27370983\n",
      "Iteration 11, loss = 1.27086675\n",
      "Iteration 12, loss = 1.27053291\n",
      "Iteration 13, loss = 1.26873323\n",
      "Iteration 14, loss = 1.26854377\n",
      "Iteration 15, loss = 1.26707453\n",
      "Iteration 16, loss = 1.26575788\n",
      "Iteration 17, loss = 1.26485640\n",
      "Iteration 18, loss = 1.26461844\n",
      "Iteration 19, loss = 1.26417751\n",
      "Iteration 20, loss = 1.26389705\n",
      "Iteration 21, loss = 1.26404215\n",
      "Iteration 22, loss = 1.26360609\n",
      "Iteration 23, loss = 1.26319799\n",
      "Iteration 24, loss = 1.26377577\n",
      "Iteration 25, loss = 1.26421771\n",
      "Iteration 26, loss = 1.26441745\n",
      "Iteration 27, loss = 1.26492769\n",
      "Iteration 28, loss = 1.26496398\n",
      "Iteration 29, loss = 1.26325245\n",
      "Iteration 30, loss = 1.26275293\n",
      "Iteration 31, loss = 1.26432131\n",
      "Iteration 32, loss = 1.26363316\n",
      "Iteration 33, loss = 1.26274686\n",
      "Iteration 34, loss = 1.26325262\n",
      "Iteration 35, loss = 1.26557386\n",
      "Iteration 36, loss = 1.26642077\n",
      "Iteration 37, loss = 1.26529815\n",
      "Iteration 38, loss = 1.26478237\n",
      "Iteration 39, loss = 1.26435851\n",
      "Iteration 40, loss = 1.26495486\n",
      "Iteration 41, loss = 1.26436922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.67107720\n",
      "Iteration 2, loss = 1.66726955\n",
      "Iteration 3, loss = 1.66368757\n",
      "Iteration 4, loss = 1.66025698\n",
      "Iteration 5, loss = 1.65717479\n",
      "Iteration 6, loss = 1.65344065\n",
      "Iteration 7, loss = 1.64995832\n",
      "Iteration 8, loss = 1.64624568\n",
      "Iteration 9, loss = 1.64204153\n",
      "Iteration 10, loss = 1.63736290\n",
      "Iteration 11, loss = 1.63325224\n",
      "Iteration 12, loss = 1.62841426\n",
      "Iteration 13, loss = 1.62302642\n",
      "Iteration 14, loss = 1.61940246\n",
      "Iteration 15, loss = 1.61526935\n",
      "Iteration 16, loss = 1.61154053\n",
      "Iteration 17, loss = 1.60817203\n",
      "Iteration 18, loss = 1.60479820\n",
      "Iteration 19, loss = 1.60185725\n",
      "Iteration 20, loss = 1.59882795\n",
      "Iteration 21, loss = 1.59560765\n",
      "Iteration 22, loss = 1.59245877\n",
      "Iteration 23, loss = 1.58981397\n",
      "Iteration 24, loss = 1.58695680\n",
      "Iteration 25, loss = 1.58394558\n",
      "Iteration 26, loss = 1.58087232\n",
      "Iteration 27, loss = 1.57817008\n",
      "Iteration 28, loss = 1.57535566\n",
      "Iteration 29, loss = 1.57316190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 1.57009599\n",
      "Iteration 31, loss = 1.56736265\n",
      "Iteration 32, loss = 1.56453602\n",
      "Iteration 33, loss = 1.56200893\n",
      "Iteration 34, loss = 1.55932244\n",
      "Iteration 35, loss = 1.55699872\n",
      "Iteration 36, loss = 1.55421569\n",
      "Iteration 37, loss = 1.55162243\n",
      "Iteration 38, loss = 1.54938508\n",
      "Iteration 39, loss = 1.54671665\n",
      "Iteration 40, loss = 1.54377497\n",
      "Iteration 41, loss = 1.54100024\n",
      "Iteration 42, loss = 1.53842807\n",
      "Iteration 43, loss = 1.53557304\n",
      "Iteration 44, loss = 1.53158530\n",
      "Iteration 45, loss = 1.52887575\n",
      "Iteration 46, loss = 1.52586527\n",
      "Iteration 47, loss = 1.52331310\n",
      "Iteration 48, loss = 1.52057954\n",
      "Iteration 49, loss = 1.51824842\n",
      "Iteration 50, loss = 1.51600723\n",
      "Iteration 51, loss = 1.51377354\n",
      "Iteration 52, loss = 1.51161681\n",
      "Iteration 53, loss = 1.50926658\n",
      "Iteration 54, loss = 1.50737487\n",
      "Iteration 55, loss = 1.50511226\n",
      "Iteration 56, loss = 1.50312101\n",
      "Iteration 57, loss = 1.50101603\n",
      "Iteration 58, loss = 1.49880407\n",
      "Iteration 59, loss = 1.49690578\n",
      "Iteration 60, loss = 1.49492852\n",
      "Iteration 61, loss = 1.49282596\n",
      "Iteration 62, loss = 1.49099294\n",
      "Iteration 63, loss = 1.48895453\n",
      "Iteration 64, loss = 1.48696082\n",
      "Iteration 65, loss = 1.48510218\n",
      "Iteration 66, loss = 1.48293747\n",
      "Iteration 67, loss = 1.48082952\n",
      "Iteration 68, loss = 1.47903970\n",
      "Iteration 69, loss = 1.47763405\n",
      "Iteration 70, loss = 1.47526545\n",
      "Iteration 71, loss = 1.47335813\n",
      "Iteration 72, loss = 1.47136955\n",
      "Iteration 73, loss = 1.46940462\n",
      "Iteration 74, loss = 1.46776320\n",
      "Iteration 75, loss = 1.46569036\n",
      "Iteration 76, loss = 1.46394260\n",
      "Iteration 77, loss = 1.46200458\n",
      "Iteration 78, loss = 1.46022035\n",
      "Iteration 79, loss = 1.45845583\n",
      "Iteration 80, loss = 1.45672125\n",
      "Iteration 81, loss = 1.45492421\n",
      "Iteration 82, loss = 1.45290642\n",
      "Iteration 83, loss = 1.45113353\n",
      "Iteration 84, loss = 1.44959566\n",
      "Iteration 85, loss = 1.44752410\n",
      "Iteration 86, loss = 1.44597943\n",
      "Iteration 87, loss = 1.44422008\n",
      "Iteration 88, loss = 1.44244330\n",
      "Iteration 89, loss = 1.44067824\n",
      "Iteration 90, loss = 1.43889258\n",
      "Iteration 91, loss = 1.43710464\n",
      "Iteration 92, loss = 1.43561796\n",
      "Iteration 93, loss = 1.43372317\n",
      "Iteration 94, loss = 1.43211922\n",
      "Iteration 95, loss = 1.43011584\n",
      "Iteration 96, loss = 1.42855918\n",
      "Iteration 97, loss = 1.42669114\n",
      "Iteration 98, loss = 1.42495333\n",
      "Iteration 99, loss = 1.42335352\n",
      "Iteration 100, loss = 1.42170077\n",
      "Iteration 101, loss = 1.42003501\n",
      "Iteration 102, loss = 1.41870271\n",
      "Iteration 103, loss = 1.41702379\n",
      "Iteration 104, loss = 1.41545018\n",
      "Iteration 105, loss = 1.41388704\n",
      "Iteration 106, loss = 1.41223871\n",
      "Iteration 107, loss = 1.41069824\n",
      "Iteration 108, loss = 1.40902977\n",
      "Iteration 109, loss = 1.40750430\n",
      "Iteration 110, loss = 1.40603868\n",
      "Iteration 111, loss = 1.40446631\n",
      "Iteration 112, loss = 1.40318050\n",
      "Iteration 113, loss = 1.40136132\n",
      "Iteration 114, loss = 1.39971824\n",
      "Iteration 115, loss = 1.39820230\n",
      "Iteration 116, loss = 1.39680038\n",
      "Iteration 117, loss = 1.39517959\n",
      "Iteration 118, loss = 1.39408787\n",
      "Iteration 119, loss = 1.39244187\n",
      "Iteration 120, loss = 1.39112706\n",
      "Iteration 121, loss = 1.38939870\n",
      "Iteration 122, loss = 1.38793077\n",
      "Iteration 123, loss = 1.38668811\n",
      "Iteration 124, loss = 1.38505075\n",
      "Iteration 125, loss = 1.38370195\n",
      "Iteration 126, loss = 1.38207411\n",
      "Iteration 127, loss = 1.38072072\n",
      "Iteration 128, loss = 1.37940736\n",
      "Iteration 129, loss = 1.37814279\n",
      "Iteration 130, loss = 1.37648624\n",
      "Iteration 131, loss = 1.37521326\n",
      "Iteration 132, loss = 1.37373220\n",
      "Iteration 133, loss = 1.37243603\n",
      "Iteration 134, loss = 1.37123914\n",
      "Iteration 135, loss = 1.36999749\n",
      "Iteration 136, loss = 1.36879805\n",
      "Iteration 137, loss = 1.36743337\n",
      "Iteration 138, loss = 1.36633176\n",
      "Iteration 139, loss = 1.36485636\n",
      "Iteration 140, loss = 1.36379334\n",
      "Iteration 141, loss = 1.36262384\n",
      "Iteration 142, loss = 1.36110279\n",
      "Iteration 143, loss = 1.35985244\n",
      "Iteration 144, loss = 1.35865443\n",
      "Iteration 145, loss = 1.35739897\n",
      "Iteration 146, loss = 1.35618012\n",
      "Iteration 147, loss = 1.35491702\n",
      "Iteration 148, loss = 1.35379691\n",
      "Iteration 149, loss = 1.35269129\n",
      "Iteration 150, loss = 1.35131598\n",
      "Iteration 151, loss = 1.35028030\n",
      "Iteration 152, loss = 1.34915464\n",
      "Iteration 153, loss = 1.34817725\n",
      "Iteration 154, loss = 1.34704729\n",
      "Iteration 155, loss = 1.34587293\n",
      "Iteration 156, loss = 1.34474350\n",
      "Iteration 157, loss = 1.34348447\n",
      "Iteration 158, loss = 1.34264349\n",
      "Iteration 159, loss = 1.34146843\n",
      "Iteration 160, loss = 1.34026240\n",
      "Iteration 161, loss = 1.33915642\n",
      "Iteration 162, loss = 1.33840452\n",
      "Iteration 163, loss = 1.33686471\n",
      "Iteration 164, loss = 1.33578514\n",
      "Iteration 165, loss = 1.33478447\n",
      "Iteration 166, loss = 1.33363152\n",
      "Iteration 167, loss = 1.33251929\n",
      "Iteration 168, loss = 1.33150020\n",
      "Iteration 169, loss = 1.33023787\n",
      "Iteration 170, loss = 1.32907190\n",
      "Iteration 171, loss = 1.32812005\n",
      "Iteration 172, loss = 1.32707772\n",
      "Iteration 173, loss = 1.32596267\n",
      "Iteration 174, loss = 1.32486225\n",
      "Iteration 175, loss = 1.32392051\n",
      "Iteration 176, loss = 1.32263111\n",
      "Iteration 177, loss = 1.32168936\n",
      "Iteration 178, loss = 1.32062155\n",
      "Iteration 179, loss = 1.31960665\n",
      "Iteration 180, loss = 1.31873155\n",
      "Iteration 181, loss = 1.31738414\n",
      "Iteration 182, loss = 1.31642195\n",
      "Iteration 183, loss = 1.31596144\n",
      "Iteration 184, loss = 1.31472226\n",
      "Iteration 185, loss = 1.31367788\n",
      "Iteration 186, loss = 1.31261790\n",
      "Iteration 187, loss = 1.31186305\n",
      "Iteration 188, loss = 1.31074950\n",
      "Iteration 189, loss = 1.30990846\n",
      "Iteration 190, loss = 1.30909481\n",
      "Iteration 191, loss = 1.30809046\n",
      "Iteration 192, loss = 1.30751507\n",
      "Iteration 193, loss = 1.30659935\n",
      "Iteration 194, loss = 1.30523653\n",
      "Iteration 195, loss = 1.30436667\n",
      "Iteration 196, loss = 1.30339651\n",
      "Iteration 197, loss = 1.30258300\n",
      "Iteration 198, loss = 1.30157442\n",
      "Iteration 199, loss = 1.30066127\n",
      "Iteration 200, loss = 1.29963169\n",
      "Iteration 1, loss = 1.44910578\n",
      "Iteration 2, loss = 1.26233857\n",
      "Iteration 3, loss = 1.30652549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 1.28958677\n",
      "Iteration 5, loss = 1.27590172\n",
      "Iteration 6, loss = 1.26532902\n",
      "Iteration 7, loss = 1.26385387\n",
      "Iteration 8, loss = 1.27838429\n",
      "Iteration 9, loss = 1.26405747\n",
      "Iteration 10, loss = 1.29265192\n",
      "Iteration 11, loss = 1.29403464\n",
      "Iteration 12, loss = 1.37582566\n",
      "Iteration 13, loss = 1.31321935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28698786\n",
      "Iteration 2, loss = 2.27861100\n",
      "Iteration 3, loss = 2.27193378\n",
      "Iteration 4, loss = 2.26449375\n",
      "Iteration 5, loss = 2.25857311\n",
      "Iteration 6, loss = 2.25191677\n",
      "Iteration 7, loss = 2.24633487\n",
      "Iteration 8, loss = 2.23997140\n",
      "Iteration 9, loss = 2.23396166\n",
      "Iteration 10, loss = 2.22822183\n",
      "Iteration 11, loss = 2.22222366\n",
      "Iteration 12, loss = 2.21600434\n",
      "Iteration 13, loss = 2.21026864\n",
      "Iteration 14, loss = 2.20391819\n",
      "Iteration 15, loss = 2.19766998\n",
      "Iteration 16, loss = 2.19126530\n",
      "Iteration 17, loss = 2.18459393\n",
      "Iteration 18, loss = 2.17772516\n",
      "Iteration 19, loss = 2.17087102\n",
      "Iteration 20, loss = 2.16234913\n",
      "Iteration 21, loss = 2.15440892\n",
      "Iteration 22, loss = 2.14528466\n",
      "Iteration 23, loss = 2.13501047\n",
      "Iteration 24, loss = 2.12446941\n",
      "Iteration 25, loss = 2.10841832\n",
      "Iteration 26, loss = 2.09465076\n",
      "Iteration 27, loss = 2.07778283\n",
      "Iteration 28, loss = 2.06677312\n",
      "Iteration 29, loss = 2.05794800\n",
      "Iteration 30, loss = 2.05058936\n",
      "Iteration 31, loss = 2.04358556\n",
      "Iteration 32, loss = 2.03689312\n",
      "Iteration 33, loss = 2.03067894\n",
      "Iteration 34, loss = 2.02412562\n",
      "Iteration 35, loss = 2.01815059\n",
      "Iteration 36, loss = 2.01209133\n",
      "Iteration 37, loss = 2.00512808\n",
      "Iteration 38, loss = 1.99797883\n",
      "Iteration 39, loss = 1.98979928\n",
      "Iteration 40, loss = 1.98108257\n",
      "Iteration 41, loss = 1.97279258\n",
      "Iteration 42, loss = 1.96495648\n",
      "Iteration 43, loss = 1.95711388\n",
      "Iteration 44, loss = 1.94902188\n",
      "Iteration 45, loss = 1.94124404\n",
      "Iteration 46, loss = 1.93403424\n",
      "Iteration 47, loss = 1.92530746\n",
      "Iteration 48, loss = 1.91811599\n",
      "Iteration 49, loss = 1.91089848\n",
      "Iteration 50, loss = 1.90353440\n",
      "Iteration 51, loss = 1.89697680\n",
      "Iteration 52, loss = 1.88992174\n",
      "Iteration 53, loss = 1.88351088\n",
      "Iteration 54, loss = 1.87726459\n",
      "Iteration 55, loss = 1.87040873\n",
      "Iteration 56, loss = 1.86444308\n",
      "Iteration 57, loss = 1.85751064\n",
      "Iteration 58, loss = 1.85162977\n",
      "Iteration 59, loss = 1.84560025\n",
      "Iteration 60, loss = 1.83971579\n",
      "Iteration 61, loss = 1.83322370\n",
      "Iteration 62, loss = 1.82761376\n",
      "Iteration 63, loss = 1.82163197\n",
      "Iteration 64, loss = 1.81592506\n",
      "Iteration 65, loss = 1.81036192\n",
      "Iteration 66, loss = 1.80468324\n",
      "Iteration 67, loss = 1.79879559\n",
      "Iteration 68, loss = 1.79339175\n",
      "Iteration 69, loss = 1.78743245\n",
      "Iteration 70, loss = 1.78172377\n",
      "Iteration 71, loss = 1.77541884\n",
      "Iteration 72, loss = 1.76868428\n",
      "Iteration 73, loss = 1.76280933\n",
      "Iteration 74, loss = 1.75771378\n",
      "Iteration 75, loss = 1.75205592\n",
      "Iteration 76, loss = 1.74685670\n",
      "Iteration 77, loss = 1.74197416\n",
      "Iteration 78, loss = 1.73716139\n",
      "Iteration 79, loss = 1.73197291\n",
      "Iteration 80, loss = 1.72700568\n",
      "Iteration 81, loss = 1.72207322\n",
      "Iteration 82, loss = 1.71710355\n",
      "Iteration 83, loss = 1.71254329\n",
      "Iteration 84, loss = 1.70735731\n",
      "Iteration 85, loss = 1.70297256\n",
      "Iteration 86, loss = 1.69818251\n",
      "Iteration 87, loss = 1.69343863\n",
      "Iteration 88, loss = 1.68858900\n",
      "Iteration 89, loss = 1.68378449\n",
      "Iteration 90, loss = 1.67900237\n",
      "Iteration 91, loss = 1.67473653\n",
      "Iteration 92, loss = 1.67012169\n",
      "Iteration 93, loss = 1.66545630\n",
      "Iteration 94, loss = 1.66087854\n",
      "Iteration 95, loss = 1.65639269\n",
      "Iteration 96, loss = 1.65194566\n",
      "Iteration 97, loss = 1.64729506\n",
      "Iteration 98, loss = 1.64318044\n",
      "Iteration 99, loss = 1.63834916\n",
      "Iteration 100, loss = 1.63400402\n",
      "Iteration 101, loss = 1.62946440\n",
      "Iteration 102, loss = 1.62487604\n",
      "Iteration 103, loss = 1.62083779\n",
      "Iteration 104, loss = 1.61656783\n",
      "Iteration 105, loss = 1.61273637\n",
      "Iteration 106, loss = 1.60906136\n",
      "Iteration 107, loss = 1.60499898\n",
      "Iteration 108, loss = 1.60148470\n",
      "Iteration 109, loss = 1.59721948\n",
      "Iteration 110, loss = 1.59352920\n",
      "Iteration 111, loss = 1.58993228\n",
      "Iteration 112, loss = 1.58634839\n",
      "Iteration 113, loss = 1.58231305\n",
      "Iteration 114, loss = 1.57872882\n",
      "Iteration 115, loss = 1.57519617\n",
      "Iteration 116, loss = 1.57165776\n",
      "Iteration 117, loss = 1.56804763\n",
      "Iteration 118, loss = 1.56407531\n",
      "Iteration 119, loss = 1.55975997\n",
      "Iteration 120, loss = 1.55409432\n",
      "Iteration 121, loss = 1.54736627\n",
      "Iteration 122, loss = 1.54081009\n",
      "Iteration 123, loss = 1.53591096\n",
      "Iteration 124, loss = 1.53031626\n",
      "Iteration 125, loss = 1.52671160\n",
      "Iteration 126, loss = 1.52290341\n",
      "Iteration 127, loss = 1.51971465\n",
      "Iteration 128, loss = 1.51617965\n",
      "Iteration 129, loss = 1.51335758\n",
      "Iteration 130, loss = 1.51014896\n",
      "Iteration 131, loss = 1.50689978\n",
      "Iteration 132, loss = 1.50380810\n",
      "Iteration 133, loss = 1.50055325\n",
      "Iteration 134, loss = 1.49707322\n",
      "Iteration 135, loss = 1.49404881\n",
      "Iteration 136, loss = 1.49104414\n",
      "Iteration 137, loss = 1.48850370\n",
      "Iteration 138, loss = 1.48538861\n",
      "Iteration 139, loss = 1.48270445\n",
      "Iteration 140, loss = 1.48005360\n",
      "Iteration 141, loss = 1.47742272\n",
      "Iteration 142, loss = 1.47481461\n",
      "Iteration 143, loss = 1.47192219\n",
      "Iteration 144, loss = 1.46933233\n",
      "Iteration 145, loss = 1.46659238\n",
      "Iteration 146, loss = 1.46393860\n",
      "Iteration 147, loss = 1.46137763\n",
      "Iteration 148, loss = 1.45904013\n",
      "Iteration 149, loss = 1.45622882\n",
      "Iteration 150, loss = 1.45405620\n",
      "Iteration 151, loss = 1.45134637\n",
      "Iteration 152, loss = 1.44932866\n",
      "Iteration 153, loss = 1.44670304\n",
      "Iteration 154, loss = 1.44440665\n",
      "Iteration 155, loss = 1.44227121\n",
      "Iteration 156, loss = 1.43985527\n",
      "Iteration 157, loss = 1.43785767\n",
      "Iteration 158, loss = 1.43536666\n",
      "Iteration 159, loss = 1.43309642\n",
      "Iteration 160, loss = 1.43095070\n",
      "Iteration 161, loss = 1.42876905\n",
      "Iteration 162, loss = 1.42665116\n",
      "Iteration 163, loss = 1.42453645\n",
      "Iteration 164, loss = 1.42261304\n",
      "Iteration 165, loss = 1.42048489\n",
      "Iteration 166, loss = 1.41821182\n",
      "Iteration 167, loss = 1.41661596\n",
      "Iteration 168, loss = 1.41462689\n",
      "Iteration 169, loss = 1.41270121\n",
      "Iteration 170, loss = 1.41060341\n",
      "Iteration 171, loss = 1.40900678\n",
      "Iteration 172, loss = 1.40685662\n",
      "Iteration 173, loss = 1.40506312\n",
      "Iteration 174, loss = 1.40333929\n",
      "Iteration 175, loss = 1.40122767\n",
      "Iteration 176, loss = 1.39949069\n",
      "Iteration 177, loss = 1.39795468\n",
      "Iteration 178, loss = 1.39600232\n",
      "Iteration 179, loss = 1.39437616\n",
      "Iteration 180, loss = 1.39267550\n",
      "Iteration 181, loss = 1.39104544\n",
      "Iteration 182, loss = 1.38942583\n",
      "Iteration 183, loss = 1.38806874\n",
      "Iteration 184, loss = 1.38626741\n",
      "Iteration 185, loss = 1.38473060\n",
      "Iteration 186, loss = 1.38311127\n",
      "Iteration 187, loss = 1.38159754\n",
      "Iteration 188, loss = 1.37998157\n",
      "Iteration 189, loss = 1.37844871\n",
      "Iteration 190, loss = 1.37666047\n",
      "Iteration 191, loss = 1.37522057\n",
      "Iteration 192, loss = 1.37375953\n",
      "Iteration 193, loss = 1.37233185\n",
      "Iteration 194, loss = 1.37073909\n",
      "Iteration 195, loss = 1.36937777\n",
      "Iteration 196, loss = 1.36805780\n",
      "Iteration 197, loss = 1.36665756\n",
      "Iteration 198, loss = 1.36536999\n",
      "Iteration 199, loss = 1.36410415\n",
      "Iteration 200, loss = 1.36268359\n",
      "Iteration 1, loss = 1.57770331\n",
      "Iteration 2, loss = 1.46018101\n",
      "Iteration 3, loss = 1.33293784\n",
      "Iteration 4, loss = 1.30900570\n",
      "Iteration 5, loss = 1.28522190\n",
      "Iteration 6, loss = 1.31256434\n",
      "Iteration 7, loss = 1.30935577\n",
      "Iteration 8, loss = 1.30448562\n",
      "Iteration 9, loss = 1.31740493\n",
      "Iteration 10, loss = 1.31469348\n",
      "Iteration 11, loss = 1.31999085\n",
      "Iteration 12, loss = 1.29898293\n",
      "Iteration 13, loss = 1.34339516\n",
      "Iteration 14, loss = 1.30248024\n",
      "Iteration 15, loss = 1.27328467\n",
      "Iteration 16, loss = 1.30877574\n",
      "Iteration 17, loss = 1.30218826\n",
      "Iteration 18, loss = 1.30521174\n",
      "Iteration 19, loss = 1.34730467\n",
      "Iteration 20, loss = 1.25500463\n",
      "Iteration 21, loss = 1.35709360\n",
      "Iteration 22, loss = 1.27464360\n",
      "Iteration 23, loss = 1.28403397\n",
      "Iteration 24, loss = 1.30090375\n",
      "Iteration 25, loss = 1.31504778\n",
      "Iteration 26, loss = 1.31200791\n",
      "Iteration 27, loss = 1.33249768\n",
      "Iteration 28, loss = 1.28967675\n",
      "Iteration 29, loss = 1.35661916\n",
      "Iteration 30, loss = 1.31573069\n",
      "Iteration 31, loss = 1.31659274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41535854\n",
      "Iteration 2, loss = 1.41387004\n",
      "Iteration 3, loss = 1.41190022\n",
      "Iteration 4, loss = 1.41036316\n",
      "Iteration 5, loss = 1.40886395\n",
      "Iteration 6, loss = 1.40721491\n",
      "Iteration 7, loss = 1.40574213\n",
      "Iteration 8, loss = 1.40413547\n",
      "Iteration 9, loss = 1.40256775\n",
      "Iteration 10, loss = 1.40108604\n",
      "Iteration 11, loss = 1.39956628\n",
      "Iteration 12, loss = 1.39781058\n",
      "Iteration 13, loss = 1.39630893\n",
      "Iteration 14, loss = 1.39455329\n",
      "Iteration 15, loss = 1.39308628\n",
      "Iteration 16, loss = 1.39157113\n",
      "Iteration 17, loss = 1.39011579\n",
      "Iteration 18, loss = 1.38879561\n",
      "Iteration 19, loss = 1.38751674\n",
      "Iteration 20, loss = 1.38598232\n",
      "Iteration 21, loss = 1.38474064\n",
      "Iteration 22, loss = 1.38370539\n",
      "Iteration 23, loss = 1.38234377\n",
      "Iteration 24, loss = 1.38132789\n",
      "Iteration 25, loss = 1.38002942\n",
      "Iteration 26, loss = 1.37881903\n",
      "Iteration 27, loss = 1.37780208\n",
      "Iteration 28, loss = 1.37679237\n",
      "Iteration 29, loss = 1.37541644\n",
      "Iteration 30, loss = 1.37433238\n",
      "Iteration 31, loss = 1.37339542\n",
      "Iteration 32, loss = 1.37225231\n",
      "Iteration 33, loss = 1.37106129\n",
      "Iteration 34, loss = 1.37010319\n",
      "Iteration 35, loss = 1.36915477\n",
      "Iteration 36, loss = 1.36784333\n",
      "Iteration 37, loss = 1.36697200\n",
      "Iteration 38, loss = 1.36604212\n",
      "Iteration 39, loss = 1.36503464\n",
      "Iteration 40, loss = 1.36393701\n",
      "Iteration 41, loss = 1.36304339\n",
      "Iteration 42, loss = 1.36199571\n",
      "Iteration 43, loss = 1.36091838\n",
      "Iteration 44, loss = 1.35991941\n",
      "Iteration 45, loss = 1.35900316\n",
      "Iteration 46, loss = 1.35812100\n",
      "Iteration 47, loss = 1.35692932\n",
      "Iteration 48, loss = 1.35613556\n",
      "Iteration 49, loss = 1.35523484\n",
      "Iteration 50, loss = 1.35435476\n",
      "Iteration 51, loss = 1.35337254\n",
      "Iteration 52, loss = 1.35257577\n",
      "Iteration 53, loss = 1.35170487\n",
      "Iteration 54, loss = 1.35068503\n",
      "Iteration 55, loss = 1.34974001\n",
      "Iteration 56, loss = 1.34906803\n",
      "Iteration 57, loss = 1.34828436\n",
      "Iteration 58, loss = 1.34716015\n",
      "Iteration 59, loss = 1.34643055\n",
      "Iteration 60, loss = 1.34559428\n",
      "Iteration 61, loss = 1.34478245\n",
      "Iteration 62, loss = 1.34380182\n",
      "Iteration 63, loss = 1.34300608\n",
      "Iteration 64, loss = 1.34223076\n",
      "Iteration 65, loss = 1.34133393\n",
      "Iteration 66, loss = 1.34050164\n",
      "Iteration 67, loss = 1.33979703\n",
      "Iteration 68, loss = 1.33919159\n",
      "Iteration 69, loss = 1.33815069\n",
      "Iteration 70, loss = 1.33736371\n",
      "Iteration 71, loss = 1.33661595\n",
      "Iteration 72, loss = 1.33586107\n",
      "Iteration 73, loss = 1.33511901\n",
      "Iteration 74, loss = 1.33431776\n",
      "Iteration 75, loss = 1.33357158\n",
      "Iteration 76, loss = 1.33272046\n",
      "Iteration 77, loss = 1.33223027\n",
      "Iteration 78, loss = 1.33141386\n",
      "Iteration 79, loss = 1.33069939\n",
      "Iteration 80, loss = 1.32987953\n",
      "Iteration 81, loss = 1.32920289\n",
      "Iteration 82, loss = 1.32845687\n",
      "Iteration 83, loss = 1.32772664\n",
      "Iteration 84, loss = 1.32701570\n",
      "Iteration 85, loss = 1.32615660\n",
      "Iteration 86, loss = 1.32545316\n",
      "Iteration 87, loss = 1.32467272\n",
      "Iteration 88, loss = 1.32390797\n",
      "Iteration 89, loss = 1.32315486\n",
      "Iteration 90, loss = 1.32255901\n",
      "Iteration 91, loss = 1.32178910\n",
      "Iteration 92, loss = 1.32121318\n",
      "Iteration 93, loss = 1.32041722\n",
      "Iteration 94, loss = 1.31965070\n",
      "Iteration 95, loss = 1.31906215\n",
      "Iteration 96, loss = 1.31823214\n",
      "Iteration 97, loss = 1.31760686\n",
      "Iteration 98, loss = 1.31694799\n",
      "Iteration 99, loss = 1.31608588\n",
      "Iteration 100, loss = 1.31550909\n",
      "Iteration 101, loss = 1.31476442\n",
      "Iteration 102, loss = 1.31409788\n",
      "Iteration 103, loss = 1.31332713\n",
      "Iteration 104, loss = 1.31266596\n",
      "Iteration 105, loss = 1.31192806\n",
      "Iteration 106, loss = 1.31125755\n",
      "Iteration 107, loss = 1.31065938\n",
      "Iteration 108, loss = 1.30987269\n",
      "Iteration 109, loss = 1.30910445\n",
      "Iteration 110, loss = 1.30869066\n",
      "Iteration 111, loss = 1.30601709\n",
      "Iteration 112, loss = 1.30537369\n",
      "Iteration 113, loss = 1.30335982\n",
      "Iteration 114, loss = 1.30016082\n",
      "Iteration 115, loss = 1.30366783\n",
      "Iteration 116, loss = 1.29550831\n",
      "Iteration 117, loss = 1.29646450\n",
      "Iteration 118, loss = 1.29419375\n",
      "Iteration 119, loss = 1.29393986\n",
      "Iteration 120, loss = 1.29337267\n",
      "Iteration 121, loss = 1.29052465\n",
      "Iteration 122, loss = 1.28996693\n",
      "Iteration 123, loss = 1.28877964\n",
      "Iteration 124, loss = 1.28766447\n",
      "Iteration 125, loss = 1.28679102\n",
      "Iteration 126, loss = 1.28732492\n",
      "Iteration 127, loss = 1.28435232\n",
      "Iteration 128, loss = 1.28301048\n",
      "Iteration 129, loss = 1.28206851\n",
      "Iteration 130, loss = 1.28078662\n",
      "Iteration 131, loss = 1.28023566\n",
      "Iteration 132, loss = 1.27894506\n",
      "Iteration 133, loss = 1.27889515\n",
      "Iteration 134, loss = 1.27849706\n",
      "Iteration 135, loss = 1.27550044\n",
      "Iteration 136, loss = 1.27423847\n",
      "Iteration 137, loss = 1.27302028\n",
      "Iteration 138, loss = 1.27196675\n",
      "Iteration 139, loss = 1.27156092\n",
      "Iteration 140, loss = 1.26920307\n",
      "Iteration 141, loss = 1.26894318\n",
      "Iteration 142, loss = 1.26802815\n",
      "Iteration 143, loss = 1.26656518\n",
      "Iteration 144, loss = 1.26586322\n",
      "Iteration 145, loss = 1.26536462\n",
      "Iteration 146, loss = 1.26381115\n",
      "Iteration 147, loss = 1.26258772\n",
      "Iteration 148, loss = 1.26180376\n",
      "Iteration 149, loss = 1.26039670\n",
      "Iteration 150, loss = 1.25896568\n",
      "Iteration 151, loss = 1.25835440\n",
      "Iteration 152, loss = 1.25623350\n",
      "Iteration 153, loss = 1.25573133\n",
      "Iteration 154, loss = 1.25412176\n",
      "Iteration 155, loss = 1.25358264\n",
      "Iteration 156, loss = 1.25232841\n",
      "Iteration 157, loss = 1.25050948\n",
      "Iteration 158, loss = 1.24951147\n",
      "Iteration 159, loss = 1.24876581\n",
      "Iteration 160, loss = 1.24735399\n",
      "Iteration 161, loss = 1.24919202\n",
      "Iteration 162, loss = 1.24512704\n",
      "Iteration 163, loss = 1.24459942\n",
      "Iteration 164, loss = 1.24326977\n",
      "Iteration 165, loss = 1.24220205\n",
      "Iteration 166, loss = 1.24071437\n",
      "Iteration 167, loss = 1.23985611\n",
      "Iteration 168, loss = 1.23884766\n",
      "Iteration 169, loss = 1.23774919\n",
      "Iteration 170, loss = 1.23615025\n",
      "Iteration 171, loss = 1.23509101\n",
      "Iteration 172, loss = 1.23553932\n",
      "Iteration 173, loss = 1.23448896\n",
      "Iteration 174, loss = 1.23248346\n",
      "Iteration 175, loss = 1.23266245\n",
      "Iteration 176, loss = 1.22970641\n",
      "Iteration 177, loss = 1.22964100\n",
      "Iteration 178, loss = 1.23081936\n",
      "Iteration 179, loss = 1.22719613\n",
      "Iteration 180, loss = 1.22556778\n",
      "Iteration 181, loss = 1.22463703\n",
      "Iteration 182, loss = 1.22431393\n",
      "Iteration 183, loss = 1.22578532\n",
      "Iteration 184, loss = 1.22251811\n",
      "Iteration 185, loss = 1.22010228\n",
      "Iteration 186, loss = 1.22031130\n",
      "Iteration 187, loss = 1.21960150\n",
      "Iteration 188, loss = 1.21866178\n",
      "Iteration 189, loss = 1.21719516\n",
      "Iteration 190, loss = 1.21599061\n",
      "Iteration 191, loss = 1.21531719\n",
      "Iteration 192, loss = 1.21406646\n",
      "Iteration 193, loss = 1.21362503\n",
      "Iteration 194, loss = 1.21193138\n",
      "Iteration 195, loss = 1.21142801\n",
      "Iteration 196, loss = 1.21008094\n",
      "Iteration 197, loss = 1.20901647\n",
      "Iteration 198, loss = 1.20850039\n",
      "Iteration 199, loss = 1.20803585\n",
      "Iteration 200, loss = 1.20648850\n",
      "Iteration 1, loss = 1.53196123\n",
      "Iteration 2, loss = 1.53906268\n",
      "Iteration 3, loss = 1.37973151\n",
      "Iteration 4, loss = 1.37300290\n",
      "Iteration 5, loss = 1.34584377\n",
      "Iteration 6, loss = 1.27784402\n",
      "Iteration 7, loss = 1.32060334\n",
      "Iteration 8, loss = 1.29601113\n",
      "Iteration 9, loss = 1.29769645\n",
      "Iteration 10, loss = 1.29722144\n",
      "Iteration 11, loss = 1.28015889\n",
      "Iteration 12, loss = 1.27544374\n",
      "Iteration 13, loss = 1.32609747\n",
      "Iteration 14, loss = 1.27788536\n",
      "Iteration 15, loss = 1.32963778\n",
      "Iteration 16, loss = 1.31251634\n",
      "Iteration 17, loss = 1.28428259\n",
      "Iteration 18, loss = 1.27971302\n",
      "Iteration 19, loss = 1.31415065\n",
      "Iteration 20, loss = 1.32625562\n",
      "Iteration 21, loss = 1.34113209\n",
      "Iteration 22, loss = 1.36811321\n",
      "Iteration 23, loss = 1.33682046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41967479\n",
      "Iteration 2, loss = 1.41468688\n",
      "Iteration 3, loss = 1.41008300\n",
      "Iteration 4, loss = 1.40526141\n",
      "Iteration 5, loss = 1.40099305\n",
      "Iteration 6, loss = 1.39640655\n",
      "Iteration 7, loss = 1.39279191\n",
      "Iteration 8, loss = 1.38857848\n",
      "Iteration 9, loss = 1.38486900\n",
      "Iteration 10, loss = 1.38109942\n",
      "Iteration 11, loss = 1.37729175\n",
      "Iteration 12, loss = 1.37358795\n",
      "Iteration 13, loss = 1.37002375\n",
      "Iteration 14, loss = 1.36621222\n",
      "Iteration 15, loss = 1.36212031\n",
      "Iteration 16, loss = 1.35816701\n",
      "Iteration 17, loss = 1.35520293\n",
      "Iteration 18, loss = 1.35166122\n",
      "Iteration 19, loss = 1.34948642\n",
      "Iteration 20, loss = 1.34745145\n",
      "Iteration 21, loss = 1.34561961\n",
      "Iteration 22, loss = 1.34406127\n",
      "Iteration 23, loss = 1.34249535\n",
      "Iteration 24, loss = 1.34082113\n",
      "Iteration 25, loss = 1.33939584\n",
      "Iteration 26, loss = 1.33760240\n",
      "Iteration 27, loss = 1.33625346\n",
      "Iteration 28, loss = 1.33449112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 1.33267171\n",
      "Iteration 30, loss = 1.33102011\n",
      "Iteration 31, loss = 1.32884037\n",
      "Iteration 32, loss = 1.32629046\n",
      "Iteration 33, loss = 1.32364877\n",
      "Iteration 34, loss = 1.32112069\n",
      "Iteration 35, loss = 1.31804995\n",
      "Iteration 36, loss = 1.31582087\n",
      "Iteration 37, loss = 1.31388157\n",
      "Iteration 38, loss = 1.31262130\n",
      "Iteration 39, loss = 1.31095525\n",
      "Iteration 40, loss = 1.30970944\n",
      "Iteration 41, loss = 1.30862574\n",
      "Iteration 42, loss = 1.30730770\n",
      "Iteration 43, loss = 1.30615404\n",
      "Iteration 44, loss = 1.30512224\n",
      "Iteration 45, loss = 1.30387425\n",
      "Iteration 46, loss = 1.30286883\n",
      "Iteration 47, loss = 1.30167515\n",
      "Iteration 48, loss = 1.30082143\n",
      "Iteration 49, loss = 1.29948622\n",
      "Iteration 50, loss = 1.29857096\n",
      "Iteration 51, loss = 1.29739399\n",
      "Iteration 52, loss = 1.29654771\n",
      "Iteration 53, loss = 1.29559543\n",
      "Iteration 54, loss = 1.29461571\n",
      "Iteration 55, loss = 1.29352649\n",
      "Iteration 56, loss = 1.29262640\n",
      "Iteration 57, loss = 1.29184046\n",
      "Iteration 58, loss = 1.29090025\n",
      "Iteration 59, loss = 1.28995211\n",
      "Iteration 60, loss = 1.28903561\n",
      "Iteration 61, loss = 1.28822224\n",
      "Iteration 62, loss = 1.28707416\n",
      "Iteration 63, loss = 1.28627075\n",
      "Iteration 64, loss = 1.28546467\n",
      "Iteration 65, loss = 1.28447203\n",
      "Iteration 66, loss = 1.28366111\n",
      "Iteration 67, loss = 1.28298797\n",
      "Iteration 68, loss = 1.28207767\n",
      "Iteration 69, loss = 1.28124929\n",
      "Iteration 70, loss = 1.28028786\n",
      "Iteration 71, loss = 1.27958384\n",
      "Iteration 72, loss = 1.27876680\n",
      "Iteration 73, loss = 1.27788456\n",
      "Iteration 74, loss = 1.27716827\n",
      "Iteration 75, loss = 1.27632605\n",
      "Iteration 76, loss = 1.27559299\n",
      "Iteration 77, loss = 1.27473504\n",
      "Iteration 78, loss = 1.27391001\n",
      "Iteration 79, loss = 1.27324492\n",
      "Iteration 80, loss = 1.27239412\n",
      "Iteration 81, loss = 1.27176441\n",
      "Iteration 82, loss = 1.27102176\n",
      "Iteration 83, loss = 1.27035364\n",
      "Iteration 84, loss = 1.26969192\n",
      "Iteration 85, loss = 1.26885780\n",
      "Iteration 86, loss = 1.26848885\n",
      "Iteration 87, loss = 1.26729511\n",
      "Iteration 88, loss = 1.26662035\n",
      "Iteration 89, loss = 1.26602013\n",
      "Iteration 90, loss = 1.26510782\n",
      "Iteration 91, loss = 1.26453057\n",
      "Iteration 92, loss = 1.26379100\n",
      "Iteration 93, loss = 1.26310832\n",
      "Iteration 94, loss = 1.26251394\n",
      "Iteration 95, loss = 1.26187111\n",
      "Iteration 96, loss = 1.26100625\n",
      "Iteration 97, loss = 1.26037929\n",
      "Iteration 98, loss = 1.25960455\n",
      "Iteration 99, loss = 1.25882370\n",
      "Iteration 100, loss = 1.25812697\n",
      "Iteration 101, loss = 1.25748254\n",
      "Iteration 102, loss = 1.25693980\n",
      "Iteration 103, loss = 1.25627574\n",
      "Iteration 104, loss = 1.25576810\n",
      "Iteration 105, loss = 1.25495848\n",
      "Iteration 106, loss = 1.25420796\n",
      "Iteration 107, loss = 1.25382445\n",
      "Iteration 108, loss = 1.25310217\n",
      "Iteration 109, loss = 1.25241740\n",
      "Iteration 110, loss = 1.25170958\n",
      "Iteration 111, loss = 1.25112953\n",
      "Iteration 112, loss = 1.25059804\n",
      "Iteration 113, loss = 1.24992985\n",
      "Iteration 114, loss = 1.24919660\n",
      "Iteration 115, loss = 1.24852277\n",
      "Iteration 116, loss = 1.24815506\n",
      "Iteration 117, loss = 1.24739445\n",
      "Iteration 118, loss = 1.24682469\n",
      "Iteration 119, loss = 1.24624227\n",
      "Iteration 120, loss = 1.24582554\n",
      "Iteration 121, loss = 1.24511524\n",
      "Iteration 122, loss = 1.24463222\n",
      "Iteration 123, loss = 1.24400808\n",
      "Iteration 124, loss = 1.24344229\n",
      "Iteration 125, loss = 1.24285719\n",
      "Iteration 126, loss = 1.24224621\n",
      "Iteration 127, loss = 1.24167701\n",
      "Iteration 128, loss = 1.24131328\n",
      "Iteration 129, loss = 1.24063239\n",
      "Iteration 130, loss = 1.24026304\n",
      "Iteration 131, loss = 1.23957081\n",
      "Iteration 132, loss = 1.23905944\n",
      "Iteration 133, loss = 1.23845844\n",
      "Iteration 134, loss = 1.23795850\n",
      "Iteration 135, loss = 1.23754271\n",
      "Iteration 136, loss = 1.23704738\n",
      "Iteration 137, loss = 1.23644350\n",
      "Iteration 138, loss = 1.23601449\n",
      "Iteration 139, loss = 1.23555328\n",
      "Iteration 140, loss = 1.23504290\n",
      "Iteration 141, loss = 1.23450477\n",
      "Iteration 142, loss = 1.23403419\n",
      "Iteration 143, loss = 1.23344461\n",
      "Iteration 144, loss = 1.23296173\n",
      "Iteration 145, loss = 1.23245361\n",
      "Iteration 146, loss = 1.23201291\n",
      "Iteration 147, loss = 1.23141143\n",
      "Iteration 148, loss = 1.23114497\n",
      "Iteration 149, loss = 1.23048251\n",
      "Iteration 150, loss = 1.22997126\n",
      "Iteration 151, loss = 1.22954611\n",
      "Iteration 152, loss = 1.22907994\n",
      "Iteration 153, loss = 1.22859322\n",
      "Iteration 154, loss = 1.22809513\n",
      "Iteration 155, loss = 1.22768330\n",
      "Iteration 156, loss = 1.22724603\n",
      "Iteration 157, loss = 1.22676053\n",
      "Iteration 158, loss = 1.22647835\n",
      "Iteration 159, loss = 1.22602985\n",
      "Iteration 160, loss = 1.22562508\n",
      "Iteration 161, loss = 1.22508839\n",
      "Iteration 162, loss = 1.22465041\n",
      "Iteration 163, loss = 1.22431461\n",
      "Iteration 164, loss = 1.22386468\n",
      "Iteration 165, loss = 1.22352518\n",
      "Iteration 166, loss = 1.22313370\n",
      "Iteration 167, loss = 1.22256680\n",
      "Iteration 168, loss = 1.22206381\n",
      "Iteration 169, loss = 1.22182526\n",
      "Iteration 170, loss = 1.22115995\n",
      "Iteration 171, loss = 1.22089654\n",
      "Iteration 172, loss = 1.22044479\n",
      "Iteration 173, loss = 1.22010610\n",
      "Iteration 174, loss = 1.21964414\n",
      "Iteration 175, loss = 1.21917585\n",
      "Iteration 176, loss = 1.21878845\n",
      "Iteration 177, loss = 1.21828968\n",
      "Iteration 178, loss = 1.21804804\n",
      "Iteration 179, loss = 1.21753806\n",
      "Iteration 180, loss = 1.21706952\n",
      "Iteration 181, loss = 1.21670037\n",
      "Iteration 182, loss = 1.21632865\n",
      "Iteration 183, loss = 1.21596947\n",
      "Iteration 184, loss = 1.21558807\n",
      "Iteration 185, loss = 1.21520867\n",
      "Iteration 186, loss = 1.21479104\n",
      "Iteration 187, loss = 1.21441068\n",
      "Iteration 188, loss = 1.21397142\n",
      "Iteration 189, loss = 1.21368778\n",
      "Iteration 190, loss = 1.21340151\n",
      "Iteration 191, loss = 1.21290869\n",
      "Iteration 192, loss = 1.21255855\n",
      "Iteration 193, loss = 1.21219233\n",
      "Iteration 194, loss = 1.21183650\n",
      "Iteration 195, loss = 1.21155905\n",
      "Iteration 196, loss = 1.21103957\n",
      "Iteration 197, loss = 1.21068769\n",
      "Iteration 198, loss = 1.21037912\n",
      "Iteration 199, loss = 1.21016589\n",
      "Iteration 200, loss = 1.20969084\n",
      "Iteration 1, loss = 1.35796966\n",
      "Iteration 2, loss = 1.33747414\n",
      "Iteration 3, loss = 1.33842690\n",
      "Iteration 4, loss = 1.27613594\n",
      "Iteration 5, loss = 1.31462000\n",
      "Iteration 6, loss = 1.34831355\n",
      "Iteration 7, loss = 1.38069801\n",
      "Iteration 8, loss = 1.32679037\n",
      "Iteration 9, loss = 1.31085539\n",
      "Iteration 10, loss = 1.31508864\n",
      "Iteration 11, loss = 1.31589293\n",
      "Iteration 12, loss = 1.33745638\n",
      "Iteration 13, loss = 1.30429637\n",
      "Iteration 14, loss = 1.32231994\n",
      "Iteration 15, loss = 1.36342494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.72939432\n",
      "Iteration 2, loss = 1.72417528\n",
      "Iteration 3, loss = 1.71853249\n",
      "Iteration 4, loss = 1.71181997\n",
      "Iteration 5, loss = 1.70490289\n",
      "Iteration 6, loss = 1.69822214\n",
      "Iteration 7, loss = 1.68909781\n",
      "Iteration 8, loss = 1.68181521\n",
      "Iteration 9, loss = 1.67521660\n",
      "Iteration 10, loss = 1.66967094\n",
      "Iteration 11, loss = 1.66459613\n",
      "Iteration 12, loss = 1.66048077\n",
      "Iteration 13, loss = 1.65629028\n",
      "Iteration 14, loss = 1.65286692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 1.64807460\n",
      "Iteration 16, loss = 1.64411730\n",
      "Iteration 17, loss = 1.63981493\n",
      "Iteration 18, loss = 1.63564355\n",
      "Iteration 19, loss = 1.63170027\n",
      "Iteration 20, loss = 1.62800070\n",
      "Iteration 21, loss = 1.62431576\n",
      "Iteration 22, loss = 1.62082714\n",
      "Iteration 23, loss = 1.61632309\n",
      "Iteration 24, loss = 1.61242691\n",
      "Iteration 25, loss = 1.60838020\n",
      "Iteration 26, loss = 1.60370061\n",
      "Iteration 27, loss = 1.59927720\n",
      "Iteration 28, loss = 1.59434791\n",
      "Iteration 29, loss = 1.58896618\n",
      "Iteration 30, loss = 1.58355349\n",
      "Iteration 31, loss = 1.57896529\n",
      "Iteration 32, loss = 1.57551202\n",
      "Iteration 33, loss = 1.57163215\n",
      "Iteration 34, loss = 1.56770789\n",
      "Iteration 35, loss = 1.56470959\n",
      "Iteration 36, loss = 1.56129570\n",
      "Iteration 37, loss = 1.55783293\n",
      "Iteration 38, loss = 1.55517536\n",
      "Iteration 39, loss = 1.55162049\n",
      "Iteration 40, loss = 1.54814506\n",
      "Iteration 41, loss = 1.54395320\n",
      "Iteration 42, loss = 1.53994849\n",
      "Iteration 43, loss = 1.53619275\n",
      "Iteration 44, loss = 1.53207263\n",
      "Iteration 45, loss = 1.52884672\n",
      "Iteration 46, loss = 1.52550521\n",
      "Iteration 47, loss = 1.52296250\n",
      "Iteration 48, loss = 1.51977097\n",
      "Iteration 49, loss = 1.51713236\n",
      "Iteration 50, loss = 1.51418252\n",
      "Iteration 51, loss = 1.51143339\n",
      "Iteration 52, loss = 1.50876187\n",
      "Iteration 53, loss = 1.50597734\n",
      "Iteration 54, loss = 1.50333455\n",
      "Iteration 55, loss = 1.50045202\n",
      "Iteration 56, loss = 1.49774400\n",
      "Iteration 57, loss = 1.49499390\n",
      "Iteration 58, loss = 1.49224258\n",
      "Iteration 59, loss = 1.48977359\n",
      "Iteration 60, loss = 1.48693411\n",
      "Iteration 61, loss = 1.48467869\n",
      "Iteration 62, loss = 1.48191476\n",
      "Iteration 63, loss = 1.47984179\n",
      "Iteration 64, loss = 1.47711494\n",
      "Iteration 65, loss = 1.47482667\n",
      "Iteration 66, loss = 1.47212221\n",
      "Iteration 67, loss = 1.46964267\n",
      "Iteration 68, loss = 1.46741638\n",
      "Iteration 69, loss = 1.46490076\n",
      "Iteration 70, loss = 1.46239253\n",
      "Iteration 71, loss = 1.46022597\n",
      "Iteration 72, loss = 1.45787941\n",
      "Iteration 73, loss = 1.45550491\n",
      "Iteration 74, loss = 1.45347396\n",
      "Iteration 75, loss = 1.45099414\n",
      "Iteration 76, loss = 1.44878154\n",
      "Iteration 77, loss = 1.44660499\n",
      "Iteration 78, loss = 1.44429651\n",
      "Iteration 79, loss = 1.44173336\n",
      "Iteration 80, loss = 1.43980475\n",
      "Iteration 81, loss = 1.43757796\n",
      "Iteration 82, loss = 1.43544656\n",
      "Iteration 83, loss = 1.43307455\n",
      "Iteration 84, loss = 1.43081486\n",
      "Iteration 85, loss = 1.42865581\n",
      "Iteration 86, loss = 1.42689601\n",
      "Iteration 87, loss = 1.42471316\n",
      "Iteration 88, loss = 1.42260641\n",
      "Iteration 89, loss = 1.42068080\n",
      "Iteration 90, loss = 1.41870574\n",
      "Iteration 91, loss = 1.41684382\n",
      "Iteration 92, loss = 1.41489396\n",
      "Iteration 93, loss = 1.41275706\n",
      "Iteration 94, loss = 1.41099734\n",
      "Iteration 95, loss = 1.40899568\n",
      "Iteration 96, loss = 1.40704207\n",
      "Iteration 97, loss = 1.40508172\n",
      "Iteration 98, loss = 1.40302749\n",
      "Iteration 99, loss = 1.40115881\n",
      "Iteration 100, loss = 1.39942128\n",
      "Iteration 101, loss = 1.39759621\n",
      "Iteration 102, loss = 1.39569061\n",
      "Iteration 103, loss = 1.39373856\n",
      "Iteration 104, loss = 1.39198913\n",
      "Iteration 105, loss = 1.39035074\n",
      "Iteration 106, loss = 1.38836769\n",
      "Iteration 107, loss = 1.38669435\n",
      "Iteration 108, loss = 1.38514448\n",
      "Iteration 109, loss = 1.38323276\n",
      "Iteration 110, loss = 1.38172720\n",
      "Iteration 111, loss = 1.37995922\n",
      "Iteration 112, loss = 1.37834422\n",
      "Iteration 113, loss = 1.37667422\n",
      "Iteration 114, loss = 1.37507137\n",
      "Iteration 115, loss = 1.37342357\n",
      "Iteration 116, loss = 1.37167807\n",
      "Iteration 117, loss = 1.37023739\n",
      "Iteration 118, loss = 1.36883278\n",
      "Iteration 119, loss = 1.36698310\n",
      "Iteration 120, loss = 1.36548358\n",
      "Iteration 121, loss = 1.36415032\n",
      "Iteration 122, loss = 1.36261680\n",
      "Iteration 123, loss = 1.36100751\n",
      "Iteration 124, loss = 1.35949939\n",
      "Iteration 125, loss = 1.35806507\n",
      "Iteration 126, loss = 1.35675234\n",
      "Iteration 127, loss = 1.35513557\n",
      "Iteration 128, loss = 1.35382525\n",
      "Iteration 129, loss = 1.35234226\n",
      "Iteration 130, loss = 1.35075037\n",
      "Iteration 131, loss = 1.34946044\n",
      "Iteration 132, loss = 1.34805450\n",
      "Iteration 133, loss = 1.34650986\n",
      "Iteration 134, loss = 1.34497080\n",
      "Iteration 135, loss = 1.34372930\n",
      "Iteration 136, loss = 1.34237134\n",
      "Iteration 137, loss = 1.34085588\n",
      "Iteration 138, loss = 1.33956519\n",
      "Iteration 139, loss = 1.33822066\n",
      "Iteration 140, loss = 1.33682332\n",
      "Iteration 141, loss = 1.33552947\n",
      "Iteration 142, loss = 1.33426941\n",
      "Iteration 143, loss = 1.33296376\n",
      "Iteration 144, loss = 1.33169962\n",
      "Iteration 145, loss = 1.33043158\n",
      "Iteration 146, loss = 1.32918395\n",
      "Iteration 147, loss = 1.32793182\n",
      "Iteration 148, loss = 1.32657491\n",
      "Iteration 149, loss = 1.32536772\n",
      "Iteration 150, loss = 1.32406883\n",
      "Iteration 151, loss = 1.32294842\n",
      "Iteration 152, loss = 1.32149107\n",
      "Iteration 153, loss = 1.32047954\n",
      "Iteration 154, loss = 1.31910919\n",
      "Iteration 155, loss = 1.31795879\n",
      "Iteration 156, loss = 1.31684550\n",
      "Iteration 157, loss = 1.31573337\n",
      "Iteration 158, loss = 1.31463404\n",
      "Iteration 159, loss = 1.31341304\n",
      "Iteration 160, loss = 1.31236153\n",
      "Iteration 161, loss = 1.31122703\n",
      "Iteration 162, loss = 1.31011371\n",
      "Iteration 163, loss = 1.30910028\n",
      "Iteration 164, loss = 1.30788729\n",
      "Iteration 165, loss = 1.30695590\n",
      "Iteration 166, loss = 1.30585989\n",
      "Iteration 167, loss = 1.30483118\n",
      "Iteration 168, loss = 1.30374637\n",
      "Iteration 169, loss = 1.30275025\n",
      "Iteration 170, loss = 1.30168789\n",
      "Iteration 171, loss = 1.30069517\n",
      "Iteration 172, loss = 1.29955408\n",
      "Iteration 173, loss = 1.29871903\n",
      "Iteration 174, loss = 1.29766615\n",
      "Iteration 175, loss = 1.29677382\n",
      "Iteration 176, loss = 1.29576661\n",
      "Iteration 177, loss = 1.29472540\n",
      "Iteration 178, loss = 1.29387901\n",
      "Iteration 179, loss = 1.29276126\n",
      "Iteration 180, loss = 1.29193060\n",
      "Iteration 181, loss = 1.29092364\n",
      "Iteration 182, loss = 1.29004121\n",
      "Iteration 183, loss = 1.28908345\n",
      "Iteration 184, loss = 1.28817769\n",
      "Iteration 185, loss = 1.28730304\n",
      "Iteration 186, loss = 1.28630080\n",
      "Iteration 187, loss = 1.28523652\n",
      "Iteration 188, loss = 1.28454330\n",
      "Iteration 189, loss = 1.28351924\n",
      "Iteration 190, loss = 1.28256114\n",
      "Iteration 191, loss = 1.28166116\n",
      "Iteration 192, loss = 1.28091037\n",
      "Iteration 193, loss = 1.27975959\n",
      "Iteration 194, loss = 1.27884250\n",
      "Iteration 195, loss = 1.27815721\n",
      "Iteration 196, loss = 1.27707216\n",
      "Iteration 197, loss = 1.27621272\n",
      "Iteration 198, loss = 1.27536030\n",
      "Iteration 199, loss = 1.27467605\n",
      "Iteration 200, loss = 1.27378644\n",
      "Iteration 1, loss = 1.43260134\n",
      "Iteration 2, loss = 1.27169575\n",
      "Iteration 3, loss = 1.26187614\n",
      "Iteration 4, loss = 1.33874274\n",
      "Iteration 5, loss = 1.26780266\n",
      "Iteration 6, loss = 1.26617995\n",
      "Iteration 7, loss = 1.23942962\n",
      "Iteration 8, loss = 1.26549564\n",
      "Iteration 9, loss = 1.21599107\n",
      "Iteration 10, loss = 1.24555115\n",
      "Iteration 11, loss = 1.26521013\n",
      "Iteration 12, loss = 1.25120241\n",
      "Iteration 13, loss = 1.27212665\n",
      "Iteration 14, loss = 1.25256110\n",
      "Iteration 15, loss = 1.25486272\n",
      "Iteration 16, loss = 1.24616341\n",
      "Iteration 17, loss = 1.24431710\n",
      "Iteration 18, loss = 1.22655403\n",
      "Iteration 19, loss = 1.26815036\n",
      "Iteration 20, loss = 1.29250027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98985010\n",
      "Iteration 2, loss = 1.98226658\n",
      "Iteration 3, loss = 1.97406344\n",
      "Iteration 4, loss = 1.96723607\n",
      "Iteration 5, loss = 1.95957611\n",
      "Iteration 6, loss = 1.95280431\n",
      "Iteration 7, loss = 1.94592960\n",
      "Iteration 8, loss = 1.93943048\n",
      "Iteration 9, loss = 1.93284364\n",
      "Iteration 10, loss = 1.92755832\n",
      "Iteration 11, loss = 1.92112407\n",
      "Iteration 12, loss = 1.91462035\n",
      "Iteration 13, loss = 1.90782512\n",
      "Iteration 14, loss = 1.90030636\n",
      "Iteration 15, loss = 1.89251208\n",
      "Iteration 16, loss = 1.88370312\n",
      "Iteration 17, loss = 1.87457780\n",
      "Iteration 18, loss = 1.86697062\n",
      "Iteration 19, loss = 1.86016519\n",
      "Iteration 20, loss = 1.85386084\n",
      "Iteration 21, loss = 1.84826106\n",
      "Iteration 22, loss = 1.84311815\n",
      "Iteration 23, loss = 1.83842374\n",
      "Iteration 24, loss = 1.83382814\n",
      "Iteration 25, loss = 1.82960861\n",
      "Iteration 26, loss = 1.82488704\n",
      "Iteration 27, loss = 1.82095154\n",
      "Iteration 28, loss = 1.81673406\n",
      "Iteration 29, loss = 1.81256518\n",
      "Iteration 30, loss = 1.80876644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 1.80472325\n",
      "Iteration 32, loss = 1.80098648\n",
      "Iteration 33, loss = 1.79702530\n",
      "Iteration 34, loss = 1.79314444\n",
      "Iteration 35, loss = 1.78955002\n",
      "Iteration 36, loss = 1.78560145\n",
      "Iteration 37, loss = 1.78219846\n",
      "Iteration 38, loss = 1.77830517\n",
      "Iteration 39, loss = 1.77474969\n",
      "Iteration 40, loss = 1.77118890\n",
      "Iteration 41, loss = 1.76760834\n",
      "Iteration 42, loss = 1.76399404\n",
      "Iteration 43, loss = 1.76064779\n",
      "Iteration 44, loss = 1.75696382\n",
      "Iteration 45, loss = 1.75355959\n",
      "Iteration 46, loss = 1.75041164\n",
      "Iteration 47, loss = 1.74699089\n",
      "Iteration 48, loss = 1.74366539\n",
      "Iteration 49, loss = 1.74024943\n",
      "Iteration 50, loss = 1.73712104\n",
      "Iteration 51, loss = 1.73361209\n",
      "Iteration 52, loss = 1.73049121\n",
      "Iteration 53, loss = 1.72707026\n",
      "Iteration 54, loss = 1.72407010\n",
      "Iteration 55, loss = 1.72069608\n",
      "Iteration 56, loss = 1.71765935\n",
      "Iteration 57, loss = 1.71436843\n",
      "Iteration 58, loss = 1.71135481\n",
      "Iteration 59, loss = 1.70837201\n",
      "Iteration 60, loss = 1.70524080\n",
      "Iteration 61, loss = 1.70216445\n",
      "Iteration 62, loss = 1.69906619\n",
      "Iteration 63, loss = 1.69570939\n",
      "Iteration 64, loss = 1.69280070\n",
      "Iteration 65, loss = 1.68898216\n",
      "Iteration 66, loss = 1.68550748\n",
      "Iteration 67, loss = 1.68127417\n",
      "Iteration 68, loss = 1.67710885\n",
      "Iteration 69, loss = 1.66974103\n",
      "Iteration 70, loss = 1.66562417\n",
      "Iteration 71, loss = 1.65843786\n",
      "Iteration 72, loss = 1.65284724\n",
      "Iteration 73, loss = 1.64821910\n",
      "Iteration 74, loss = 1.64432204\n",
      "Iteration 75, loss = 1.64113774\n",
      "Iteration 76, loss = 1.63779718\n",
      "Iteration 77, loss = 1.63464134\n",
      "Iteration 78, loss = 1.63179861\n",
      "Iteration 79, loss = 1.62858966\n",
      "Iteration 80, loss = 1.62581100\n",
      "Iteration 81, loss = 1.62284579\n",
      "Iteration 82, loss = 1.61995477\n",
      "Iteration 83, loss = 1.61697205\n",
      "Iteration 84, loss = 1.61385189\n",
      "Iteration 85, loss = 1.61099962\n",
      "Iteration 86, loss = 1.60755399\n",
      "Iteration 87, loss = 1.60438261\n",
      "Iteration 88, loss = 1.60040315\n",
      "Iteration 89, loss = 1.59657510\n",
      "Iteration 90, loss = 1.59239557\n",
      "Iteration 91, loss = 1.58859607\n",
      "Iteration 92, loss = 1.58466664\n",
      "Iteration 93, loss = 1.58107988\n",
      "Iteration 94, loss = 1.57760768\n",
      "Iteration 95, loss = 1.57484971\n",
      "Iteration 96, loss = 1.57220188\n",
      "Iteration 97, loss = 1.56954078\n",
      "Iteration 98, loss = 1.56671038\n",
      "Iteration 99, loss = 1.56407686\n",
      "Iteration 100, loss = 1.56144481\n",
      "Iteration 101, loss = 1.55876142\n",
      "Iteration 102, loss = 1.55621083\n",
      "Iteration 103, loss = 1.55374466\n",
      "Iteration 104, loss = 1.55100109\n",
      "Iteration 105, loss = 1.54852948\n",
      "Iteration 106, loss = 1.54597177\n",
      "Iteration 107, loss = 1.54349182\n",
      "Iteration 108, loss = 1.54086697\n",
      "Iteration 109, loss = 1.53835260\n",
      "Iteration 110, loss = 1.53591567\n",
      "Iteration 111, loss = 1.53336492\n",
      "Iteration 112, loss = 1.53100737\n",
      "Iteration 113, loss = 1.52829575\n",
      "Iteration 114, loss = 1.52591593\n",
      "Iteration 115, loss = 1.52340523\n",
      "Iteration 116, loss = 1.52111561\n",
      "Iteration 117, loss = 1.51854752\n",
      "Iteration 118, loss = 1.51629063\n",
      "Iteration 119, loss = 1.51391348\n",
      "Iteration 120, loss = 1.51147341\n",
      "Iteration 121, loss = 1.50936619\n",
      "Iteration 122, loss = 1.50688637\n",
      "Iteration 123, loss = 1.50452936\n",
      "Iteration 124, loss = 1.50224955\n",
      "Iteration 125, loss = 1.49980905\n",
      "Iteration 126, loss = 1.49745256\n",
      "Iteration 127, loss = 1.49505904\n",
      "Iteration 128, loss = 1.49271652\n",
      "Iteration 129, loss = 1.49027994\n",
      "Iteration 130, loss = 1.48793701\n",
      "Iteration 131, loss = 1.48569943\n",
      "Iteration 132, loss = 1.48328555\n",
      "Iteration 133, loss = 1.48103930\n",
      "Iteration 134, loss = 1.47891339\n",
      "Iteration 135, loss = 1.47652556\n",
      "Iteration 136, loss = 1.47404034\n",
      "Iteration 137, loss = 1.47207947\n",
      "Iteration 138, loss = 1.46997567\n",
      "Iteration 139, loss = 1.46783138\n",
      "Iteration 140, loss = 1.46546997\n",
      "Iteration 141, loss = 1.46352027\n",
      "Iteration 142, loss = 1.46116336\n",
      "Iteration 143, loss = 1.45907008\n",
      "Iteration 144, loss = 1.45698342\n",
      "Iteration 145, loss = 1.45468455\n",
      "Iteration 146, loss = 1.45274225\n",
      "Iteration 147, loss = 1.45046811\n",
      "Iteration 148, loss = 1.44811595\n",
      "Iteration 149, loss = 1.44604180\n",
      "Iteration 150, loss = 1.44372205\n",
      "Iteration 151, loss = 1.44176208\n",
      "Iteration 152, loss = 1.43959149\n",
      "Iteration 153, loss = 1.43738970\n",
      "Iteration 154, loss = 1.43547350\n",
      "Iteration 155, loss = 1.43310651\n",
      "Iteration 156, loss = 1.43102745\n",
      "Iteration 157, loss = 1.42940669\n",
      "Iteration 158, loss = 1.42698731\n",
      "Iteration 159, loss = 1.42506591\n",
      "Iteration 160, loss = 1.42332011\n",
      "Iteration 161, loss = 1.42128386\n",
      "Iteration 162, loss = 1.41898595\n",
      "Iteration 163, loss = 1.41708280\n",
      "Iteration 164, loss = 1.41528056\n",
      "Iteration 165, loss = 1.41329540\n",
      "Iteration 166, loss = 1.41129853\n",
      "Iteration 167, loss = 1.40959165\n",
      "Iteration 168, loss = 1.40775626\n",
      "Iteration 169, loss = 1.40575672\n",
      "Iteration 170, loss = 1.40392816\n",
      "Iteration 171, loss = 1.40205642\n",
      "Iteration 172, loss = 1.40019343\n",
      "Iteration 173, loss = 1.39842155\n",
      "Iteration 174, loss = 1.39626992\n",
      "Iteration 175, loss = 1.39450042\n",
      "Iteration 176, loss = 1.39253827\n",
      "Iteration 177, loss = 1.39063412\n",
      "Iteration 178, loss = 1.38894771\n",
      "Iteration 179, loss = 1.38688175\n",
      "Iteration 180, loss = 1.38508131\n",
      "Iteration 181, loss = 1.38321472\n",
      "Iteration 182, loss = 1.38144288\n",
      "Iteration 183, loss = 1.37956123\n",
      "Iteration 184, loss = 1.37788074\n",
      "Iteration 185, loss = 1.37610085\n",
      "Iteration 186, loss = 1.37432376\n",
      "Iteration 187, loss = 1.37261797\n",
      "Iteration 188, loss = 1.37069439\n",
      "Iteration 189, loss = 1.36902774\n",
      "Iteration 190, loss = 1.36723063\n",
      "Iteration 191, loss = 1.36565503\n",
      "Iteration 192, loss = 1.36380967\n",
      "Iteration 193, loss = 1.36220846\n",
      "Iteration 194, loss = 1.36044669\n",
      "Iteration 195, loss = 1.35884963\n",
      "Iteration 196, loss = 1.35735357\n",
      "Iteration 197, loss = 1.35564084\n",
      "Iteration 198, loss = 1.35396535\n",
      "Iteration 199, loss = 1.35255577\n",
      "Iteration 200, loss = 1.35103745\n",
      "Iteration 1, loss = 1.48407535\n",
      "Iteration 2, loss = 1.24096855\n",
      "Iteration 3, loss = 1.32656595\n",
      "Iteration 4, loss = 1.28910499\n",
      "Iteration 5, loss = 1.28037148\n",
      "Iteration 6, loss = 1.25792337\n",
      "Iteration 7, loss = 1.27953664\n",
      "Iteration 8, loss = 1.28502886\n",
      "Iteration 9, loss = 1.24904188\n",
      "Iteration 10, loss = 1.26520641\n",
      "Iteration 11, loss = 1.28772728\n",
      "Iteration 12, loss = 1.23644030\n",
      "Iteration 13, loss = 1.25082693\n",
      "Iteration 14, loss = 1.24359467\n",
      "Iteration 15, loss = 1.23310748\n",
      "Iteration 16, loss = 1.25767817\n",
      "Iteration 17, loss = 1.23633962\n",
      "Iteration 18, loss = 1.23574218\n",
      "Iteration 19, loss = 1.26963258\n",
      "Iteration 20, loss = 1.24903782\n",
      "Iteration 21, loss = 1.25754131\n",
      "Iteration 22, loss = 1.23781096\n",
      "Iteration 23, loss = 1.24790526\n",
      "Iteration 24, loss = 1.23047510\n",
      "Iteration 25, loss = 1.24559358\n",
      "Iteration 26, loss = 1.23695095\n",
      "Iteration 27, loss = 1.25658015\n",
      "Iteration 28, loss = 1.23727126\n",
      "Iteration 29, loss = 1.28588294\n",
      "Iteration 30, loss = 1.25161647\n",
      "Iteration 31, loss = 1.31308247\n",
      "Iteration 32, loss = 1.21275406\n",
      "Iteration 33, loss = 1.28856842\n",
      "Iteration 34, loss = 1.22549270\n",
      "Iteration 35, loss = 1.26987324\n",
      "Iteration 36, loss = 1.22436656\n",
      "Iteration 37, loss = 1.22256644\n",
      "Iteration 38, loss = 1.25040446\n",
      "Iteration 39, loss = 1.22809402\n",
      "Iteration 40, loss = 1.22649802\n",
      "Iteration 41, loss = 1.26912082\n",
      "Iteration 42, loss = 1.24668450\n",
      "Iteration 43, loss = 1.24705509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27574367\n",
      "Iteration 2, loss = 1.27279383\n",
      "Iteration 3, loss = 1.27033554\n",
      "Iteration 4, loss = 1.26724522\n",
      "Iteration 5, loss = 1.26561706\n",
      "Iteration 6, loss = 1.26223598\n",
      "Iteration 7, loss = 1.26011364\n",
      "Iteration 8, loss = 1.25829184\n",
      "Iteration 9, loss = 1.25600422\n",
      "Iteration 10, loss = 1.25476750\n",
      "Iteration 11, loss = 1.25249315\n",
      "Iteration 12, loss = 1.25080204\n",
      "Iteration 13, loss = 1.24896235\n",
      "Iteration 14, loss = 1.24761240\n",
      "Iteration 15, loss = 1.24569943\n",
      "Iteration 16, loss = 1.24406595\n",
      "Iteration 17, loss = 1.24215518\n",
      "Iteration 18, loss = 1.24086753\n",
      "Iteration 19, loss = 1.23915955\n",
      "Iteration 20, loss = 1.23746544\n",
      "Iteration 21, loss = 1.23573414\n",
      "Iteration 22, loss = 1.23416397\n",
      "Iteration 23, loss = 1.23236069\n",
      "Iteration 24, loss = 1.23037896\n",
      "Iteration 25, loss = 1.22762509\n",
      "Iteration 26, loss = 1.22492115\n",
      "Iteration 27, loss = 1.22286888\n",
      "Iteration 28, loss = 1.22003177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29, loss = 1.21823359\n",
      "Iteration 30, loss = 1.21684550\n",
      "Iteration 31, loss = 1.21555776\n",
      "Iteration 32, loss = 1.21434447\n",
      "Iteration 33, loss = 1.21340094\n",
      "Iteration 34, loss = 1.21264647\n",
      "Iteration 35, loss = 1.21180839\n",
      "Iteration 36, loss = 1.21075721\n",
      "Iteration 37, loss = 1.21007812\n",
      "Iteration 38, loss = 1.20930923\n",
      "Iteration 39, loss = 1.20837140\n",
      "Iteration 40, loss = 1.20761768\n",
      "Iteration 41, loss = 1.20690029\n",
      "Iteration 42, loss = 1.20612682\n",
      "Iteration 43, loss = 1.20537722\n",
      "Iteration 44, loss = 1.20468149\n",
      "Iteration 45, loss = 1.20396193\n",
      "Iteration 46, loss = 1.20305808\n",
      "Iteration 47, loss = 1.20239338\n",
      "Iteration 48, loss = 1.20166764\n",
      "Iteration 49, loss = 1.20090163\n",
      "Iteration 50, loss = 1.20030689\n",
      "Iteration 51, loss = 1.19958169\n",
      "Iteration 52, loss = 1.19895160\n",
      "Iteration 53, loss = 1.19826990\n",
      "Iteration 54, loss = 1.19763524\n",
      "Iteration 55, loss = 1.19715919\n",
      "Iteration 56, loss = 1.19636245\n",
      "Iteration 57, loss = 1.19585651\n",
      "Iteration 58, loss = 1.19512860\n",
      "Iteration 59, loss = 1.19453481\n",
      "Iteration 60, loss = 1.19403788\n",
      "Iteration 61, loss = 1.19333558\n",
      "Iteration 62, loss = 1.19291814\n",
      "Iteration 63, loss = 1.19241902\n",
      "Iteration 64, loss = 1.19196171\n",
      "Iteration 65, loss = 1.19120867\n",
      "Iteration 66, loss = 1.19069704\n",
      "Iteration 67, loss = 1.19027015\n",
      "Iteration 68, loss = 1.18984169\n",
      "Iteration 69, loss = 1.18935815\n",
      "Iteration 70, loss = 1.18887607\n",
      "Iteration 71, loss = 1.18844084\n",
      "Iteration 72, loss = 1.18802631\n",
      "Iteration 73, loss = 1.18767546\n",
      "Iteration 74, loss = 1.18721671\n",
      "Iteration 75, loss = 1.18677891\n",
      "Iteration 76, loss = 1.18639229\n",
      "Iteration 77, loss = 1.18592938\n",
      "Iteration 78, loss = 1.18559534\n",
      "Iteration 79, loss = 1.18512795\n",
      "Iteration 80, loss = 1.18470132\n",
      "Iteration 81, loss = 1.18441254\n",
      "Iteration 82, loss = 1.18397007\n",
      "Iteration 83, loss = 1.18356008\n",
      "Iteration 84, loss = 1.18317427\n",
      "Iteration 85, loss = 1.18275546\n",
      "Iteration 86, loss = 1.18237038\n",
      "Iteration 87, loss = 1.18202969\n",
      "Iteration 88, loss = 1.18163735\n",
      "Iteration 89, loss = 1.18128325\n",
      "Iteration 90, loss = 1.18092401\n",
      "Iteration 91, loss = 1.18052053\n",
      "Iteration 92, loss = 1.18017140\n",
      "Iteration 93, loss = 1.17997853\n",
      "Iteration 94, loss = 1.17946709\n",
      "Iteration 95, loss = 1.17924398\n",
      "Iteration 96, loss = 1.17884721\n",
      "Iteration 97, loss = 1.17851826\n",
      "Iteration 98, loss = 1.17820030\n",
      "Iteration 99, loss = 1.17797678\n",
      "Iteration 100, loss = 1.17765837\n",
      "Iteration 101, loss = 1.17735791\n",
      "Iteration 102, loss = 1.17701260\n",
      "Iteration 103, loss = 1.17679175\n",
      "Iteration 104, loss = 1.17650133\n",
      "Iteration 105, loss = 1.17615884\n",
      "Iteration 106, loss = 1.17596474\n",
      "Iteration 107, loss = 1.17576965\n",
      "Iteration 108, loss = 1.17532432\n",
      "Iteration 109, loss = 1.17512004\n",
      "Iteration 110, loss = 1.17489165\n",
      "Iteration 111, loss = 1.17455811\n",
      "Iteration 112, loss = 1.17433146\n",
      "Iteration 113, loss = 1.17405233\n",
      "Iteration 114, loss = 1.17383304\n",
      "Iteration 115, loss = 1.17353139\n",
      "Iteration 116, loss = 1.17335251\n",
      "Iteration 117, loss = 1.17313144\n",
      "Iteration 118, loss = 1.17291297\n",
      "Iteration 119, loss = 1.17267797\n",
      "Iteration 120, loss = 1.17242013\n",
      "Iteration 121, loss = 1.17219558\n",
      "Iteration 122, loss = 1.17196399\n",
      "Iteration 123, loss = 1.17181336\n",
      "Iteration 124, loss = 1.17159452\n",
      "Iteration 125, loss = 1.17126862\n",
      "Iteration 126, loss = 1.17108786\n",
      "Iteration 127, loss = 1.17084751\n",
      "Iteration 128, loss = 1.17064384\n",
      "Iteration 129, loss = 1.17033558\n",
      "Iteration 130, loss = 1.17007047\n",
      "Iteration 131, loss = 1.16989669\n",
      "Iteration 132, loss = 1.16967820\n",
      "Iteration 133, loss = 1.16953229\n",
      "Iteration 134, loss = 1.16929280\n",
      "Iteration 135, loss = 1.16913777\n",
      "Iteration 136, loss = 1.16886048\n",
      "Iteration 137, loss = 1.16876059\n",
      "Iteration 138, loss = 1.16850840\n",
      "Iteration 139, loss = 1.16836063\n",
      "Iteration 140, loss = 1.16819079\n",
      "Iteration 141, loss = 1.16801242\n",
      "Iteration 142, loss = 1.16788508\n",
      "Iteration 143, loss = 1.16759255\n",
      "Iteration 144, loss = 1.16742896\n",
      "Iteration 145, loss = 1.16729724\n",
      "Iteration 146, loss = 1.16711887\n",
      "Iteration 147, loss = 1.16685380\n",
      "Iteration 148, loss = 1.16665204\n",
      "Iteration 149, loss = 1.16646739\n",
      "Iteration 150, loss = 1.16635554\n",
      "Iteration 151, loss = 1.16610424\n",
      "Iteration 152, loss = 1.16594033\n",
      "Iteration 153, loss = 1.16580355\n",
      "Iteration 154, loss = 1.16558955\n",
      "Iteration 155, loss = 1.16550264\n",
      "Iteration 156, loss = 1.16528349\n",
      "Iteration 157, loss = 1.16516659\n",
      "Iteration 158, loss = 1.16488927\n",
      "Iteration 159, loss = 1.16481624\n",
      "Iteration 160, loss = 1.16457040\n",
      "Iteration 161, loss = 1.16442481\n",
      "Iteration 162, loss = 1.16424429\n",
      "Iteration 163, loss = 1.16409216\n",
      "Iteration 164, loss = 1.16389141\n",
      "Iteration 165, loss = 1.16380522\n",
      "Iteration 166, loss = 1.16355167\n",
      "Iteration 167, loss = 1.16341998\n",
      "Iteration 168, loss = 1.16328417\n",
      "Iteration 169, loss = 1.16307224\n",
      "Iteration 170, loss = 1.16299841\n",
      "Iteration 171, loss = 1.16280467\n",
      "Iteration 172, loss = 1.16259366\n",
      "Iteration 173, loss = 1.16247433\n",
      "Iteration 174, loss = 1.16231377\n",
      "Iteration 175, loss = 1.16216653\n",
      "Iteration 176, loss = 1.16202858\n",
      "Iteration 177, loss = 1.16189062\n",
      "Iteration 178, loss = 1.16166062\n",
      "Iteration 179, loss = 1.16151060\n",
      "Iteration 180, loss = 1.16135215\n",
      "Iteration 181, loss = 1.16124370\n",
      "Iteration 182, loss = 1.16106730\n",
      "Iteration 183, loss = 1.16092357\n",
      "Iteration 184, loss = 1.16083170\n",
      "Iteration 185, loss = 1.16065407\n",
      "Iteration 186, loss = 1.16048309\n",
      "Iteration 187, loss = 1.16040124\n",
      "Iteration 188, loss = 1.16025083\n",
      "Iteration 189, loss = 1.16014025\n",
      "Iteration 190, loss = 1.15997982\n",
      "Iteration 191, loss = 1.15990023\n",
      "Iteration 192, loss = 1.15967593\n",
      "Iteration 193, loss = 1.15955415\n",
      "Iteration 194, loss = 1.15939678\n",
      "Iteration 195, loss = 1.15926750\n",
      "Iteration 196, loss = 1.15908353\n",
      "Iteration 197, loss = 1.15890350\n",
      "Iteration 198, loss = 1.15873961\n",
      "Iteration 199, loss = 1.15861343\n",
      "Iteration 200, loss = 1.15843032\n",
      "Iteration 1, loss = 1.60335615\n",
      "Iteration 2, loss = 1.41762136\n",
      "Iteration 3, loss = 1.32013550\n",
      "Iteration 4, loss = 1.27688163\n",
      "Iteration 5, loss = 1.27224293\n",
      "Iteration 6, loss = 1.27774285\n",
      "Iteration 7, loss = 1.33173915\n",
      "Iteration 8, loss = 1.26785541\n",
      "Iteration 9, loss = 1.27619250\n",
      "Iteration 10, loss = 1.27546957\n",
      "Iteration 11, loss = 1.26129507\n",
      "Iteration 12, loss = 1.38704040\n",
      "Iteration 13, loss = 1.30572741\n",
      "Iteration 14, loss = 1.30894297\n",
      "Iteration 15, loss = 1.29589603\n",
      "Iteration 16, loss = 1.26519998\n",
      "Iteration 17, loss = 1.30790451\n",
      "Iteration 18, loss = 1.28472231\n",
      "Iteration 19, loss = 1.25579769\n",
      "Iteration 20, loss = 1.31517874\n",
      "Iteration 21, loss = 1.29802950\n",
      "Iteration 22, loss = 1.28678314\n",
      "Iteration 23, loss = 1.27841913\n",
      "Iteration 24, loss = 1.27906153\n",
      "Iteration 25, loss = 1.26365045\n",
      "Iteration 26, loss = 1.28381408\n",
      "Iteration 27, loss = 1.30350964\n",
      "Iteration 28, loss = 1.27410062\n",
      "Iteration 29, loss = 1.32142453\n",
      "Iteration 30, loss = 1.26296249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55400776\n",
      "Iteration 2, loss = 1.54930755\n",
      "Iteration 3, loss = 1.54472489\n",
      "Iteration 4, loss = 1.54047564\n",
      "Iteration 5, loss = 1.53576206\n",
      "Iteration 6, loss = 1.53203572\n",
      "Iteration 7, loss = 1.52724649\n",
      "Iteration 8, loss = 1.52365270\n",
      "Iteration 9, loss = 1.51912393\n",
      "Iteration 10, loss = 1.51523769\n",
      "Iteration 11, loss = 1.51138821\n",
      "Iteration 12, loss = 1.50797417\n",
      "Iteration 13, loss = 1.50385465\n",
      "Iteration 14, loss = 1.50028037\n",
      "Iteration 15, loss = 1.49660560\n",
      "Iteration 16, loss = 1.49262090\n",
      "Iteration 17, loss = 1.48941086\n",
      "Iteration 18, loss = 1.48540764\n",
      "Iteration 19, loss = 1.48178078\n",
      "Iteration 20, loss = 1.47837238\n",
      "Iteration 21, loss = 1.47484130\n",
      "Iteration 22, loss = 1.47169269\n",
      "Iteration 23, loss = 1.46818970\n",
      "Iteration 24, loss = 1.46494506\n",
      "Iteration 25, loss = 1.46189899\n",
      "Iteration 26, loss = 1.45871222\n",
      "Iteration 27, loss = 1.45556231\n",
      "Iteration 28, loss = 1.45235260\n",
      "Iteration 29, loss = 1.44961523\n",
      "Iteration 30, loss = 1.44657562\n",
      "Iteration 31, loss = 1.44366685\n",
      "Iteration 32, loss = 1.44073630\n",
      "Iteration 33, loss = 1.43803002\n",
      "Iteration 34, loss = 1.43510635\n",
      "Iteration 35, loss = 1.43265922\n",
      "Iteration 36, loss = 1.43004727\n",
      "Iteration 37, loss = 1.42710024\n",
      "Iteration 38, loss = 1.42442336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 1.42204201\n",
      "Iteration 40, loss = 1.41931696\n",
      "Iteration 41, loss = 1.41666321\n",
      "Iteration 42, loss = 1.41424488\n",
      "Iteration 43, loss = 1.41186121\n",
      "Iteration 44, loss = 1.40943742\n",
      "Iteration 45, loss = 1.40682215\n",
      "Iteration 46, loss = 1.40455681\n",
      "Iteration 47, loss = 1.40203431\n",
      "Iteration 48, loss = 1.40043063\n",
      "Iteration 49, loss = 1.39754025\n",
      "Iteration 50, loss = 1.39530847\n",
      "Iteration 51, loss = 1.39320974\n",
      "Iteration 52, loss = 1.39110197\n",
      "Iteration 53, loss = 1.38903183\n",
      "Iteration 54, loss = 1.38677315\n",
      "Iteration 55, loss = 1.38468410\n",
      "Iteration 56, loss = 1.38281770\n",
      "Iteration 57, loss = 1.38076715\n",
      "Iteration 58, loss = 1.37904268\n",
      "Iteration 59, loss = 1.37709217\n",
      "Iteration 60, loss = 1.37499652\n",
      "Iteration 61, loss = 1.37309980\n",
      "Iteration 62, loss = 1.37162573\n",
      "Iteration 63, loss = 1.36974713\n",
      "Iteration 64, loss = 1.36788573\n",
      "Iteration 65, loss = 1.36602373\n",
      "Iteration 66, loss = 1.36437277\n",
      "Iteration 67, loss = 1.36276755\n",
      "Iteration 68, loss = 1.36092339\n",
      "Iteration 69, loss = 1.35960037\n",
      "Iteration 70, loss = 1.35755567\n",
      "Iteration 71, loss = 1.35610221\n",
      "Iteration 72, loss = 1.35459373\n",
      "Iteration 73, loss = 1.35250432\n",
      "Iteration 74, loss = 1.35114937\n",
      "Iteration 75, loss = 1.34960282\n",
      "Iteration 76, loss = 1.34762460\n",
      "Iteration 77, loss = 1.34625468\n",
      "Iteration 78, loss = 1.34458677\n",
      "Iteration 79, loss = 1.34300029\n",
      "Iteration 80, loss = 1.34157287\n",
      "Iteration 81, loss = 1.34000123\n",
      "Iteration 82, loss = 1.33845109\n",
      "Iteration 83, loss = 1.33709253\n",
      "Iteration 84, loss = 1.33589709\n",
      "Iteration 85, loss = 1.33435947\n",
      "Iteration 86, loss = 1.33331783\n",
      "Iteration 87, loss = 1.33201687\n",
      "Iteration 88, loss = 1.33081085\n",
      "Iteration 89, loss = 1.32964841\n",
      "Iteration 90, loss = 1.32865961\n",
      "Iteration 91, loss = 1.32752685\n",
      "Iteration 92, loss = 1.32638024\n",
      "Iteration 93, loss = 1.32527495\n",
      "Iteration 94, loss = 1.32421993\n",
      "Iteration 95, loss = 1.32322480\n",
      "Iteration 96, loss = 1.32207783\n",
      "Iteration 97, loss = 1.32127217\n",
      "Iteration 98, loss = 1.32000575\n",
      "Iteration 99, loss = 1.31884021\n",
      "Iteration 100, loss = 1.31816834\n",
      "Iteration 101, loss = 1.31710972\n",
      "Iteration 102, loss = 1.31606544\n",
      "Iteration 103, loss = 1.31541125\n",
      "Iteration 104, loss = 1.31433805\n",
      "Iteration 105, loss = 1.31369295\n",
      "Iteration 106, loss = 1.31268698\n",
      "Iteration 107, loss = 1.31201793\n",
      "Iteration 108, loss = 1.31120834\n",
      "Iteration 109, loss = 1.31030256\n",
      "Iteration 110, loss = 1.30962803\n",
      "Iteration 111, loss = 1.30886066\n",
      "Iteration 112, loss = 1.30820158\n",
      "Iteration 113, loss = 1.30752247\n",
      "Iteration 114, loss = 1.30667140\n",
      "Iteration 115, loss = 1.30583688\n",
      "Iteration 116, loss = 1.30506915\n",
      "Iteration 117, loss = 1.30447739\n",
      "Iteration 118, loss = 1.30367957\n",
      "Iteration 119, loss = 1.30294489\n",
      "Iteration 120, loss = 1.30222147\n",
      "Iteration 121, loss = 1.30162972\n",
      "Iteration 122, loss = 1.30103491\n",
      "Iteration 123, loss = 1.30040916\n",
      "Iteration 124, loss = 1.29976654\n",
      "Iteration 125, loss = 1.29932800\n",
      "Iteration 126, loss = 1.29868598\n",
      "Iteration 127, loss = 1.29827410\n",
      "Iteration 128, loss = 1.29756210\n",
      "Iteration 129, loss = 1.29714681\n",
      "Iteration 130, loss = 1.29652666\n",
      "Iteration 131, loss = 1.29603892\n",
      "Iteration 132, loss = 1.29548199\n",
      "Iteration 133, loss = 1.29499073\n",
      "Iteration 134, loss = 1.29457360\n",
      "Iteration 135, loss = 1.29401277\n",
      "Iteration 136, loss = 1.29368465\n",
      "Iteration 137, loss = 1.29312045\n",
      "Iteration 138, loss = 1.29258844\n",
      "Iteration 139, loss = 1.29211653\n",
      "Iteration 140, loss = 1.29180136\n",
      "Iteration 141, loss = 1.29124897\n",
      "Iteration 142, loss = 1.29093290\n",
      "Iteration 143, loss = 1.29048340\n",
      "Iteration 144, loss = 1.29009336\n",
      "Iteration 145, loss = 1.28966062\n",
      "Iteration 146, loss = 1.28929484\n",
      "Iteration 147, loss = 1.28883002\n",
      "Iteration 148, loss = 1.28846574\n",
      "Iteration 149, loss = 1.28800573\n",
      "Iteration 150, loss = 1.28764839\n",
      "Iteration 151, loss = 1.28721288\n",
      "Iteration 152, loss = 1.28680303\n",
      "Iteration 153, loss = 1.28642345\n",
      "Iteration 154, loss = 1.28599122\n",
      "Iteration 155, loss = 1.28557422\n",
      "Iteration 156, loss = 1.28514310\n",
      "Iteration 157, loss = 1.28496427\n",
      "Iteration 158, loss = 1.28451734\n",
      "Iteration 159, loss = 1.28413855\n",
      "Iteration 160, loss = 1.28384667\n",
      "Iteration 161, loss = 1.28335868\n",
      "Iteration 162, loss = 1.28312326\n",
      "Iteration 163, loss = 1.28269402\n",
      "Iteration 164, loss = 1.28239130\n",
      "Iteration 165, loss = 1.28204919\n",
      "Iteration 166, loss = 1.28173430\n",
      "Iteration 167, loss = 1.28147019\n",
      "Iteration 168, loss = 1.28115293\n",
      "Iteration 169, loss = 1.28085238\n",
      "Iteration 170, loss = 1.28049366\n",
      "Iteration 171, loss = 1.28023717\n",
      "Iteration 172, loss = 1.27990846\n",
      "Iteration 173, loss = 1.27955841\n",
      "Iteration 174, loss = 1.27930940\n",
      "Iteration 175, loss = 1.27903356\n",
      "Iteration 176, loss = 1.27875912\n",
      "Iteration 177, loss = 1.27854744\n",
      "Iteration 178, loss = 1.27836395\n",
      "Iteration 179, loss = 1.27799875\n",
      "Iteration 180, loss = 1.27787712\n",
      "Iteration 181, loss = 1.27754142\n",
      "Iteration 182, loss = 1.27726204\n",
      "Iteration 183, loss = 1.27706869\n",
      "Iteration 184, loss = 1.27682820\n",
      "Iteration 185, loss = 1.27654254\n",
      "Iteration 186, loss = 1.27629783\n",
      "Iteration 187, loss = 1.27603991\n",
      "Iteration 188, loss = 1.27583326\n",
      "Iteration 189, loss = 1.27566738\n",
      "Iteration 190, loss = 1.27532918\n",
      "Iteration 191, loss = 1.27513153\n",
      "Iteration 192, loss = 1.27489540\n",
      "Iteration 193, loss = 1.27470341\n",
      "Iteration 194, loss = 1.27452407\n",
      "Iteration 195, loss = 1.27430902\n",
      "Iteration 196, loss = 1.27403254\n",
      "Iteration 197, loss = 1.27382437\n",
      "Iteration 198, loss = 1.27359707\n",
      "Iteration 199, loss = 1.27338771\n",
      "Iteration 200, loss = 1.27319642\n",
      "Iteration 1, loss = 1.48386807\n",
      "Iteration 2, loss = 1.28175326\n",
      "Iteration 3, loss = 1.38783325\n",
      "Iteration 4, loss = 1.35246363\n",
      "Iteration 5, loss = 1.27628203\n",
      "Iteration 6, loss = 1.31313344\n",
      "Iteration 7, loss = 1.25416739\n",
      "Iteration 8, loss = 1.27327532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 9, loss = 1.27978845\n",
      "Iteration 10, loss = 1.28056214\n",
      "Iteration 11, loss = 1.26013895\n",
      "Iteration 12, loss = 1.28813517\n",
      "Iteration 13, loss = 1.27240872\n",
      "Iteration 14, loss = 1.28066533\n",
      "Iteration 15, loss = 1.25569645\n",
      "Iteration 16, loss = 1.26132973\n",
      "Iteration 17, loss = 1.26446140\n",
      "Iteration 18, loss = 1.26001824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.62596888\n",
      "Iteration 2, loss = 1.62199502\n",
      "Iteration 3, loss = 1.61773074\n",
      "Iteration 4, loss = 1.61399653\n",
      "Iteration 5, loss = 1.61019897\n",
      "Iteration 6, loss = 1.60580381\n",
      "Iteration 7, loss = 1.60156993\n",
      "Iteration 8, loss = 1.59756823\n",
      "Iteration 9, loss = 1.59279452\n",
      "Iteration 10, loss = 1.58784906\n",
      "Iteration 11, loss = 1.58129870\n",
      "Iteration 12, loss = 1.57569495\n",
      "Iteration 13, loss = 1.56725702\n",
      "Iteration 14, loss = 1.56056888\n",
      "Iteration 15, loss = 1.55252227\n",
      "Iteration 16, loss = 1.54276592\n",
      "Iteration 17, loss = 1.53452406\n",
      "Iteration 18, loss = 1.52426031\n",
      "Iteration 19, loss = 1.51454170\n",
      "Iteration 20, loss = 1.50255702\n",
      "Iteration 21, loss = 1.48765594\n",
      "Iteration 22, loss = 1.47385397\n",
      "Iteration 23, loss = 1.45712490\n",
      "Iteration 24, loss = 1.44171090\n",
      "Iteration 25, loss = 1.43109840\n",
      "Iteration 26, loss = 1.42179408\n",
      "Iteration 27, loss = 1.41524289\n",
      "Iteration 28, loss = 1.41245967\n",
      "Iteration 29, loss = 1.40922285\n",
      "Iteration 30, loss = 1.40632440\n",
      "Iteration 31, loss = 1.40370830\n",
      "Iteration 32, loss = 1.40206748\n",
      "Iteration 33, loss = 1.39953884\n",
      "Iteration 34, loss = 1.39743365\n",
      "Iteration 35, loss = 1.39597894\n",
      "Iteration 36, loss = 1.39340122\n",
      "Iteration 37, loss = 1.39153341\n",
      "Iteration 38, loss = 1.38941141\n",
      "Iteration 39, loss = 1.38746655\n",
      "Iteration 40, loss = 1.38537131\n",
      "Iteration 41, loss = 1.38372558\n",
      "Iteration 42, loss = 1.38171271\n",
      "Iteration 43, loss = 1.37964545\n",
      "Iteration 44, loss = 1.37754424\n",
      "Iteration 45, loss = 1.37567493\n",
      "Iteration 46, loss = 1.37380481\n",
      "Iteration 47, loss = 1.37094719\n",
      "Iteration 48, loss = 1.36857124\n",
      "Iteration 49, loss = 1.36646152\n",
      "Iteration 50, loss = 1.36345330\n",
      "Iteration 51, loss = 1.36205504\n",
      "Iteration 52, loss = 1.35906068\n",
      "Iteration 53, loss = 1.35696016\n",
      "Iteration 54, loss = 1.35451786\n",
      "Iteration 55, loss = 1.35261145\n",
      "Iteration 56, loss = 1.35062986\n",
      "Iteration 57, loss = 1.34860872\n",
      "Iteration 58, loss = 1.34639585\n",
      "Iteration 59, loss = 1.34460319\n",
      "Iteration 60, loss = 1.34260597\n",
      "Iteration 61, loss = 1.34087352\n",
      "Iteration 62, loss = 1.33928065\n",
      "Iteration 63, loss = 1.33784347\n",
      "Iteration 64, loss = 1.33613309\n",
      "Iteration 65, loss = 1.33491556\n",
      "Iteration 66, loss = 1.33316425\n",
      "Iteration 67, loss = 1.33167865\n",
      "Iteration 68, loss = 1.33035695\n",
      "Iteration 69, loss = 1.32882551\n",
      "Iteration 70, loss = 1.32758677\n",
      "Iteration 71, loss = 1.32656262\n",
      "Iteration 72, loss = 1.32435257\n",
      "Iteration 73, loss = 1.32318094\n",
      "Iteration 74, loss = 1.32177428\n",
      "Iteration 75, loss = 1.32033198\n",
      "Iteration 76, loss = 1.31899997\n",
      "Iteration 77, loss = 1.31754576\n",
      "Iteration 78, loss = 1.31625759\n",
      "Iteration 79, loss = 1.31495206\n",
      "Iteration 80, loss = 1.31369799\n",
      "Iteration 81, loss = 1.31232348\n",
      "Iteration 82, loss = 1.31084181\n",
      "Iteration 83, loss = 1.30973983\n",
      "Iteration 84, loss = 1.30834756\n",
      "Iteration 85, loss = 1.30704028\n",
      "Iteration 86, loss = 1.30562418\n",
      "Iteration 87, loss = 1.30422857\n",
      "Iteration 88, loss = 1.30310104\n",
      "Iteration 89, loss = 1.30169486\n",
      "Iteration 90, loss = 1.30048290\n",
      "Iteration 91, loss = 1.29904393\n",
      "Iteration 92, loss = 1.29765470\n",
      "Iteration 93, loss = 1.29631695\n",
      "Iteration 94, loss = 1.29524682\n",
      "Iteration 95, loss = 1.29399941\n",
      "Iteration 96, loss = 1.29248364\n",
      "Iteration 97, loss = 1.29122370\n",
      "Iteration 98, loss = 1.29013200\n",
      "Iteration 99, loss = 1.28906142\n",
      "Iteration 100, loss = 1.28739528\n",
      "Iteration 101, loss = 1.28610984\n",
      "Iteration 102, loss = 1.28523542\n",
      "Iteration 103, loss = 1.28392213\n",
      "Iteration 104, loss = 1.28260856\n",
      "Iteration 105, loss = 1.28151883\n",
      "Iteration 106, loss = 1.28034241\n",
      "Iteration 107, loss = 1.27916572\n",
      "Iteration 108, loss = 1.27796100\n",
      "Iteration 109, loss = 1.27673539\n",
      "Iteration 110, loss = 1.27549263\n",
      "Iteration 111, loss = 1.27451488\n",
      "Iteration 112, loss = 1.27297314\n",
      "Iteration 113, loss = 1.27176482\n",
      "Iteration 114, loss = 1.27051477\n",
      "Iteration 115, loss = 1.26949066\n",
      "Iteration 116, loss = 1.26815199\n",
      "Iteration 117, loss = 1.26707050\n",
      "Iteration 118, loss = 1.26584284\n",
      "Iteration 119, loss = 1.26450673\n",
      "Iteration 120, loss = 1.26332044\n",
      "Iteration 121, loss = 1.26237392\n",
      "Iteration 122, loss = 1.26113127\n",
      "Iteration 123, loss = 1.25989137\n",
      "Iteration 124, loss = 1.25884524\n",
      "Iteration 125, loss = 1.25753550\n",
      "Iteration 126, loss = 1.25646795\n",
      "Iteration 127, loss = 1.25519466\n",
      "Iteration 128, loss = 1.25407940\n",
      "Iteration 129, loss = 1.25295745\n",
      "Iteration 130, loss = 1.25183873\n",
      "Iteration 131, loss = 1.25077566\n",
      "Iteration 132, loss = 1.24969261\n",
      "Iteration 133, loss = 1.24854172\n",
      "Iteration 134, loss = 1.24739347\n",
      "Iteration 135, loss = 1.24660186\n",
      "Iteration 136, loss = 1.24539676\n",
      "Iteration 137, loss = 1.24431666\n",
      "Iteration 138, loss = 1.24339381\n",
      "Iteration 139, loss = 1.24214402\n",
      "Iteration 140, loss = 1.24112698\n",
      "Iteration 141, loss = 1.24005456\n",
      "Iteration 142, loss = 1.23912527\n",
      "Iteration 143, loss = 1.23800333\n",
      "Iteration 144, loss = 1.23686243\n",
      "Iteration 145, loss = 1.23587515\n",
      "Iteration 146, loss = 1.23475879\n",
      "Iteration 147, loss = 1.23375346\n",
      "Iteration 148, loss = 1.23285762\n",
      "Iteration 149, loss = 1.23182827\n",
      "Iteration 150, loss = 1.23082105\n",
      "Iteration 151, loss = 1.22990244\n",
      "Iteration 152, loss = 1.22882804\n",
      "Iteration 153, loss = 1.22768432\n",
      "Iteration 154, loss = 1.22681421\n",
      "Iteration 155, loss = 1.22599363\n",
      "Iteration 156, loss = 1.22498575\n",
      "Iteration 157, loss = 1.22394530\n",
      "Iteration 158, loss = 1.22297381\n",
      "Iteration 159, loss = 1.22208504\n",
      "Iteration 160, loss = 1.22099359\n",
      "Iteration 161, loss = 1.22008146\n",
      "Iteration 162, loss = 1.21902224\n",
      "Iteration 163, loss = 1.21813228\n",
      "Iteration 164, loss = 1.21733972\n",
      "Iteration 165, loss = 1.21629960\n",
      "Iteration 166, loss = 1.21533806\n",
      "Iteration 167, loss = 1.21428985\n",
      "Iteration 168, loss = 1.21337571\n",
      "Iteration 169, loss = 1.21255096\n",
      "Iteration 170, loss = 1.21138495\n",
      "Iteration 171, loss = 1.21027174\n",
      "Iteration 172, loss = 1.20913226\n",
      "Iteration 173, loss = 1.20805542\n",
      "Iteration 174, loss = 1.20658422\n",
      "Iteration 175, loss = 1.20499735\n",
      "Iteration 176, loss = 1.20325639\n",
      "Iteration 177, loss = 1.20041320\n",
      "Iteration 178, loss = 1.19928353\n",
      "Iteration 179, loss = 1.19804356\n",
      "Iteration 180, loss = 1.19726790\n",
      "Iteration 181, loss = 1.19580426\n",
      "Iteration 182, loss = 1.19521484\n",
      "Iteration 183, loss = 1.19170010\n",
      "Iteration 184, loss = 1.19060454\n",
      "Iteration 185, loss = 1.18413790\n",
      "Iteration 186, loss = 1.18340911\n",
      "Iteration 187, loss = 1.18252784\n",
      "Iteration 188, loss = 1.18151397\n",
      "Iteration 189, loss = 1.18069905\n",
      "Iteration 190, loss = 1.17967904\n",
      "Iteration 191, loss = 1.17879448\n",
      "Iteration 192, loss = 1.17784441\n",
      "Iteration 193, loss = 1.17677550\n",
      "Iteration 194, loss = 1.17581907\n",
      "Iteration 195, loss = 1.17499115\n",
      "Iteration 196, loss = 1.17400079\n",
      "Iteration 197, loss = 1.17317126\n",
      "Iteration 198, loss = 1.17231154\n",
      "Iteration 199, loss = 1.17134561\n",
      "Iteration 200, loss = 1.17064402\n",
      "Iteration 1, loss = 1.52196362\n",
      "Iteration 2, loss = 1.29549455\n",
      "Iteration 3, loss = 1.30415526\n",
      "Iteration 4, loss = 1.25808104\n",
      "Iteration 5, loss = 1.27043731\n",
      "Iteration 6, loss = 1.31697911\n",
      "Iteration 7, loss = 1.27351335\n",
      "Iteration 8, loss = 1.27065926\n",
      "Iteration 9, loss = 1.29709377\n",
      "Iteration 10, loss = 1.29298497\n",
      "Iteration 11, loss = 1.25671177\n",
      "Iteration 12, loss = 1.27068486\n",
      "Iteration 13, loss = 1.27898239\n",
      "Iteration 14, loss = 1.29021833\n",
      "Iteration 15, loss = 1.28933570\n",
      "Iteration 16, loss = 1.28282214\n",
      "Iteration 17, loss = 1.28802500\n",
      "Iteration 18, loss = 1.30483194\n",
      "Iteration 19, loss = 1.24171703\n",
      "Iteration 20, loss = 1.28817022\n",
      "Iteration 21, loss = 1.27906967\n",
      "Iteration 22, loss = 1.24923780\n",
      "Iteration 23, loss = 1.28579867\n",
      "Iteration 24, loss = 1.24888036\n",
      "Iteration 25, loss = 1.33807837\n",
      "Iteration 26, loss = 1.27430970\n",
      "Iteration 27, loss = 1.27215708\n",
      "Iteration 28, loss = 1.25953465\n",
      "Iteration 29, loss = 1.26933604\n",
      "Iteration 30, loss = 1.24953260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75989522\n",
      "Iteration 2, loss = 1.75493409\n",
      "Iteration 3, loss = 1.75026303\n",
      "Iteration 4, loss = 1.74559023\n",
      "Iteration 5, loss = 1.74092471\n",
      "Iteration 6, loss = 1.73619543\n",
      "Iteration 7, loss = 1.73168659\n",
      "Iteration 8, loss = 1.72660960\n",
      "Iteration 9, loss = 1.72200868\n",
      "Iteration 10, loss = 1.71695005\n",
      "Iteration 11, loss = 1.71222593\n",
      "Iteration 12, loss = 1.70697516\n",
      "Iteration 13, loss = 1.70225287\n",
      "Iteration 14, loss = 1.69794846\n",
      "Iteration 15, loss = 1.69355640\n",
      "Iteration 16, loss = 1.68924455\n",
      "Iteration 17, loss = 1.68481338\n",
      "Iteration 18, loss = 1.68075521\n",
      "Iteration 19, loss = 1.67680626\n",
      "Iteration 20, loss = 1.67290483\n",
      "Iteration 21, loss = 1.66917662\n",
      "Iteration 22, loss = 1.66529433\n",
      "Iteration 23, loss = 1.66172049\n",
      "Iteration 24, loss = 1.65795043\n",
      "Iteration 25, loss = 1.65432866\n",
      "Iteration 26, loss = 1.65073830\n",
      "Iteration 27, loss = 1.64729281\n",
      "Iteration 28, loss = 1.64403028\n",
      "Iteration 29, loss = 1.64043045\n",
      "Iteration 30, loss = 1.63717015\n",
      "Iteration 31, loss = 1.63395662\n",
      "Iteration 32, loss = 1.63052873\n",
      "Iteration 33, loss = 1.62703840\n",
      "Iteration 34, loss = 1.62394744\n",
      "Iteration 35, loss = 1.62051971\n",
      "Iteration 36, loss = 1.61739723\n",
      "Iteration 37, loss = 1.61405281\n",
      "Iteration 38, loss = 1.61112594\n",
      "Iteration 39, loss = 1.60762382\n",
      "Iteration 40, loss = 1.60461531\n",
      "Iteration 41, loss = 1.60183536\n",
      "Iteration 42, loss = 1.59856144\n",
      "Iteration 43, loss = 1.59572455\n",
      "Iteration 44, loss = 1.59269860\n",
      "Iteration 45, loss = 1.59005995\n",
      "Iteration 46, loss = 1.58681355\n",
      "Iteration 47, loss = 1.58426564\n",
      "Iteration 48, loss = 1.58140414\n",
      "Iteration 49, loss = 1.57856758\n",
      "Iteration 50, loss = 1.57569637\n",
      "Iteration 51, loss = 1.57292581\n",
      "Iteration 52, loss = 1.57017965\n",
      "Iteration 53, loss = 1.56737405\n",
      "Iteration 54, loss = 1.56499496\n",
      "Iteration 55, loss = 1.56217630\n",
      "Iteration 56, loss = 1.55957451\n",
      "Iteration 57, loss = 1.55696529\n",
      "Iteration 58, loss = 1.55438965\n",
      "Iteration 59, loss = 1.55182947\n",
      "Iteration 60, loss = 1.54942544\n",
      "Iteration 61, loss = 1.54696122\n",
      "Iteration 62, loss = 1.54470045\n",
      "Iteration 63, loss = 1.54218707\n",
      "Iteration 64, loss = 1.53990360\n",
      "Iteration 65, loss = 1.53767346\n",
      "Iteration 66, loss = 1.53530165\n",
      "Iteration 67, loss = 1.53297245\n",
      "Iteration 68, loss = 1.53068655\n",
      "Iteration 69, loss = 1.52841504\n",
      "Iteration 70, loss = 1.52624228\n",
      "Iteration 71, loss = 1.52384034\n",
      "Iteration 72, loss = 1.52183795\n",
      "Iteration 73, loss = 1.51933835\n",
      "Iteration 74, loss = 1.51739478\n",
      "Iteration 75, loss = 1.51524719\n",
      "Iteration 76, loss = 1.51290269\n",
      "Iteration 77, loss = 1.51085828\n",
      "Iteration 78, loss = 1.50871053\n",
      "Iteration 79, loss = 1.50671520\n",
      "Iteration 80, loss = 1.50454323\n",
      "Iteration 81, loss = 1.50260353\n",
      "Iteration 82, loss = 1.50054012\n",
      "Iteration 83, loss = 1.49829837\n",
      "Iteration 84, loss = 1.49630363\n",
      "Iteration 85, loss = 1.49438748\n",
      "Iteration 86, loss = 1.49238823\n",
      "Iteration 87, loss = 1.49041757\n",
      "Iteration 88, loss = 1.48844202\n",
      "Iteration 89, loss = 1.48650383\n",
      "Iteration 90, loss = 1.48468293\n",
      "Iteration 91, loss = 1.48285264\n",
      "Iteration 92, loss = 1.48086603\n",
      "Iteration 93, loss = 1.47916033\n",
      "Iteration 94, loss = 1.47735260\n",
      "Iteration 95, loss = 1.47548839\n",
      "Iteration 96, loss = 1.47369134\n",
      "Iteration 97, loss = 1.47219218\n",
      "Iteration 98, loss = 1.47025787\n",
      "Iteration 99, loss = 1.46870253\n",
      "Iteration 100, loss = 1.46685756\n",
      "Iteration 101, loss = 1.46516734\n",
      "Iteration 102, loss = 1.46334398\n",
      "Iteration 103, loss = 1.46163974\n",
      "Iteration 104, loss = 1.45995691\n",
      "Iteration 105, loss = 1.45823623\n",
      "Iteration 106, loss = 1.45647249\n",
      "Iteration 107, loss = 1.45471560\n",
      "Iteration 108, loss = 1.45308393\n",
      "Iteration 109, loss = 1.45133019\n",
      "Iteration 110, loss = 1.44963950\n",
      "Iteration 111, loss = 1.44810874\n",
      "Iteration 112, loss = 1.44633815\n",
      "Iteration 113, loss = 1.44490711\n",
      "Iteration 114, loss = 1.44330682\n",
      "Iteration 115, loss = 1.44160905\n",
      "Iteration 116, loss = 1.43999845\n",
      "Iteration 117, loss = 1.43840975\n",
      "Iteration 118, loss = 1.43687845\n",
      "Iteration 119, loss = 1.43525230\n",
      "Iteration 120, loss = 1.43388362\n",
      "Iteration 121, loss = 1.43218525\n",
      "Iteration 122, loss = 1.43058439\n",
      "Iteration 123, loss = 1.42908611\n",
      "Iteration 124, loss = 1.42749804\n",
      "Iteration 125, loss = 1.42598862\n",
      "Iteration 126, loss = 1.42440965\n",
      "Iteration 127, loss = 1.42287890\n",
      "Iteration 128, loss = 1.42149591\n",
      "Iteration 129, loss = 1.41998109\n",
      "Iteration 130, loss = 1.41860381\n",
      "Iteration 131, loss = 1.41727951\n",
      "Iteration 132, loss = 1.41561503\n",
      "Iteration 133, loss = 1.41403078\n",
      "Iteration 134, loss = 1.41296115\n",
      "Iteration 135, loss = 1.41133388\n",
      "Iteration 136, loss = 1.40990428\n",
      "Iteration 137, loss = 1.40850232\n",
      "Iteration 138, loss = 1.40717694\n",
      "Iteration 139, loss = 1.40579488\n",
      "Iteration 140, loss = 1.40452464\n",
      "Iteration 141, loss = 1.40293323\n",
      "Iteration 142, loss = 1.40159598\n",
      "Iteration 143, loss = 1.40020165\n",
      "Iteration 144, loss = 1.39872324\n",
      "Iteration 145, loss = 1.39752284\n",
      "Iteration 146, loss = 1.39595120\n",
      "Iteration 147, loss = 1.39464565\n",
      "Iteration 148, loss = 1.39338252\n",
      "Iteration 149, loss = 1.39208250\n",
      "Iteration 150, loss = 1.39079513\n",
      "Iteration 151, loss = 1.38963468\n",
      "Iteration 152, loss = 1.38824467\n",
      "Iteration 153, loss = 1.38706752\n",
      "Iteration 154, loss = 1.38568949\n",
      "Iteration 155, loss = 1.38436515\n",
      "Iteration 156, loss = 1.38306722\n",
      "Iteration 157, loss = 1.38194992\n",
      "Iteration 158, loss = 1.38075703\n",
      "Iteration 159, loss = 1.37951759\n",
      "Iteration 160, loss = 1.37842950\n",
      "Iteration 161, loss = 1.37717984\n",
      "Iteration 162, loss = 1.37584530\n",
      "Iteration 163, loss = 1.37462597\n",
      "Iteration 164, loss = 1.37347717\n",
      "Iteration 165, loss = 1.37216318\n",
      "Iteration 166, loss = 1.37084750\n",
      "Iteration 167, loss = 1.36971120\n",
      "Iteration 168, loss = 1.36837890\n",
      "Iteration 169, loss = 1.36727985\n",
      "Iteration 170, loss = 1.36611254\n",
      "Iteration 171, loss = 1.36483110\n",
      "Iteration 172, loss = 1.36374548\n",
      "Iteration 173, loss = 1.36260376\n",
      "Iteration 174, loss = 1.36150185\n",
      "Iteration 175, loss = 1.36017945\n",
      "Iteration 176, loss = 1.35910337\n",
      "Iteration 177, loss = 1.35788249\n",
      "Iteration 178, loss = 1.35685290\n",
      "Iteration 179, loss = 1.35564237\n",
      "Iteration 180, loss = 1.35452020\n",
      "Iteration 181, loss = 1.35342633\n",
      "Iteration 182, loss = 1.35244577\n",
      "Iteration 183, loss = 1.35104793\n",
      "Iteration 184, loss = 1.35003299\n",
      "Iteration 185, loss = 1.34902397\n",
      "Iteration 186, loss = 1.34784565\n",
      "Iteration 187, loss = 1.34684280\n",
      "Iteration 188, loss = 1.34594202\n",
      "Iteration 189, loss = 1.34469428\n",
      "Iteration 190, loss = 1.34381474\n",
      "Iteration 191, loss = 1.34270814\n",
      "Iteration 192, loss = 1.34173015\n",
      "Iteration 193, loss = 1.34068370\n",
      "Iteration 194, loss = 1.33972451\n",
      "Iteration 195, loss = 1.33858755\n",
      "Iteration 196, loss = 1.33772438\n",
      "Iteration 197, loss = 1.33669380\n",
      "Iteration 198, loss = 1.33570779\n",
      "Iteration 199, loss = 1.33478367\n",
      "Iteration 200, loss = 1.33383828\n",
      "Iteration 1, loss = 1.37892599\n",
      "Iteration 2, loss = 1.58735240\n",
      "Iteration 3, loss = 1.30490821\n",
      "Iteration 4, loss = 1.39836692\n",
      "Iteration 5, loss = 1.25496907\n",
      "Iteration 6, loss = 1.30419280\n",
      "Iteration 7, loss = 1.28359612\n",
      "Iteration 8, loss = 1.26440608\n",
      "Iteration 9, loss = 1.29562676\n",
      "Iteration 10, loss = 1.29599505\n",
      "Iteration 11, loss = 1.26638414\n",
      "Iteration 12, loss = 1.28975982\n",
      "Iteration 13, loss = 1.25336208\n",
      "Iteration 14, loss = 1.25618538\n",
      "Iteration 15, loss = 1.26397232\n",
      "Iteration 16, loss = 1.28255134\n",
      "Iteration 17, loss = 1.25533321\n",
      "Iteration 18, loss = 1.27746086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19, loss = 1.27541537\n",
      "Iteration 20, loss = 1.29061697\n",
      "Iteration 21, loss = 1.24317084\n",
      "Iteration 22, loss = 1.27842431\n",
      "Iteration 23, loss = 1.31720407\n",
      "Iteration 24, loss = 1.26958901\n",
      "Iteration 25, loss = 1.25067658\n",
      "Iteration 26, loss = 1.31385339\n",
      "Iteration 27, loss = 1.30181188\n",
      "Iteration 28, loss = 1.24822296\n",
      "Iteration 29, loss = 1.28997638\n",
      "Iteration 30, loss = 1.29043633\n",
      "Iteration 31, loss = 1.25792834\n",
      "Iteration 32, loss = 1.29663147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44206866\n",
      "Iteration 2, loss = 1.39618638\n",
      "Iteration 3, loss = 1.35634292\n",
      "Iteration 4, loss = 1.33486076\n",
      "Iteration 5, loss = 1.31866606\n",
      "Iteration 6, loss = 1.30588705\n",
      "Iteration 7, loss = 1.29698784\n",
      "Iteration 8, loss = 1.29142975\n",
      "Iteration 9, loss = 1.28643850\n",
      "Iteration 10, loss = 1.28299638\n",
      "Iteration 11, loss = 1.28083289\n",
      "Iteration 12, loss = 1.27788117\n",
      "Iteration 13, loss = 1.27630616\n",
      "Iteration 14, loss = 1.27503540\n",
      "Iteration 15, loss = 1.27516695\n",
      "Iteration 16, loss = 1.27370252\n",
      "Iteration 17, loss = 1.27257054\n",
      "Iteration 18, loss = 1.27258893\n",
      "Iteration 19, loss = 1.27169304\n",
      "Iteration 20, loss = 1.27162715\n",
      "Iteration 21, loss = 1.27117788\n",
      "Iteration 22, loss = 1.27126764\n",
      "Iteration 23, loss = 1.27073524\n",
      "Iteration 24, loss = 1.27078486\n",
      "Iteration 25, loss = 1.27039156\n",
      "Iteration 26, loss = 1.27021978\n",
      "Iteration 27, loss = 1.27055786\n",
      "Iteration 28, loss = 1.26990754\n",
      "Iteration 29, loss = 1.26973543\n",
      "Iteration 30, loss = 1.26993018\n",
      "Iteration 31, loss = 1.26966777\n",
      "Iteration 32, loss = 1.26976088\n",
      "Iteration 33, loss = 1.26943859\n",
      "Iteration 34, loss = 1.27011722\n",
      "Iteration 35, loss = 1.26964171\n",
      "Iteration 36, loss = 1.26900448\n",
      "Iteration 37, loss = 1.26945387\n",
      "Iteration 38, loss = 1.26883553\n",
      "Iteration 39, loss = 1.26887647\n",
      "Iteration 40, loss = 1.26886567\n",
      "Iteration 41, loss = 1.26899968\n",
      "Iteration 42, loss = 1.26834415\n",
      "Iteration 43, loss = 1.26836035\n",
      "Iteration 44, loss = 1.26832332\n",
      "Iteration 45, loss = 1.26882741\n",
      "Iteration 46, loss = 1.26845337\n",
      "Iteration 47, loss = 1.26803370\n",
      "Iteration 48, loss = 1.26821296\n",
      "Iteration 49, loss = 1.26763832\n",
      "Iteration 50, loss = 1.26765674\n",
      "Iteration 51, loss = 1.26761645\n",
      "Iteration 52, loss = 1.26742361\n",
      "Iteration 53, loss = 1.26740873\n",
      "Iteration 54, loss = 1.26791897\n",
      "Iteration 55, loss = 1.26656530\n",
      "Iteration 56, loss = 1.26716920\n",
      "Iteration 57, loss = 1.26687745\n",
      "Iteration 58, loss = 1.26691496\n",
      "Iteration 59, loss = 1.26720858\n",
      "Iteration 60, loss = 1.26678895\n",
      "Iteration 61, loss = 1.26601094\n",
      "Iteration 62, loss = 1.26636665\n",
      "Iteration 63, loss = 1.26610684\n",
      "Iteration 64, loss = 1.26843326\n",
      "Iteration 65, loss = 1.26542381\n",
      "Iteration 66, loss = 1.26557469\n",
      "Iteration 67, loss = 1.26565219\n",
      "Iteration 68, loss = 1.26553152\n",
      "Iteration 69, loss = 1.26511342\n",
      "Iteration 70, loss = 1.26516624\n",
      "Iteration 71, loss = 1.26628054\n",
      "Iteration 72, loss = 1.26477246\n",
      "Iteration 73, loss = 1.26439276\n",
      "Iteration 74, loss = 1.26562522\n",
      "Iteration 75, loss = 1.26435860\n",
      "Iteration 76, loss = 1.26389771\n",
      "Iteration 77, loss = 1.26482093\n",
      "Iteration 78, loss = 1.26355757\n",
      "Iteration 79, loss = 1.26392748\n",
      "Iteration 80, loss = 1.26405173\n",
      "Iteration 81, loss = 1.26325904\n",
      "Iteration 82, loss = 1.26300497\n",
      "Iteration 83, loss = 1.26282506\n",
      "Iteration 84, loss = 1.26260071\n",
      "Iteration 85, loss = 1.26287805\n",
      "Iteration 86, loss = 1.26263986\n",
      "Iteration 87, loss = 1.26237266\n",
      "Iteration 88, loss = 1.26240532\n",
      "Iteration 89, loss = 1.26287230\n",
      "Iteration 90, loss = 1.26244502\n",
      "Iteration 91, loss = 1.26229470\n",
      "Iteration 92, loss = 1.26260284\n",
      "Iteration 93, loss = 1.26215232\n",
      "Iteration 94, loss = 1.26260813\n",
      "Iteration 95, loss = 1.26190244\n",
      "Iteration 96, loss = 1.26231684\n",
      "Iteration 97, loss = 1.26178988\n",
      "Iteration 98, loss = 1.26204529\n",
      "Iteration 99, loss = 1.26112762\n",
      "Iteration 100, loss = 1.26054917\n",
      "Iteration 101, loss = 1.26161371\n",
      "Iteration 102, loss = 1.26010283\n",
      "Iteration 103, loss = 1.26132478\n",
      "Iteration 104, loss = 1.25942766\n",
      "Iteration 105, loss = 1.26069266\n",
      "Iteration 106, loss = 1.25896560\n",
      "Iteration 107, loss = 1.26007746\n",
      "Iteration 108, loss = 1.25842311\n",
      "Iteration 109, loss = 1.25747633\n",
      "Iteration 110, loss = 1.25861400\n",
      "Iteration 111, loss = 1.26163429\n",
      "Iteration 112, loss = 1.25851821\n",
      "Iteration 113, loss = 1.25731174\n",
      "Iteration 114, loss = 1.25790732\n",
      "Iteration 115, loss = 1.25830248\n",
      "Iteration 116, loss = 1.25586114\n",
      "Iteration 117, loss = 1.25631269\n",
      "Iteration 118, loss = 1.25939554\n",
      "Iteration 119, loss = 1.25562921\n",
      "Iteration 120, loss = 1.25498579\n",
      "Iteration 121, loss = 1.25513827\n",
      "Iteration 122, loss = 1.25534567\n",
      "Iteration 123, loss = 1.25809457\n",
      "Iteration 124, loss = 1.25578960\n",
      "Iteration 125, loss = 1.25643637\n",
      "Iteration 126, loss = 1.25481922\n",
      "Iteration 127, loss = 1.25533487\n",
      "Iteration 128, loss = 1.25558898\n",
      "Iteration 129, loss = 1.25483557\n",
      "Iteration 130, loss = 1.25394699\n",
      "Iteration 131, loss = 1.25288831\n",
      "Iteration 132, loss = 1.25320785\n",
      "Iteration 133, loss = 1.25345721\n",
      "Iteration 134, loss = 1.25347900\n",
      "Iteration 135, loss = 1.25399515\n",
      "Iteration 136, loss = 1.25286639\n",
      "Iteration 137, loss = 1.25369955\n",
      "Iteration 138, loss = 1.25310561\n",
      "Iteration 139, loss = 1.25379995\n",
      "Iteration 140, loss = 1.25514649\n",
      "Iteration 141, loss = 1.25422563\n",
      "Iteration 142, loss = 1.25237918\n",
      "Iteration 143, loss = 1.25407563\n",
      "Iteration 144, loss = 1.25022945\n",
      "Iteration 145, loss = 1.25101186\n",
      "Iteration 146, loss = 1.25262829\n",
      "Iteration 147, loss = 1.25051216\n",
      "Iteration 148, loss = 1.25052296\n",
      "Iteration 149, loss = 1.25198133\n",
      "Iteration 150, loss = 1.25298971\n",
      "Iteration 151, loss = 1.25038141\n",
      "Iteration 152, loss = 1.24975930\n",
      "Iteration 153, loss = 1.24885285\n",
      "Iteration 154, loss = 1.25091207\n",
      "Iteration 155, loss = 1.24941994\n",
      "Iteration 156, loss = 1.24949312\n",
      "Iteration 157, loss = 1.24983514\n",
      "Iteration 158, loss = 1.25118211\n",
      "Iteration 159, loss = 1.24874949\n",
      "Iteration 160, loss = 1.24953682\n",
      "Iteration 161, loss = 1.24822693\n",
      "Iteration 162, loss = 1.24822464\n",
      "Iteration 163, loss = 1.24775765\n",
      "Iteration 164, loss = 1.24988694\n",
      "Iteration 165, loss = 1.24893629\n",
      "Iteration 166, loss = 1.24773066\n",
      "Iteration 167, loss = 1.24922758\n",
      "Iteration 168, loss = 1.24920817\n",
      "Iteration 169, loss = 1.24745484\n",
      "Iteration 170, loss = 1.24627647\n",
      "Iteration 171, loss = 1.24872078\n",
      "Iteration 172, loss = 1.24884778\n",
      "Iteration 173, loss = 1.24642547\n",
      "Iteration 174, loss = 1.24552689\n",
      "Iteration 175, loss = 1.24814212\n",
      "Iteration 176, loss = 1.24654009\n",
      "Iteration 177, loss = 1.24551048\n",
      "Iteration 178, loss = 1.24556916\n",
      "Iteration 179, loss = 1.24708600\n",
      "Iteration 180, loss = 1.24532489\n",
      "Iteration 181, loss = 1.24629104\n",
      "Iteration 182, loss = 1.24481804\n",
      "Iteration 183, loss = 1.24565658\n",
      "Iteration 184, loss = 1.24539956\n",
      "Iteration 185, loss = 1.24452856\n",
      "Iteration 186, loss = 1.24388680\n",
      "Iteration 187, loss = 1.24522376\n",
      "Iteration 188, loss = 1.24372603\n",
      "Iteration 189, loss = 1.24531660\n",
      "Iteration 190, loss = 1.24448717\n",
      "Iteration 191, loss = 1.24673842\n",
      "Iteration 192, loss = 1.24595027\n",
      "Iteration 193, loss = 1.24339994\n",
      "Iteration 194, loss = 1.24335517\n",
      "Iteration 195, loss = 1.24338243\n",
      "Iteration 196, loss = 1.24337647\n",
      "Iteration 197, loss = 1.24425434\n",
      "Iteration 198, loss = 1.24283724\n",
      "Iteration 199, loss = 1.24271416\n",
      "Iteration 200, loss = 1.24289442\n",
      "Iteration 1, loss = 1.92394202\n",
      "Iteration 2, loss = 1.92182074\n",
      "Iteration 3, loss = 1.91988528\n",
      "Iteration 4, loss = 1.91794974\n",
      "Iteration 5, loss = 1.91573551\n",
      "Iteration 6, loss = 1.91389872\n",
      "Iteration 7, loss = 1.91199045\n",
      "Iteration 8, loss = 1.90999522\n",
      "Iteration 9, loss = 1.90814648\n",
      "Iteration 10, loss = 1.90623307\n",
      "Iteration 11, loss = 1.90413404\n",
      "Iteration 12, loss = 1.90222264\n",
      "Iteration 13, loss = 1.90011114\n",
      "Iteration 14, loss = 1.89810148\n",
      "Iteration 15, loss = 1.89611467\n",
      "Iteration 16, loss = 1.89399036\n",
      "Iteration 17, loss = 1.89184995\n",
      "Iteration 18, loss = 1.88976551\n",
      "Iteration 19, loss = 1.88779500\n",
      "Iteration 20, loss = 1.88564939\n",
      "Iteration 21, loss = 1.88360695\n",
      "Iteration 22, loss = 1.88149850\n",
      "Iteration 23, loss = 1.87954813\n",
      "Iteration 24, loss = 1.87773090\n",
      "Iteration 25, loss = 1.87559666\n",
      "Iteration 26, loss = 1.87368436\n",
      "Iteration 27, loss = 1.87193013\n",
      "Iteration 28, loss = 1.86990110\n",
      "Iteration 29, loss = 1.86804157\n",
      "Iteration 30, loss = 1.86614521\n",
      "Iteration 31, loss = 1.86429572\n",
      "Iteration 32, loss = 1.86241064\n",
      "Iteration 33, loss = 1.86044440\n",
      "Iteration 34, loss = 1.85864627\n",
      "Iteration 35, loss = 1.85668883\n",
      "Iteration 36, loss = 1.85486340\n",
      "Iteration 37, loss = 1.85306137\n",
      "Iteration 38, loss = 1.85123190\n",
      "Iteration 39, loss = 1.84942134\n",
      "Iteration 40, loss = 1.84766548\n",
      "Iteration 41, loss = 1.84582883\n",
      "Iteration 42, loss = 1.84397081\n",
      "Iteration 43, loss = 1.84227587\n",
      "Iteration 44, loss = 1.84057790\n",
      "Iteration 45, loss = 1.83860612\n",
      "Iteration 46, loss = 1.83685124\n",
      "Iteration 47, loss = 1.83521638\n",
      "Iteration 48, loss = 1.83345415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 1.83180241\n",
      "Iteration 50, loss = 1.82999018\n",
      "Iteration 51, loss = 1.82829962\n",
      "Iteration 52, loss = 1.82652119\n",
      "Iteration 53, loss = 1.82484924\n",
      "Iteration 54, loss = 1.82311119\n",
      "Iteration 55, loss = 1.82132676\n",
      "Iteration 56, loss = 1.81966772\n",
      "Iteration 57, loss = 1.81802137\n",
      "Iteration 58, loss = 1.81638524\n",
      "Iteration 59, loss = 1.81476208\n",
      "Iteration 60, loss = 1.81314942\n",
      "Iteration 61, loss = 1.81152400\n",
      "Iteration 62, loss = 1.81000203\n",
      "Iteration 63, loss = 1.80854450\n",
      "Iteration 64, loss = 1.80683164\n",
      "Iteration 65, loss = 1.80539928\n",
      "Iteration 66, loss = 1.80386500\n",
      "Iteration 67, loss = 1.80231332\n",
      "Iteration 68, loss = 1.80084177\n",
      "Iteration 69, loss = 1.79936514\n",
      "Iteration 70, loss = 1.79786594\n",
      "Iteration 71, loss = 1.79636555\n",
      "Iteration 72, loss = 1.79479022\n",
      "Iteration 73, loss = 1.79336506\n",
      "Iteration 74, loss = 1.79195519\n",
      "Iteration 75, loss = 1.79055696\n",
      "Iteration 76, loss = 1.78901273\n",
      "Iteration 77, loss = 1.78755619\n",
      "Iteration 78, loss = 1.78613809\n",
      "Iteration 79, loss = 1.78464527\n",
      "Iteration 80, loss = 1.78319013\n",
      "Iteration 81, loss = 1.78174545\n",
      "Iteration 82, loss = 1.78034018\n",
      "Iteration 83, loss = 1.77887119\n",
      "Iteration 84, loss = 1.77750430\n",
      "Iteration 85, loss = 1.77616215\n",
      "Iteration 86, loss = 1.77468225\n",
      "Iteration 87, loss = 1.77337342\n",
      "Iteration 88, loss = 1.77191226\n",
      "Iteration 89, loss = 1.77061197\n",
      "Iteration 90, loss = 1.76902611\n",
      "Iteration 91, loss = 1.76766525\n",
      "Iteration 92, loss = 1.76623601\n",
      "Iteration 93, loss = 1.76483608\n",
      "Iteration 94, loss = 1.76344368\n",
      "Iteration 95, loss = 1.76213376\n",
      "Iteration 96, loss = 1.76075703\n",
      "Iteration 97, loss = 1.75946049\n",
      "Iteration 98, loss = 1.75808916\n",
      "Iteration 99, loss = 1.75676145\n",
      "Iteration 100, loss = 1.75541319\n",
      "Iteration 101, loss = 1.75401008\n",
      "Iteration 102, loss = 1.75268829\n",
      "Iteration 103, loss = 1.75144084\n",
      "Iteration 104, loss = 1.74984776\n",
      "Iteration 105, loss = 1.74853875\n",
      "Iteration 106, loss = 1.74702391\n",
      "Iteration 107, loss = 1.74575624\n",
      "Iteration 108, loss = 1.74436125\n",
      "Iteration 109, loss = 1.74299296\n",
      "Iteration 110, loss = 1.74169401\n",
      "Iteration 111, loss = 1.74029319\n",
      "Iteration 112, loss = 1.73894957\n",
      "Iteration 113, loss = 1.73761823\n",
      "Iteration 114, loss = 1.73637442\n",
      "Iteration 115, loss = 1.73489618\n",
      "Iteration 116, loss = 1.73368726\n",
      "Iteration 117, loss = 1.73214816\n",
      "Iteration 118, loss = 1.73086731\n",
      "Iteration 119, loss = 1.72950901\n",
      "Iteration 120, loss = 1.72807110\n",
      "Iteration 121, loss = 1.72675513\n",
      "Iteration 122, loss = 1.72533644\n",
      "Iteration 123, loss = 1.72412888\n",
      "Iteration 124, loss = 1.72272265\n",
      "Iteration 125, loss = 1.72137526\n",
      "Iteration 126, loss = 1.72005415\n",
      "Iteration 127, loss = 1.71878460\n",
      "Iteration 128, loss = 1.71738463\n",
      "Iteration 129, loss = 1.71607343\n",
      "Iteration 130, loss = 1.71470259\n",
      "Iteration 131, loss = 1.71336829\n",
      "Iteration 132, loss = 1.71212569\n",
      "Iteration 133, loss = 1.71078422\n",
      "Iteration 134, loss = 1.70947742\n",
      "Iteration 135, loss = 1.70825744\n",
      "Iteration 136, loss = 1.70692213\n",
      "Iteration 137, loss = 1.70555137\n",
      "Iteration 138, loss = 1.70412266\n",
      "Iteration 139, loss = 1.70283862\n",
      "Iteration 140, loss = 1.70145451\n",
      "Iteration 141, loss = 1.70012915\n",
      "Iteration 142, loss = 1.69879027\n",
      "Iteration 143, loss = 1.69739814\n",
      "Iteration 144, loss = 1.69602727\n",
      "Iteration 145, loss = 1.69461886\n",
      "Iteration 146, loss = 1.69335443\n",
      "Iteration 147, loss = 1.69190594\n",
      "Iteration 148, loss = 1.69075981\n",
      "Iteration 149, loss = 1.68938574\n",
      "Iteration 150, loss = 1.68799813\n",
      "Iteration 151, loss = 1.68674527\n",
      "Iteration 152, loss = 1.68550051\n",
      "Iteration 153, loss = 1.68417981\n",
      "Iteration 154, loss = 1.68276977\n",
      "Iteration 155, loss = 1.68154265\n",
      "Iteration 156, loss = 1.68005659\n",
      "Iteration 157, loss = 1.67875785\n",
      "Iteration 158, loss = 1.67725170\n",
      "Iteration 159, loss = 1.67581084\n",
      "Iteration 160, loss = 1.67443186\n",
      "Iteration 161, loss = 1.67296720\n",
      "Iteration 162, loss = 1.67140708\n",
      "Iteration 163, loss = 1.66989417\n",
      "Iteration 164, loss = 1.66832623\n",
      "Iteration 165, loss = 1.66672979\n",
      "Iteration 166, loss = 1.66517705\n",
      "Iteration 167, loss = 1.66393406\n",
      "Iteration 168, loss = 1.66209556\n",
      "Iteration 169, loss = 1.66054240\n",
      "Iteration 170, loss = 1.65863270\n",
      "Iteration 171, loss = 1.65678409\n",
      "Iteration 172, loss = 1.65518553\n",
      "Iteration 173, loss = 1.65328070\n",
      "Iteration 174, loss = 1.65178561\n",
      "Iteration 175, loss = 1.65032688\n",
      "Iteration 176, loss = 1.64882709\n",
      "Iteration 177, loss = 1.64740641\n",
      "Iteration 178, loss = 1.64600259\n",
      "Iteration 179, loss = 1.64458979\n",
      "Iteration 180, loss = 1.64327174\n",
      "Iteration 181, loss = 1.64176407\n",
      "Iteration 182, loss = 1.64042036\n",
      "Iteration 183, loss = 1.63889824\n",
      "Iteration 184, loss = 1.63741256\n",
      "Iteration 185, loss = 1.63603955\n",
      "Iteration 186, loss = 1.63456212\n",
      "Iteration 187, loss = 1.63303388\n",
      "Iteration 188, loss = 1.63159994\n",
      "Iteration 189, loss = 1.63019601\n",
      "Iteration 190, loss = 1.62877626\n",
      "Iteration 191, loss = 1.62731606\n",
      "Iteration 192, loss = 1.62588871\n",
      "Iteration 193, loss = 1.62455926\n",
      "Iteration 194, loss = 1.62320145\n",
      "Iteration 195, loss = 1.62162873\n",
      "Iteration 196, loss = 1.62018598\n",
      "Iteration 197, loss = 1.61884055\n",
      "Iteration 198, loss = 1.61739264\n",
      "Iteration 199, loss = 1.61595056\n",
      "Iteration 200, loss = 1.61447685\n",
      "Iteration 1, loss = 1.49176081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.32668941\n",
      "Iteration 3, loss = 1.31955352\n",
      "Iteration 4, loss = 1.31882770\n",
      "Iteration 5, loss = 1.31520778\n",
      "Iteration 6, loss = 1.25639985\n",
      "Iteration 7, loss = 1.26938715\n",
      "Iteration 8, loss = 1.28897192\n",
      "Iteration 9, loss = 1.25419502\n",
      "Iteration 10, loss = 1.26153240\n",
      "Iteration 11, loss = 1.27357703\n",
      "Iteration 12, loss = 1.25521410\n",
      "Iteration 13, loss = 1.27468941\n",
      "Iteration 14, loss = 1.26621082\n",
      "Iteration 15, loss = 1.27256827\n",
      "Iteration 16, loss = 1.26759209\n",
      "Iteration 17, loss = 1.25369335\n",
      "Iteration 18, loss = 1.25025188\n",
      "Iteration 19, loss = 1.25656640\n",
      "Iteration 20, loss = 1.26222567\n",
      "Iteration 21, loss = 1.25649792\n",
      "Iteration 22, loss = 1.26518735\n",
      "Iteration 23, loss = 1.25695628\n",
      "Iteration 24, loss = 1.25894337\n",
      "Iteration 25, loss = 1.25141034\n",
      "Iteration 26, loss = 1.25555483\n",
      "Iteration 27, loss = 1.25663493\n",
      "Iteration 28, loss = 1.26340446\n",
      "Iteration 29, loss = 1.25157730\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49637787\n",
      "Iteration 2, loss = 1.49478419\n",
      "Iteration 3, loss = 1.49310155\n",
      "Iteration 4, loss = 1.49156935\n",
      "Iteration 5, loss = 1.49010407\n",
      "Iteration 6, loss = 1.48846525\n",
      "Iteration 7, loss = 1.48718876\n",
      "Iteration 8, loss = 1.48569237\n",
      "Iteration 9, loss = 1.48444937\n",
      "Iteration 10, loss = 1.48303979\n",
      "Iteration 11, loss = 1.48179004\n",
      "Iteration 12, loss = 1.48034130\n",
      "Iteration 13, loss = 1.47911278\n",
      "Iteration 14, loss = 1.47792592\n",
      "Iteration 15, loss = 1.47668433\n",
      "Iteration 16, loss = 1.47533552\n",
      "Iteration 17, loss = 1.47403903\n",
      "Iteration 18, loss = 1.47295190\n",
      "Iteration 19, loss = 1.47158217\n",
      "Iteration 20, loss = 1.47036824\n",
      "Iteration 21, loss = 1.46909354\n",
      "Iteration 22, loss = 1.46773882\n",
      "Iteration 23, loss = 1.46665767\n",
      "Iteration 24, loss = 1.46546844\n",
      "Iteration 25, loss = 1.46418685\n",
      "Iteration 26, loss = 1.46309853\n",
      "Iteration 27, loss = 1.46190116\n",
      "Iteration 28, loss = 1.46072931\n",
      "Iteration 29, loss = 1.45961271\n",
      "Iteration 30, loss = 1.45841484\n",
      "Iteration 31, loss = 1.45731065\n",
      "Iteration 32, loss = 1.45624240\n",
      "Iteration 33, loss = 1.45515172\n",
      "Iteration 34, loss = 1.45408107\n",
      "Iteration 35, loss = 1.45306845\n",
      "Iteration 36, loss = 1.45201116\n",
      "Iteration 37, loss = 1.45091149\n",
      "Iteration 38, loss = 1.44982148\n",
      "Iteration 39, loss = 1.44885474\n",
      "Iteration 40, loss = 1.44770853\n",
      "Iteration 41, loss = 1.44666609\n",
      "Iteration 42, loss = 1.44560065\n",
      "Iteration 43, loss = 1.44457898\n",
      "Iteration 44, loss = 1.44353494\n",
      "Iteration 45, loss = 1.44250061\n",
      "Iteration 46, loss = 1.44144908\n",
      "Iteration 47, loss = 1.44043786\n",
      "Iteration 48, loss = 1.43944731\n",
      "Iteration 49, loss = 1.43844967\n",
      "Iteration 50, loss = 1.43745090\n",
      "Iteration 51, loss = 1.43645516\n",
      "Iteration 52, loss = 1.43554765\n",
      "Iteration 53, loss = 1.43463715\n",
      "Iteration 54, loss = 1.43371302\n",
      "Iteration 55, loss = 1.43268330\n",
      "Iteration 56, loss = 1.43179679\n",
      "Iteration 57, loss = 1.43083155\n",
      "Iteration 58, loss = 1.42992221\n",
      "Iteration 59, loss = 1.42890299\n",
      "Iteration 60, loss = 1.42801158\n",
      "Iteration 61, loss = 1.42710642\n",
      "Iteration 62, loss = 1.42616502\n",
      "Iteration 63, loss = 1.42520136\n",
      "Iteration 64, loss = 1.42429804\n",
      "Iteration 65, loss = 1.42346776\n",
      "Iteration 66, loss = 1.42260691\n",
      "Iteration 67, loss = 1.42164589\n",
      "Iteration 68, loss = 1.42087936\n",
      "Iteration 69, loss = 1.42002548\n",
      "Iteration 70, loss = 1.41917284\n",
      "Iteration 71, loss = 1.41839948\n",
      "Iteration 72, loss = 1.41757262\n",
      "Iteration 73, loss = 1.41685918\n",
      "Iteration 74, loss = 1.41604216\n",
      "Iteration 75, loss = 1.41518049\n",
      "Iteration 76, loss = 1.41446645\n",
      "Iteration 77, loss = 1.41362085\n",
      "Iteration 78, loss = 1.41285103\n",
      "Iteration 79, loss = 1.41216281\n",
      "Iteration 80, loss = 1.41130688\n",
      "Iteration 81, loss = 1.41057132\n",
      "Iteration 82, loss = 1.40982598\n",
      "Iteration 83, loss = 1.40907494\n",
      "Iteration 84, loss = 1.40832966\n",
      "Iteration 85, loss = 1.40758543\n",
      "Iteration 86, loss = 1.40685579\n",
      "Iteration 87, loss = 1.40607007\n",
      "Iteration 88, loss = 1.40533385\n",
      "Iteration 89, loss = 1.40458554\n",
      "Iteration 90, loss = 1.40385306\n",
      "Iteration 91, loss = 1.40311772\n",
      "Iteration 92, loss = 1.40235072\n",
      "Iteration 93, loss = 1.40166864\n",
      "Iteration 94, loss = 1.40091574\n",
      "Iteration 95, loss = 1.40026286\n",
      "Iteration 96, loss = 1.39951181\n",
      "Iteration 97, loss = 1.39878660\n",
      "Iteration 98, loss = 1.39805865\n",
      "Iteration 99, loss = 1.39730957\n",
      "Iteration 100, loss = 1.39670484\n",
      "Iteration 101, loss = 1.39591224\n",
      "Iteration 102, loss = 1.39520167\n",
      "Iteration 103, loss = 1.39454337\n",
      "Iteration 104, loss = 1.39381080\n",
      "Iteration 105, loss = 1.39309696\n",
      "Iteration 106, loss = 1.39245100\n",
      "Iteration 107, loss = 1.39169469\n",
      "Iteration 108, loss = 1.39100982\n",
      "Iteration 109, loss = 1.39040384\n",
      "Iteration 110, loss = 1.38966832\n",
      "Iteration 111, loss = 1.38907105\n",
      "Iteration 112, loss = 1.38829484\n",
      "Iteration 113, loss = 1.38770464\n",
      "Iteration 114, loss = 1.38697973\n",
      "Iteration 115, loss = 1.38637881\n",
      "Iteration 116, loss = 1.38569097\n",
      "Iteration 117, loss = 1.38509348\n",
      "Iteration 118, loss = 1.38442431\n",
      "Iteration 119, loss = 1.38382890\n",
      "Iteration 120, loss = 1.38318738\n",
      "Iteration 121, loss = 1.38257470\n",
      "Iteration 122, loss = 1.38188456\n",
      "Iteration 123, loss = 1.38129162\n",
      "Iteration 124, loss = 1.38066891\n",
      "Iteration 125, loss = 1.38000103\n",
      "Iteration 126, loss = 1.37944785\n",
      "Iteration 127, loss = 1.37882546\n",
      "Iteration 128, loss = 1.37830031\n",
      "Iteration 129, loss = 1.37765717\n",
      "Iteration 130, loss = 1.37706247\n",
      "Iteration 131, loss = 1.37646958\n",
      "Iteration 132, loss = 1.37589780\n",
      "Iteration 133, loss = 1.37527969\n",
      "Iteration 134, loss = 1.37478032\n",
      "Iteration 135, loss = 1.37408093\n",
      "Iteration 136, loss = 1.37349278\n",
      "Iteration 137, loss = 1.37288973\n",
      "Iteration 138, loss = 1.37223032\n",
      "Iteration 139, loss = 1.37166000\n",
      "Iteration 140, loss = 1.37102034\n",
      "Iteration 141, loss = 1.37048188\n",
      "Iteration 142, loss = 1.36988962\n",
      "Iteration 143, loss = 1.36922414\n",
      "Iteration 144, loss = 1.36864121\n",
      "Iteration 145, loss = 1.36807271\n",
      "Iteration 146, loss = 1.36747226\n",
      "Iteration 147, loss = 1.36685889\n",
      "Iteration 148, loss = 1.36633895\n",
      "Iteration 149, loss = 1.36578226\n",
      "Iteration 150, loss = 1.36519422\n",
      "Iteration 151, loss = 1.36478583\n",
      "Iteration 152, loss = 1.36415561\n",
      "Iteration 153, loss = 1.36357531\n",
      "Iteration 154, loss = 1.36306097\n",
      "Iteration 155, loss = 1.36253289\n",
      "Iteration 156, loss = 1.36198855\n",
      "Iteration 157, loss = 1.36143188\n",
      "Iteration 158, loss = 1.36089778\n",
      "Iteration 159, loss = 1.36040285\n",
      "Iteration 160, loss = 1.35974933\n",
      "Iteration 161, loss = 1.35921838\n",
      "Iteration 162, loss = 1.35867364\n",
      "Iteration 163, loss = 1.35811611\n",
      "Iteration 164, loss = 1.35760022\n",
      "Iteration 165, loss = 1.35698566\n",
      "Iteration 166, loss = 1.35653766\n",
      "Iteration 167, loss = 1.35595472\n",
      "Iteration 168, loss = 1.35540099\n",
      "Iteration 169, loss = 1.35483270\n",
      "Iteration 170, loss = 1.35430123\n",
      "Iteration 171, loss = 1.35373113\n",
      "Iteration 172, loss = 1.35316017\n",
      "Iteration 173, loss = 1.35266732\n",
      "Iteration 174, loss = 1.35210095\n",
      "Iteration 175, loss = 1.35159478\n",
      "Iteration 176, loss = 1.35099113\n",
      "Iteration 177, loss = 1.35053952\n",
      "Iteration 178, loss = 1.34990312\n",
      "Iteration 179, loss = 1.34943014\n",
      "Iteration 180, loss = 1.34897872\n",
      "Iteration 181, loss = 1.34850897\n",
      "Iteration 182, loss = 1.34790790\n",
      "Iteration 183, loss = 1.34744008\n",
      "Iteration 184, loss = 1.34696637\n",
      "Iteration 185, loss = 1.34650350\n",
      "Iteration 186, loss = 1.34596815\n",
      "Iteration 187, loss = 1.34550084\n",
      "Iteration 188, loss = 1.34503561\n",
      "Iteration 189, loss = 1.34454387\n",
      "Iteration 190, loss = 1.34405363\n",
      "Iteration 191, loss = 1.34352978\n",
      "Iteration 192, loss = 1.34303544\n",
      "Iteration 193, loss = 1.34260203\n",
      "Iteration 194, loss = 1.34206851\n",
      "Iteration 195, loss = 1.34164044\n",
      "Iteration 196, loss = 1.34115331\n",
      "Iteration 197, loss = 1.34074291\n",
      "Iteration 198, loss = 1.34022767\n",
      "Iteration 199, loss = 1.33979063\n",
      "Iteration 200, loss = 1.33938744\n",
      "Iteration 1, loss = 1.57532439\n",
      "Iteration 2, loss = 1.31963028\n",
      "Iteration 3, loss = 1.30223885\n",
      "Iteration 4, loss = 1.28124415\n",
      "Iteration 5, loss = 1.27291671\n",
      "Iteration 6, loss = 1.27183305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.30012237\n",
      "Iteration 8, loss = 1.26258537\n",
      "Iteration 9, loss = 1.28328358\n",
      "Iteration 10, loss = 1.28023117\n",
      "Iteration 11, loss = 1.28521547\n",
      "Iteration 12, loss = 1.26707373\n",
      "Iteration 13, loss = 1.26768808\n",
      "Iteration 14, loss = 1.27586003\n",
      "Iteration 15, loss = 1.27165926\n",
      "Iteration 16, loss = 1.27524216\n",
      "Iteration 17, loss = 1.29820084\n",
      "Iteration 18, loss = 1.26975530\n",
      "Iteration 19, loss = 1.28070661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.95274506\n",
      "Iteration 2, loss = 1.95031659\n",
      "Iteration 3, loss = 1.94793117\n",
      "Iteration 4, loss = 1.94539949\n",
      "Iteration 5, loss = 1.94304648\n",
      "Iteration 6, loss = 1.94050734\n",
      "Iteration 7, loss = 1.93813490\n",
      "Iteration 8, loss = 1.93567505\n",
      "Iteration 9, loss = 1.93315508\n",
      "Iteration 10, loss = 1.93081359\n",
      "Iteration 11, loss = 1.92826710\n",
      "Iteration 12, loss = 1.92580233\n",
      "Iteration 13, loss = 1.92349875\n",
      "Iteration 14, loss = 1.92124341\n",
      "Iteration 15, loss = 1.91908927\n",
      "Iteration 16, loss = 1.91701529\n",
      "Iteration 17, loss = 1.91488192\n",
      "Iteration 18, loss = 1.91279228\n",
      "Iteration 19, loss = 1.91073998\n",
      "Iteration 20, loss = 1.90888290\n",
      "Iteration 21, loss = 1.90689079\n",
      "Iteration 22, loss = 1.90500133\n",
      "Iteration 23, loss = 1.90304139\n",
      "Iteration 24, loss = 1.90117191\n",
      "Iteration 25, loss = 1.89940164\n",
      "Iteration 26, loss = 1.89752441\n",
      "Iteration 27, loss = 1.89570052\n",
      "Iteration 28, loss = 1.89383971\n",
      "Iteration 29, loss = 1.89208789\n",
      "Iteration 30, loss = 1.89022504\n",
      "Iteration 31, loss = 1.88842690\n",
      "Iteration 32, loss = 1.88668473\n",
      "Iteration 33, loss = 1.88487881\n",
      "Iteration 34, loss = 1.88302511\n",
      "Iteration 35, loss = 1.88127916\n",
      "Iteration 36, loss = 1.87959291\n",
      "Iteration 37, loss = 1.87755559\n",
      "Iteration 38, loss = 1.87581778\n",
      "Iteration 39, loss = 1.87393029\n",
      "Iteration 40, loss = 1.87205704\n",
      "Iteration 41, loss = 1.87017438\n",
      "Iteration 42, loss = 1.86829007\n",
      "Iteration 43, loss = 1.86652033\n",
      "Iteration 44, loss = 1.86480657\n",
      "Iteration 45, loss = 1.86303685\n",
      "Iteration 46, loss = 1.86125421\n",
      "Iteration 47, loss = 1.85943629\n",
      "Iteration 48, loss = 1.85771005\n",
      "Iteration 49, loss = 1.85594052\n",
      "Iteration 50, loss = 1.85425654\n",
      "Iteration 51, loss = 1.85233974\n",
      "Iteration 52, loss = 1.85063829\n",
      "Iteration 53, loss = 1.84898027\n",
      "Iteration 54, loss = 1.84718852\n",
      "Iteration 55, loss = 1.84536693\n",
      "Iteration 56, loss = 1.84384564\n",
      "Iteration 57, loss = 1.84199702\n",
      "Iteration 58, loss = 1.84021761\n",
      "Iteration 59, loss = 1.83852777\n",
      "Iteration 60, loss = 1.83693374\n",
      "Iteration 61, loss = 1.83498951\n",
      "Iteration 62, loss = 1.83328797\n",
      "Iteration 63, loss = 1.83163033\n",
      "Iteration 64, loss = 1.83009549\n",
      "Iteration 65, loss = 1.82821375\n",
      "Iteration 66, loss = 1.82644452\n",
      "Iteration 67, loss = 1.82479567\n",
      "Iteration 68, loss = 1.82318181\n",
      "Iteration 69, loss = 1.82138638\n",
      "Iteration 70, loss = 1.81982218\n",
      "Iteration 71, loss = 1.81811052\n",
      "Iteration 72, loss = 1.81647319\n",
      "Iteration 73, loss = 1.81473342\n",
      "Iteration 74, loss = 1.81309584\n",
      "Iteration 75, loss = 1.81135941\n",
      "Iteration 76, loss = 1.80963178\n",
      "Iteration 77, loss = 1.80810244\n",
      "Iteration 78, loss = 1.80630604\n",
      "Iteration 79, loss = 1.80469723\n",
      "Iteration 80, loss = 1.80317075\n",
      "Iteration 81, loss = 1.80126846\n",
      "Iteration 82, loss = 1.79964999\n",
      "Iteration 83, loss = 1.79789998\n",
      "Iteration 84, loss = 1.79623691\n",
      "Iteration 85, loss = 1.79452186\n",
      "Iteration 86, loss = 1.79286241\n",
      "Iteration 87, loss = 1.79119572\n",
      "Iteration 88, loss = 1.78945754\n",
      "Iteration 89, loss = 1.78778631\n",
      "Iteration 90, loss = 1.78617913\n",
      "Iteration 91, loss = 1.78458846\n",
      "Iteration 92, loss = 1.78288618\n",
      "Iteration 93, loss = 1.78134036\n",
      "Iteration 94, loss = 1.77979792\n",
      "Iteration 95, loss = 1.77822631\n",
      "Iteration 96, loss = 1.77662911\n",
      "Iteration 97, loss = 1.77494976\n",
      "Iteration 98, loss = 1.77339949\n",
      "Iteration 99, loss = 1.77168596\n",
      "Iteration 100, loss = 1.77010372\n",
      "Iteration 101, loss = 1.76845566\n",
      "Iteration 102, loss = 1.76703918\n",
      "Iteration 103, loss = 1.76532935\n",
      "Iteration 104, loss = 1.76373992\n",
      "Iteration 105, loss = 1.76211743\n",
      "Iteration 106, loss = 1.76059415\n",
      "Iteration 107, loss = 1.75894434\n",
      "Iteration 108, loss = 1.75735269\n",
      "Iteration 109, loss = 1.75586017\n",
      "Iteration 110, loss = 1.75422727\n",
      "Iteration 111, loss = 1.75270610\n",
      "Iteration 112, loss = 1.75126074\n",
      "Iteration 113, loss = 1.74958864\n",
      "Iteration 114, loss = 1.74815275\n",
      "Iteration 115, loss = 1.74662901\n",
      "Iteration 116, loss = 1.74514241\n",
      "Iteration 117, loss = 1.74362750\n",
      "Iteration 118, loss = 1.74204559\n",
      "Iteration 119, loss = 1.74054373\n",
      "Iteration 120, loss = 1.73914667\n",
      "Iteration 121, loss = 1.73750630\n",
      "Iteration 122, loss = 1.73606257\n",
      "Iteration 123, loss = 1.73450119\n",
      "Iteration 124, loss = 1.73291332\n",
      "Iteration 125, loss = 1.73142363\n",
      "Iteration 126, loss = 1.72988012\n",
      "Iteration 127, loss = 1.72846566\n",
      "Iteration 128, loss = 1.72689710\n",
      "Iteration 129, loss = 1.72537793\n",
      "Iteration 130, loss = 1.72380010\n",
      "Iteration 131, loss = 1.72239625\n",
      "Iteration 132, loss = 1.72081320\n",
      "Iteration 133, loss = 1.71910343\n",
      "Iteration 134, loss = 1.71762243\n",
      "Iteration 135, loss = 1.71588363\n",
      "Iteration 136, loss = 1.71439767\n",
      "Iteration 137, loss = 1.71269725\n",
      "Iteration 138, loss = 1.71122499\n",
      "Iteration 139, loss = 1.70983332\n",
      "Iteration 140, loss = 1.70819218\n",
      "Iteration 141, loss = 1.70669719\n",
      "Iteration 142, loss = 1.70536660\n",
      "Iteration 143, loss = 1.70386239\n",
      "Iteration 144, loss = 1.70234751\n",
      "Iteration 145, loss = 1.70090696\n",
      "Iteration 146, loss = 1.69937542\n",
      "Iteration 147, loss = 1.69785185\n",
      "Iteration 148, loss = 1.69638032\n",
      "Iteration 149, loss = 1.69499095\n",
      "Iteration 150, loss = 1.69357232\n",
      "Iteration 151, loss = 1.69210341\n",
      "Iteration 152, loss = 1.69061091\n",
      "Iteration 153, loss = 1.68934197\n",
      "Iteration 154, loss = 1.68772216\n",
      "Iteration 155, loss = 1.68636086\n",
      "Iteration 156, loss = 1.68488402\n",
      "Iteration 157, loss = 1.68349473\n",
      "Iteration 158, loss = 1.68204376\n",
      "Iteration 159, loss = 1.68066628\n",
      "Iteration 160, loss = 1.67919405\n",
      "Iteration 161, loss = 1.67773119\n",
      "Iteration 162, loss = 1.67630732\n",
      "Iteration 163, loss = 1.67481742\n",
      "Iteration 164, loss = 1.67353718\n",
      "Iteration 165, loss = 1.67193628\n",
      "Iteration 166, loss = 1.67056756\n",
      "Iteration 167, loss = 1.66912456\n",
      "Iteration 168, loss = 1.66768046\n",
      "Iteration 169, loss = 1.66630423\n",
      "Iteration 170, loss = 1.66492017\n",
      "Iteration 171, loss = 1.66345206\n",
      "Iteration 172, loss = 1.66204626\n",
      "Iteration 173, loss = 1.66060365\n",
      "Iteration 174, loss = 1.65923881\n",
      "Iteration 175, loss = 1.65783946\n",
      "Iteration 176, loss = 1.65645866\n",
      "Iteration 177, loss = 1.65499012\n",
      "Iteration 178, loss = 1.65362845\n",
      "Iteration 179, loss = 1.65230774\n",
      "Iteration 180, loss = 1.65093642\n",
      "Iteration 181, loss = 1.64957308\n",
      "Iteration 182, loss = 1.64809708\n",
      "Iteration 183, loss = 1.64680593\n",
      "Iteration 184, loss = 1.64551206\n",
      "Iteration 185, loss = 1.64400043\n",
      "Iteration 186, loss = 1.64270704\n",
      "Iteration 187, loss = 1.64131053\n",
      "Iteration 188, loss = 1.63987346\n",
      "Iteration 189, loss = 1.63851614\n",
      "Iteration 190, loss = 1.63705331\n",
      "Iteration 191, loss = 1.63567887\n",
      "Iteration 192, loss = 1.63437308\n",
      "Iteration 193, loss = 1.63303840\n",
      "Iteration 194, loss = 1.63165785\n",
      "Iteration 195, loss = 1.63027990\n",
      "Iteration 196, loss = 1.62908907\n",
      "Iteration 197, loss = 1.62754366\n",
      "Iteration 198, loss = 1.62630907\n",
      "Iteration 199, loss = 1.62493467\n",
      "Iteration 200, loss = 1.62355458\n",
      "Iteration 1, loss = 1.61584795\n",
      "Iteration 2, loss = 1.28688672\n",
      "Iteration 3, loss = 1.28102280\n",
      "Iteration 4, loss = 1.28808753\n",
      "Iteration 5, loss = 1.28857236\n",
      "Iteration 6, loss = 1.27461709\n",
      "Iteration 7, loss = 1.27267065\n",
      "Iteration 8, loss = 1.27505989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 1.26590167\n",
      "Iteration 10, loss = 1.27385124\n",
      "Iteration 11, loss = 1.27134588\n",
      "Iteration 12, loss = 1.26622454\n",
      "Iteration 13, loss = 1.26260643\n",
      "Iteration 14, loss = 1.26563381\n",
      "Iteration 15, loss = 1.26637022\n",
      "Iteration 16, loss = 1.26341175\n",
      "Iteration 17, loss = 1.27682373\n",
      "Iteration 18, loss = 1.27325657\n",
      "Iteration 19, loss = 1.26313384\n",
      "Iteration 20, loss = 1.27908482\n",
      "Iteration 21, loss = 1.26932530\n",
      "Iteration 22, loss = 1.26529413\n",
      "Iteration 23, loss = 1.26862290\n",
      "Iteration 24, loss = 1.26632182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.59919622\n",
      "Iteration 2, loss = 1.59718348\n",
      "Iteration 3, loss = 1.59508607\n",
      "Iteration 4, loss = 1.59327807\n",
      "Iteration 5, loss = 1.59142361\n",
      "Iteration 6, loss = 1.58940848\n",
      "Iteration 7, loss = 1.58785721\n",
      "Iteration 8, loss = 1.58595383\n",
      "Iteration 9, loss = 1.58433776\n",
      "Iteration 10, loss = 1.58259693\n",
      "Iteration 11, loss = 1.58103254\n",
      "Iteration 12, loss = 1.57933214\n",
      "Iteration 13, loss = 1.57781635\n",
      "Iteration 14, loss = 1.57621567\n",
      "Iteration 15, loss = 1.57472865\n",
      "Iteration 16, loss = 1.57318904\n",
      "Iteration 17, loss = 1.57181009\n",
      "Iteration 18, loss = 1.57032216\n",
      "Iteration 19, loss = 1.56881517\n",
      "Iteration 20, loss = 1.56731617\n",
      "Iteration 21, loss = 1.56585723\n",
      "Iteration 22, loss = 1.56427601\n",
      "Iteration 23, loss = 1.56286235\n",
      "Iteration 24, loss = 1.56143565\n",
      "Iteration 25, loss = 1.55994904\n",
      "Iteration 26, loss = 1.55858308\n",
      "Iteration 27, loss = 1.55709646\n",
      "Iteration 28, loss = 1.55572579\n",
      "Iteration 29, loss = 1.55420540\n",
      "Iteration 30, loss = 1.55284798\n",
      "Iteration 31, loss = 1.55145738\n",
      "Iteration 32, loss = 1.55000233\n",
      "Iteration 33, loss = 1.54869066\n",
      "Iteration 34, loss = 1.54729575\n",
      "Iteration 35, loss = 1.54603247\n",
      "Iteration 36, loss = 1.54453689\n",
      "Iteration 37, loss = 1.54317232\n",
      "Iteration 38, loss = 1.54185065\n",
      "Iteration 39, loss = 1.54056400\n",
      "Iteration 40, loss = 1.53910931\n",
      "Iteration 41, loss = 1.53800500\n",
      "Iteration 42, loss = 1.53651549\n",
      "Iteration 43, loss = 1.53524427\n",
      "Iteration 44, loss = 1.53388817\n",
      "Iteration 45, loss = 1.53258045\n",
      "Iteration 46, loss = 1.53130252\n",
      "Iteration 47, loss = 1.52984326\n",
      "Iteration 48, loss = 1.52848325\n",
      "Iteration 49, loss = 1.52716263\n",
      "Iteration 50, loss = 1.52572708\n",
      "Iteration 51, loss = 1.52436738\n",
      "Iteration 52, loss = 1.52311516\n",
      "Iteration 53, loss = 1.52176325\n",
      "Iteration 54, loss = 1.52046969\n",
      "Iteration 55, loss = 1.51923869\n",
      "Iteration 56, loss = 1.51793540\n",
      "Iteration 57, loss = 1.51662010\n",
      "Iteration 58, loss = 1.51530865\n",
      "Iteration 59, loss = 1.51405319\n",
      "Iteration 60, loss = 1.51285140\n",
      "Iteration 61, loss = 1.51153934\n",
      "Iteration 62, loss = 1.51032392\n",
      "Iteration 63, loss = 1.50912800\n",
      "Iteration 64, loss = 1.50782555\n",
      "Iteration 65, loss = 1.50658091\n",
      "Iteration 66, loss = 1.50531759\n",
      "Iteration 67, loss = 1.50398424\n",
      "Iteration 68, loss = 1.50284501\n",
      "Iteration 69, loss = 1.50154693\n",
      "Iteration 70, loss = 1.50024315\n",
      "Iteration 71, loss = 1.49900186\n",
      "Iteration 72, loss = 1.49782387\n",
      "Iteration 73, loss = 1.49651190\n",
      "Iteration 74, loss = 1.49542671\n",
      "Iteration 75, loss = 1.49421327\n",
      "Iteration 76, loss = 1.49291171\n",
      "Iteration 77, loss = 1.49178425\n",
      "Iteration 78, loss = 1.49059640\n",
      "Iteration 79, loss = 1.48945053\n",
      "Iteration 80, loss = 1.48829758\n",
      "Iteration 81, loss = 1.48705614\n",
      "Iteration 82, loss = 1.48595943\n",
      "Iteration 83, loss = 1.48477162\n",
      "Iteration 84, loss = 1.48355058\n",
      "Iteration 85, loss = 1.48232376\n",
      "Iteration 86, loss = 1.48112813\n",
      "Iteration 87, loss = 1.48000240\n",
      "Iteration 88, loss = 1.47897246\n",
      "Iteration 89, loss = 1.47761509\n",
      "Iteration 90, loss = 1.47649442\n",
      "Iteration 91, loss = 1.47537658\n",
      "Iteration 92, loss = 1.47421289\n",
      "Iteration 93, loss = 1.47309327\n",
      "Iteration 94, loss = 1.47213551\n",
      "Iteration 95, loss = 1.47084258\n",
      "Iteration 96, loss = 1.46979853\n",
      "Iteration 97, loss = 1.46853194\n",
      "Iteration 98, loss = 1.46753975\n",
      "Iteration 99, loss = 1.46629555\n",
      "Iteration 100, loss = 1.46524173\n",
      "Iteration 101, loss = 1.46407639\n",
      "Iteration 102, loss = 1.46297087\n",
      "Iteration 103, loss = 1.46188599\n",
      "Iteration 104, loss = 1.46065996\n",
      "Iteration 105, loss = 1.45953795\n",
      "Iteration 106, loss = 1.45846063\n",
      "Iteration 107, loss = 1.45719322\n",
      "Iteration 108, loss = 1.45619412\n",
      "Iteration 109, loss = 1.45502191\n",
      "Iteration 110, loss = 1.45388334\n",
      "Iteration 111, loss = 1.45276845\n",
      "Iteration 112, loss = 1.45164766\n",
      "Iteration 113, loss = 1.45041792\n",
      "Iteration 114, loss = 1.44932271\n",
      "Iteration 115, loss = 1.44827459\n",
      "Iteration 116, loss = 1.44709957\n",
      "Iteration 117, loss = 1.44610276\n",
      "Iteration 118, loss = 1.44496941\n",
      "Iteration 119, loss = 1.44394254\n",
      "Iteration 120, loss = 1.44290930\n",
      "Iteration 121, loss = 1.44196151\n",
      "Iteration 122, loss = 1.44085267\n",
      "Iteration 123, loss = 1.43984266\n",
      "Iteration 124, loss = 1.43883668\n",
      "Iteration 125, loss = 1.43785029\n",
      "Iteration 126, loss = 1.43683116\n",
      "Iteration 127, loss = 1.43577714\n",
      "Iteration 128, loss = 1.43471265\n",
      "Iteration 129, loss = 1.43364243\n",
      "Iteration 130, loss = 1.43277962\n",
      "Iteration 131, loss = 1.43162643\n",
      "Iteration 132, loss = 1.43070423\n",
      "Iteration 133, loss = 1.42968604\n",
      "Iteration 134, loss = 1.42863799\n",
      "Iteration 135, loss = 1.42763410\n",
      "Iteration 136, loss = 1.42670928\n",
      "Iteration 137, loss = 1.42554459\n",
      "Iteration 138, loss = 1.42466587\n",
      "Iteration 139, loss = 1.42354770\n",
      "Iteration 140, loss = 1.42259797\n",
      "Iteration 141, loss = 1.42160662\n",
      "Iteration 142, loss = 1.42057124\n",
      "Iteration 143, loss = 1.41963749\n",
      "Iteration 144, loss = 1.41877768\n",
      "Iteration 145, loss = 1.41774617\n",
      "Iteration 146, loss = 1.41679858\n",
      "Iteration 147, loss = 1.41595220\n",
      "Iteration 148, loss = 1.41499943\n",
      "Iteration 149, loss = 1.41397620\n",
      "Iteration 150, loss = 1.41311330\n",
      "Iteration 151, loss = 1.41214965\n",
      "Iteration 152, loss = 1.41121985\n",
      "Iteration 153, loss = 1.41018372\n",
      "Iteration 154, loss = 1.40938290\n",
      "Iteration 155, loss = 1.40840893\n",
      "Iteration 156, loss = 1.40744538\n",
      "Iteration 157, loss = 1.40656100\n",
      "Iteration 158, loss = 1.40548894\n",
      "Iteration 159, loss = 1.40453544\n",
      "Iteration 160, loss = 1.40362228\n",
      "Iteration 161, loss = 1.40278247\n",
      "Iteration 162, loss = 1.40187456\n",
      "Iteration 163, loss = 1.40093953\n",
      "Iteration 164, loss = 1.40003972\n",
      "Iteration 165, loss = 1.39924494\n",
      "Iteration 166, loss = 1.39821762\n",
      "Iteration 167, loss = 1.39746038\n",
      "Iteration 168, loss = 1.39657697\n",
      "Iteration 169, loss = 1.39568469\n",
      "Iteration 170, loss = 1.39488411\n",
      "Iteration 171, loss = 1.39404871\n",
      "Iteration 172, loss = 1.39317465\n",
      "Iteration 173, loss = 1.39229858\n",
      "Iteration 174, loss = 1.39152989\n",
      "Iteration 175, loss = 1.39054566\n",
      "Iteration 176, loss = 1.38974775\n",
      "Iteration 177, loss = 1.38887343\n",
      "Iteration 178, loss = 1.38795598\n",
      "Iteration 179, loss = 1.38715981\n",
      "Iteration 180, loss = 1.38635654\n",
      "Iteration 181, loss = 1.38547451\n",
      "Iteration 182, loss = 1.38462687\n",
      "Iteration 183, loss = 1.38376852\n",
      "Iteration 184, loss = 1.38288032\n",
      "Iteration 185, loss = 1.38194015\n",
      "Iteration 186, loss = 1.38124339\n",
      "Iteration 187, loss = 1.38041424\n",
      "Iteration 188, loss = 1.37944051\n",
      "Iteration 189, loss = 1.37863272\n",
      "Iteration 190, loss = 1.37776381\n",
      "Iteration 191, loss = 1.37705595\n",
      "Iteration 192, loss = 1.37613222\n",
      "Iteration 193, loss = 1.37534334\n",
      "Iteration 194, loss = 1.37478120\n",
      "Iteration 195, loss = 1.37392099\n",
      "Iteration 196, loss = 1.37307157\n",
      "Iteration 197, loss = 1.37240182\n",
      "Iteration 198, loss = 1.37165030\n",
      "Iteration 199, loss = 1.37090168\n",
      "Iteration 200, loss = 1.37024265\n",
      "Iteration 1, loss = 1.54906919\n",
      "Iteration 2, loss = 1.30753128\n",
      "Iteration 3, loss = 1.29794148\n",
      "Iteration 4, loss = 1.31013419\n",
      "Iteration 5, loss = 1.28285190\n",
      "Iteration 6, loss = 1.27137119\n",
      "Iteration 7, loss = 1.27833064\n",
      "Iteration 8, loss = 1.27573526\n",
      "Iteration 9, loss = 1.26821857\n",
      "Iteration 10, loss = 1.27034395\n",
      "Iteration 11, loss = 1.26937135\n",
      "Iteration 12, loss = 1.28171341\n",
      "Iteration 13, loss = 1.28038276\n",
      "Iteration 14, loss = 1.27455616\n",
      "Iteration 15, loss = 1.27922954\n",
      "Iteration 16, loss = 1.27885493\n",
      "Iteration 17, loss = 1.28106875\n",
      "Iteration 18, loss = 1.27513803\n",
      "Iteration 19, loss = 1.27099619\n",
      "Iteration 20, loss = 1.27208119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58365426\n",
      "Iteration 2, loss = 1.58164446\n",
      "Iteration 3, loss = 1.57966453\n",
      "Iteration 4, loss = 1.57764269\n",
      "Iteration 5, loss = 1.57569102\n",
      "Iteration 6, loss = 1.57375910\n",
      "Iteration 7, loss = 1.57162585\n",
      "Iteration 8, loss = 1.56975140\n",
      "Iteration 9, loss = 1.56786430\n",
      "Iteration 10, loss = 1.56610364\n",
      "Iteration 11, loss = 1.56402219\n",
      "Iteration 12, loss = 1.56224845\n",
      "Iteration 13, loss = 1.56037056\n",
      "Iteration 14, loss = 1.55826593\n",
      "Iteration 15, loss = 1.55656871\n",
      "Iteration 16, loss = 1.55450446\n",
      "Iteration 17, loss = 1.55269589\n",
      "Iteration 18, loss = 1.55089319\n",
      "Iteration 19, loss = 1.54897113\n",
      "Iteration 20, loss = 1.54712943\n",
      "Iteration 21, loss = 1.54537936\n",
      "Iteration 22, loss = 1.54368969\n",
      "Iteration 23, loss = 1.54191096\n",
      "Iteration 24, loss = 1.54004281\n",
      "Iteration 25, loss = 1.53808689\n",
      "Iteration 26, loss = 1.53618422\n",
      "Iteration 27, loss = 1.53445846\n",
      "Iteration 28, loss = 1.53240689\n",
      "Iteration 29, loss = 1.53040846\n",
      "Iteration 30, loss = 1.52814234\n",
      "Iteration 31, loss = 1.52626596\n",
      "Iteration 32, loss = 1.52385931\n",
      "Iteration 33, loss = 1.52198981\n",
      "Iteration 34, loss = 1.51990103\n",
      "Iteration 35, loss = 1.51777809\n",
      "Iteration 36, loss = 1.51606161\n",
      "Iteration 37, loss = 1.51433289\n",
      "Iteration 38, loss = 1.51228243\n",
      "Iteration 39, loss = 1.51040619\n",
      "Iteration 40, loss = 1.50872950\n",
      "Iteration 41, loss = 1.50697581\n",
      "Iteration 42, loss = 1.50527688\n",
      "Iteration 43, loss = 1.50346071\n",
      "Iteration 44, loss = 1.50183290\n",
      "Iteration 45, loss = 1.49992023\n",
      "Iteration 46, loss = 1.49830257\n",
      "Iteration 47, loss = 1.49664343\n",
      "Iteration 48, loss = 1.49496433\n",
      "Iteration 49, loss = 1.49318316\n",
      "Iteration 50, loss = 1.49142344\n",
      "Iteration 51, loss = 1.48986712\n",
      "Iteration 52, loss = 1.48825961\n",
      "Iteration 53, loss = 1.48669665\n",
      "Iteration 54, loss = 1.48509878\n",
      "Iteration 55, loss = 1.48353317\n",
      "Iteration 56, loss = 1.48208522\n",
      "Iteration 57, loss = 1.48044996\n",
      "Iteration 58, loss = 1.47897395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59, loss = 1.47745855\n",
      "Iteration 60, loss = 1.47598058\n",
      "Iteration 61, loss = 1.47441802\n",
      "Iteration 62, loss = 1.47286946\n",
      "Iteration 63, loss = 1.47153666\n",
      "Iteration 64, loss = 1.46987399\n",
      "Iteration 65, loss = 1.46845582\n",
      "Iteration 66, loss = 1.46685713\n",
      "Iteration 67, loss = 1.46551768\n",
      "Iteration 68, loss = 1.46397124\n",
      "Iteration 69, loss = 1.46246170\n",
      "Iteration 70, loss = 1.46090882\n",
      "Iteration 71, loss = 1.45954398\n",
      "Iteration 72, loss = 1.45801481\n",
      "Iteration 73, loss = 1.45649360\n",
      "Iteration 74, loss = 1.45508947\n",
      "Iteration 75, loss = 1.45359726\n",
      "Iteration 76, loss = 1.45201425\n",
      "Iteration 77, loss = 1.45067466\n",
      "Iteration 78, loss = 1.44911402\n",
      "Iteration 79, loss = 1.44763841\n",
      "Iteration 80, loss = 1.44615339\n",
      "Iteration 81, loss = 1.44474434\n",
      "Iteration 82, loss = 1.44330234\n",
      "Iteration 83, loss = 1.44181558\n",
      "Iteration 84, loss = 1.44025944\n",
      "Iteration 85, loss = 1.43900036\n",
      "Iteration 86, loss = 1.43751418\n",
      "Iteration 87, loss = 1.43619739\n",
      "Iteration 88, loss = 1.43475254\n",
      "Iteration 89, loss = 1.43359640\n",
      "Iteration 90, loss = 1.43222480\n",
      "Iteration 91, loss = 1.43092687\n",
      "Iteration 92, loss = 1.42963455\n",
      "Iteration 93, loss = 1.42829625\n",
      "Iteration 94, loss = 1.42693116\n",
      "Iteration 95, loss = 1.42568826\n",
      "Iteration 96, loss = 1.42429674\n",
      "Iteration 97, loss = 1.42305932\n",
      "Iteration 98, loss = 1.42173641\n",
      "Iteration 99, loss = 1.42026368\n",
      "Iteration 100, loss = 1.41900970\n",
      "Iteration 101, loss = 1.41781099\n",
      "Iteration 102, loss = 1.41644783\n",
      "Iteration 103, loss = 1.41512975\n",
      "Iteration 104, loss = 1.41386951\n",
      "Iteration 105, loss = 1.41275177\n",
      "Iteration 106, loss = 1.41136060\n",
      "Iteration 107, loss = 1.41020973\n",
      "Iteration 108, loss = 1.40899670\n",
      "Iteration 109, loss = 1.40777710\n",
      "Iteration 110, loss = 1.40669011\n",
      "Iteration 111, loss = 1.40547923\n",
      "Iteration 112, loss = 1.40422409\n",
      "Iteration 113, loss = 1.40318481\n",
      "Iteration 114, loss = 1.40193472\n",
      "Iteration 115, loss = 1.40084280\n",
      "Iteration 116, loss = 1.39981292\n",
      "Iteration 117, loss = 1.39855767\n",
      "Iteration 118, loss = 1.39745504\n",
      "Iteration 119, loss = 1.39638065\n",
      "Iteration 120, loss = 1.39515681\n",
      "Iteration 121, loss = 1.39411501\n",
      "Iteration 122, loss = 1.39289576\n",
      "Iteration 123, loss = 1.39194471\n",
      "Iteration 124, loss = 1.39066870\n",
      "Iteration 125, loss = 1.38954816\n",
      "Iteration 126, loss = 1.38843218\n",
      "Iteration 127, loss = 1.38741426\n",
      "Iteration 128, loss = 1.38626060\n",
      "Iteration 129, loss = 1.38510246\n",
      "Iteration 130, loss = 1.38403844\n",
      "Iteration 131, loss = 1.38311122\n",
      "Iteration 132, loss = 1.38185671\n",
      "Iteration 133, loss = 1.38085182\n",
      "Iteration 134, loss = 1.37977344\n",
      "Iteration 135, loss = 1.37876191\n",
      "Iteration 136, loss = 1.37765524\n",
      "Iteration 137, loss = 1.37659451\n",
      "Iteration 138, loss = 1.37552096\n",
      "Iteration 139, loss = 1.37449141\n",
      "Iteration 140, loss = 1.37335287\n",
      "Iteration 141, loss = 1.37232885\n",
      "Iteration 142, loss = 1.37132124\n",
      "Iteration 143, loss = 1.37011871\n",
      "Iteration 144, loss = 1.36912437\n",
      "Iteration 145, loss = 1.36802818\n",
      "Iteration 146, loss = 1.36704761\n",
      "Iteration 147, loss = 1.36594908\n",
      "Iteration 148, loss = 1.36477397\n",
      "Iteration 149, loss = 1.36371402\n",
      "Iteration 150, loss = 1.36258366\n",
      "Iteration 151, loss = 1.36129920\n",
      "Iteration 152, loss = 1.36007465\n",
      "Iteration 153, loss = 1.35875986\n",
      "Iteration 154, loss = 1.35748301\n",
      "Iteration 155, loss = 1.35623276\n",
      "Iteration 156, loss = 1.35488373\n",
      "Iteration 157, loss = 1.35368803\n",
      "Iteration 158, loss = 1.35257095\n",
      "Iteration 159, loss = 1.35139012\n",
      "Iteration 160, loss = 1.35036521\n",
      "Iteration 161, loss = 1.34926955\n",
      "Iteration 162, loss = 1.34832080\n",
      "Iteration 163, loss = 1.34728704\n",
      "Iteration 164, loss = 1.34625460\n",
      "Iteration 165, loss = 1.34526521\n",
      "Iteration 166, loss = 1.34420522\n",
      "Iteration 167, loss = 1.34330541\n",
      "Iteration 168, loss = 1.34221591\n",
      "Iteration 169, loss = 1.34130173\n",
      "Iteration 170, loss = 1.34032126\n",
      "Iteration 171, loss = 1.33931158\n",
      "Iteration 172, loss = 1.33834774\n",
      "Iteration 173, loss = 1.33748517\n",
      "Iteration 174, loss = 1.33635997\n",
      "Iteration 175, loss = 1.33539519\n",
      "Iteration 176, loss = 1.33447078\n",
      "Iteration 177, loss = 1.33357567\n",
      "Iteration 178, loss = 1.33254601\n",
      "Iteration 179, loss = 1.33178919\n",
      "Iteration 180, loss = 1.33087254\n",
      "Iteration 181, loss = 1.32983154\n",
      "Iteration 182, loss = 1.32905745\n",
      "Iteration 183, loss = 1.32812841\n",
      "Iteration 184, loss = 1.32723355\n",
      "Iteration 185, loss = 1.32637015\n",
      "Iteration 186, loss = 1.32546339\n",
      "Iteration 187, loss = 1.32453023\n",
      "Iteration 188, loss = 1.32374445\n",
      "Iteration 189, loss = 1.32283283\n",
      "Iteration 190, loss = 1.32206525\n",
      "Iteration 191, loss = 1.32112385\n",
      "Iteration 192, loss = 1.32019190\n",
      "Iteration 193, loss = 1.31936831\n",
      "Iteration 194, loss = 1.31848635\n",
      "Iteration 195, loss = 1.31761198\n",
      "Iteration 196, loss = 1.31672044\n",
      "Iteration 197, loss = 1.31594133\n",
      "Iteration 198, loss = 1.31512208\n",
      "Iteration 199, loss = 1.31420618\n",
      "Iteration 200, loss = 1.31334818\n",
      "Iteration 1, loss = 1.37822270\n",
      "Iteration 2, loss = 1.23754054\n",
      "Iteration 3, loss = 1.23613326\n",
      "Iteration 4, loss = 1.22576087\n",
      "Iteration 5, loss = 1.22771609\n",
      "Iteration 6, loss = 1.21771478\n",
      "Iteration 7, loss = 1.21824451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.22206356\n",
      "Iteration 9, loss = 1.21880585\n",
      "Iteration 10, loss = 1.21482937\n",
      "Iteration 11, loss = 1.22177385\n",
      "Iteration 12, loss = 1.21572848\n",
      "Iteration 13, loss = 1.22408471\n",
      "Iteration 14, loss = 1.22771332\n",
      "Iteration 15, loss = 1.21236752\n",
      "Iteration 16, loss = 1.22636792\n",
      "Iteration 17, loss = 1.22320793\n",
      "Iteration 18, loss = 1.21960444\n",
      "Iteration 19, loss = 1.21576947\n",
      "Iteration 20, loss = 1.21554269\n",
      "Iteration 21, loss = 1.21949931\n",
      "Iteration 22, loss = 1.21543285\n",
      "Iteration 23, loss = 1.21483556\n",
      "Iteration 24, loss = 1.21664352\n",
      "Iteration 25, loss = 1.21507618\n",
      "Iteration 26, loss = 1.21964694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.74242661\n",
      "Iteration 2, loss = 1.74021354\n",
      "Iteration 3, loss = 1.73810374\n",
      "Iteration 4, loss = 1.73595980\n",
      "Iteration 5, loss = 1.73393497\n",
      "Iteration 6, loss = 1.73182951\n",
      "Iteration 7, loss = 1.72977277\n",
      "Iteration 8, loss = 1.72798863\n",
      "Iteration 9, loss = 1.72621532\n",
      "Iteration 10, loss = 1.72412499\n",
      "Iteration 11, loss = 1.72238450\n",
      "Iteration 12, loss = 1.72052070\n",
      "Iteration 13, loss = 1.71869651\n",
      "Iteration 14, loss = 1.71692118\n",
      "Iteration 15, loss = 1.71509765\n",
      "Iteration 16, loss = 1.71342171\n",
      "Iteration 17, loss = 1.71155242\n",
      "Iteration 18, loss = 1.70982937\n",
      "Iteration 19, loss = 1.70803198\n",
      "Iteration 20, loss = 1.70648205\n",
      "Iteration 21, loss = 1.70461190\n",
      "Iteration 22, loss = 1.70287959\n",
      "Iteration 23, loss = 1.70132804\n",
      "Iteration 24, loss = 1.69951531\n",
      "Iteration 25, loss = 1.69778657\n",
      "Iteration 26, loss = 1.69603243\n",
      "Iteration 27, loss = 1.69424776\n",
      "Iteration 28, loss = 1.69253605\n",
      "Iteration 29, loss = 1.69075783\n",
      "Iteration 30, loss = 1.68911277\n",
      "Iteration 31, loss = 1.68725165\n",
      "Iteration 32, loss = 1.68560154\n",
      "Iteration 33, loss = 1.68371418\n",
      "Iteration 34, loss = 1.68197093\n",
      "Iteration 35, loss = 1.68033385\n",
      "Iteration 36, loss = 1.67843106\n",
      "Iteration 37, loss = 1.67662702\n",
      "Iteration 38, loss = 1.67499981\n",
      "Iteration 39, loss = 1.67294198\n",
      "Iteration 40, loss = 1.67127932\n",
      "Iteration 41, loss = 1.66944655\n",
      "Iteration 42, loss = 1.66773917\n",
      "Iteration 43, loss = 1.66597812\n",
      "Iteration 44, loss = 1.66424415\n",
      "Iteration 45, loss = 1.66240199\n",
      "Iteration 46, loss = 1.66071249\n",
      "Iteration 47, loss = 1.65896599\n",
      "Iteration 48, loss = 1.65706941\n",
      "Iteration 49, loss = 1.65531849\n",
      "Iteration 50, loss = 1.65333231\n",
      "Iteration 51, loss = 1.65169004\n",
      "Iteration 52, loss = 1.64967542\n",
      "Iteration 53, loss = 1.64782410\n",
      "Iteration 54, loss = 1.64596162\n",
      "Iteration 55, loss = 1.64401970\n",
      "Iteration 56, loss = 1.64220227\n",
      "Iteration 57, loss = 1.64030769\n",
      "Iteration 58, loss = 1.63841067\n",
      "Iteration 59, loss = 1.63639628\n",
      "Iteration 60, loss = 1.63480576\n",
      "Iteration 61, loss = 1.63281823\n",
      "Iteration 62, loss = 1.63090279\n",
      "Iteration 63, loss = 1.62894155\n",
      "Iteration 64, loss = 1.62708528\n",
      "Iteration 65, loss = 1.62517695\n",
      "Iteration 66, loss = 1.62314552\n",
      "Iteration 67, loss = 1.62117776\n",
      "Iteration 68, loss = 1.61915619\n",
      "Iteration 69, loss = 1.61736723\n",
      "Iteration 70, loss = 1.61548446\n",
      "Iteration 71, loss = 1.61343892\n",
      "Iteration 72, loss = 1.61171029\n",
      "Iteration 73, loss = 1.60996736\n",
      "Iteration 74, loss = 1.60807536\n",
      "Iteration 75, loss = 1.60615207\n",
      "Iteration 76, loss = 1.60424661\n",
      "Iteration 77, loss = 1.60251208\n",
      "Iteration 78, loss = 1.60039592\n",
      "Iteration 79, loss = 1.59863806\n",
      "Iteration 80, loss = 1.59695262\n",
      "Iteration 81, loss = 1.59491270\n",
      "Iteration 82, loss = 1.59326466\n",
      "Iteration 83, loss = 1.59154106\n",
      "Iteration 84, loss = 1.58962512\n",
      "Iteration 85, loss = 1.58794682\n",
      "Iteration 86, loss = 1.58623092\n",
      "Iteration 87, loss = 1.58435682\n",
      "Iteration 88, loss = 1.58275346\n",
      "Iteration 89, loss = 1.58098695\n",
      "Iteration 90, loss = 1.57919272\n",
      "Iteration 91, loss = 1.57739831\n",
      "Iteration 92, loss = 1.57584657\n",
      "Iteration 93, loss = 1.57420325\n",
      "Iteration 94, loss = 1.57243489\n",
      "Iteration 95, loss = 1.57092271\n",
      "Iteration 96, loss = 1.56930916\n",
      "Iteration 97, loss = 1.56759109\n",
      "Iteration 98, loss = 1.56603113\n",
      "Iteration 99, loss = 1.56436073\n",
      "Iteration 100, loss = 1.56261776\n",
      "Iteration 101, loss = 1.56113754\n",
      "Iteration 102, loss = 1.55940203\n",
      "Iteration 103, loss = 1.55775260\n",
      "Iteration 104, loss = 1.55602704\n",
      "Iteration 105, loss = 1.55439358\n",
      "Iteration 106, loss = 1.55269277\n",
      "Iteration 107, loss = 1.55100519\n",
      "Iteration 108, loss = 1.54956578\n",
      "Iteration 109, loss = 1.54782169\n",
      "Iteration 110, loss = 1.54619482\n",
      "Iteration 111, loss = 1.54453282\n",
      "Iteration 112, loss = 1.54307478\n",
      "Iteration 113, loss = 1.54154484\n",
      "Iteration 114, loss = 1.53998387\n",
      "Iteration 115, loss = 1.53845637\n",
      "Iteration 116, loss = 1.53700037\n",
      "Iteration 117, loss = 1.53548155\n",
      "Iteration 118, loss = 1.53408177\n",
      "Iteration 119, loss = 1.53266853\n",
      "Iteration 120, loss = 1.53101511\n",
      "Iteration 121, loss = 1.52968671\n",
      "Iteration 122, loss = 1.52823266\n",
      "Iteration 123, loss = 1.52663422\n",
      "Iteration 124, loss = 1.52524738\n",
      "Iteration 125, loss = 1.52383023\n",
      "Iteration 126, loss = 1.52226624\n",
      "Iteration 127, loss = 1.52087731\n",
      "Iteration 128, loss = 1.51960705\n",
      "Iteration 129, loss = 1.51812765\n",
      "Iteration 130, loss = 1.51656344\n",
      "Iteration 131, loss = 1.51509363\n",
      "Iteration 132, loss = 1.51377760\n",
      "Iteration 133, loss = 1.51244206\n",
      "Iteration 134, loss = 1.51094117\n",
      "Iteration 135, loss = 1.50963857\n",
      "Iteration 136, loss = 1.50822566\n",
      "Iteration 137, loss = 1.50686112\n",
      "Iteration 138, loss = 1.50555668\n",
      "Iteration 139, loss = 1.50412740\n",
      "Iteration 140, loss = 1.50263010\n",
      "Iteration 141, loss = 1.50149306\n",
      "Iteration 142, loss = 1.50007996\n",
      "Iteration 143, loss = 1.49878249\n",
      "Iteration 144, loss = 1.49742040\n",
      "Iteration 145, loss = 1.49606420\n",
      "Iteration 146, loss = 1.49464998\n",
      "Iteration 147, loss = 1.49351933\n",
      "Iteration 148, loss = 1.49216873\n",
      "Iteration 149, loss = 1.49074819\n",
      "Iteration 150, loss = 1.48950387\n",
      "Iteration 151, loss = 1.48820058\n",
      "Iteration 152, loss = 1.48687846\n",
      "Iteration 153, loss = 1.48570247\n",
      "Iteration 154, loss = 1.48427895\n",
      "Iteration 155, loss = 1.48297500\n",
      "Iteration 156, loss = 1.48173482\n",
      "Iteration 157, loss = 1.48034333\n",
      "Iteration 158, loss = 1.47905153\n",
      "Iteration 159, loss = 1.47777808\n",
      "Iteration 160, loss = 1.47651694\n",
      "Iteration 161, loss = 1.47521864\n",
      "Iteration 162, loss = 1.47410187\n",
      "Iteration 163, loss = 1.47271154\n",
      "Iteration 164, loss = 1.47134893\n",
      "Iteration 165, loss = 1.47019998\n",
      "Iteration 166, loss = 1.46895138\n",
      "Iteration 167, loss = 1.46765490\n",
      "Iteration 168, loss = 1.46648453\n",
      "Iteration 169, loss = 1.46522420\n",
      "Iteration 170, loss = 1.46391483\n",
      "Iteration 171, loss = 1.46287276\n",
      "Iteration 172, loss = 1.46161625\n",
      "Iteration 173, loss = 1.46034925\n",
      "Iteration 174, loss = 1.45919248\n",
      "Iteration 175, loss = 1.45793991\n",
      "Iteration 176, loss = 1.45680652\n",
      "Iteration 177, loss = 1.45560197\n",
      "Iteration 178, loss = 1.45425077\n",
      "Iteration 179, loss = 1.45317349\n",
      "Iteration 180, loss = 1.45191584\n",
      "Iteration 181, loss = 1.45069975\n",
      "Iteration 182, loss = 1.44953971\n",
      "Iteration 183, loss = 1.44833254\n",
      "Iteration 184, loss = 1.44707951\n",
      "Iteration 185, loss = 1.44587831\n",
      "Iteration 186, loss = 1.44474343\n",
      "Iteration 187, loss = 1.44348389\n",
      "Iteration 188, loss = 1.44234535\n",
      "Iteration 189, loss = 1.44105032\n",
      "Iteration 190, loss = 1.43994736\n",
      "Iteration 191, loss = 1.43881071\n",
      "Iteration 192, loss = 1.43761439\n",
      "Iteration 193, loss = 1.43659085\n",
      "Iteration 194, loss = 1.43535833\n",
      "Iteration 195, loss = 1.43435687\n",
      "Iteration 196, loss = 1.43319494\n",
      "Iteration 197, loss = 1.43208208\n",
      "Iteration 198, loss = 1.43107055\n",
      "Iteration 199, loss = 1.42998461\n",
      "Iteration 200, loss = 1.42883614\n",
      "Iteration 1, loss = 1.50106308\n",
      "Iteration 2, loss = 1.26512259\n",
      "Iteration 3, loss = 1.25909147\n",
      "Iteration 4, loss = 1.23902835\n",
      "Iteration 5, loss = 1.23121510\n",
      "Iteration 6, loss = 1.22082107\n",
      "Iteration 7, loss = 1.22273686\n",
      "Iteration 8, loss = 1.21613996\n",
      "Iteration 9, loss = 1.21313260\n",
      "Iteration 10, loss = 1.21722833\n",
      "Iteration 11, loss = 1.23115049"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 12, loss = 1.22237941\n",
      "Iteration 13, loss = 1.23221563\n",
      "Iteration 14, loss = 1.21809323\n",
      "Iteration 15, loss = 1.22228213\n",
      "Iteration 16, loss = 1.22850808\n",
      "Iteration 17, loss = 1.21531877\n",
      "Iteration 18, loss = 1.21496260\n",
      "Iteration 19, loss = 1.21555425\n",
      "Iteration 20, loss = 1.21450096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52176615\n",
      "Iteration 2, loss = 1.51984083\n",
      "Iteration 3, loss = 1.51767405\n",
      "Iteration 4, loss = 1.51588415\n",
      "Iteration 5, loss = 1.51382031\n",
      "Iteration 6, loss = 1.51198668\n",
      "Iteration 7, loss = 1.50989575\n",
      "Iteration 8, loss = 1.50818959\n",
      "Iteration 9, loss = 1.50608261\n",
      "Iteration 10, loss = 1.50414178\n",
      "Iteration 11, loss = 1.50222766\n",
      "Iteration 12, loss = 1.50028993\n",
      "Iteration 13, loss = 1.49840489\n",
      "Iteration 14, loss = 1.49632999\n",
      "Iteration 15, loss = 1.49462880\n",
      "Iteration 16, loss = 1.49251657\n",
      "Iteration 17, loss = 1.49080280\n",
      "Iteration 18, loss = 1.48896911\n",
      "Iteration 19, loss = 1.48707354\n",
      "Iteration 20, loss = 1.48514466\n",
      "Iteration 21, loss = 1.48342335\n",
      "Iteration 22, loss = 1.48180444\n",
      "Iteration 23, loss = 1.48001443\n",
      "Iteration 24, loss = 1.47825507\n",
      "Iteration 25, loss = 1.47671777\n",
      "Iteration 26, loss = 1.47479842\n",
      "Iteration 27, loss = 1.47318158\n",
      "Iteration 28, loss = 1.47146729\n",
      "Iteration 29, loss = 1.46989483\n",
      "Iteration 30, loss = 1.46804140\n",
      "Iteration 31, loss = 1.46662330\n",
      "Iteration 32, loss = 1.46486828\n",
      "Iteration 33, loss = 1.46338614\n",
      "Iteration 34, loss = 1.46175866\n",
      "Iteration 35, loss = 1.46012897\n",
      "Iteration 36, loss = 1.45849332\n",
      "Iteration 37, loss = 1.45709911\n",
      "Iteration 38, loss = 1.45542541\n",
      "Iteration 39, loss = 1.45380762\n",
      "Iteration 40, loss = 1.45226803\n",
      "Iteration 41, loss = 1.45073539\n",
      "Iteration 42, loss = 1.44922230\n",
      "Iteration 43, loss = 1.44764732\n",
      "Iteration 44, loss = 1.44612291\n",
      "Iteration 45, loss = 1.44443606\n",
      "Iteration 46, loss = 1.44310121\n",
      "Iteration 47, loss = 1.44170947\n",
      "Iteration 48, loss = 1.44000602\n",
      "Iteration 49, loss = 1.43872128\n",
      "Iteration 50, loss = 1.43703797\n",
      "Iteration 51, loss = 1.43566590\n",
      "Iteration 52, loss = 1.43399238\n",
      "Iteration 53, loss = 1.43262100\n",
      "Iteration 54, loss = 1.43111277\n",
      "Iteration 55, loss = 1.42964217\n",
      "Iteration 56, loss = 1.42812709\n",
      "Iteration 57, loss = 1.42665013\n",
      "Iteration 58, loss = 1.42517305\n",
      "Iteration 59, loss = 1.42383445\n",
      "Iteration 60, loss = 1.42244569\n",
      "Iteration 61, loss = 1.42104906\n",
      "Iteration 62, loss = 1.41961690\n",
      "Iteration 63, loss = 1.41819119\n",
      "Iteration 64, loss = 1.41689788\n",
      "Iteration 65, loss = 1.41533596\n",
      "Iteration 66, loss = 1.41394106\n",
      "Iteration 67, loss = 1.41249967\n",
      "Iteration 68, loss = 1.41122937\n",
      "Iteration 69, loss = 1.40968858\n",
      "Iteration 70, loss = 1.40842143\n",
      "Iteration 71, loss = 1.40706434\n",
      "Iteration 72, loss = 1.40577174\n",
      "Iteration 73, loss = 1.40434818\n",
      "Iteration 74, loss = 1.40297753\n",
      "Iteration 75, loss = 1.40186659\n",
      "Iteration 76, loss = 1.40030660\n",
      "Iteration 77, loss = 1.39905592\n",
      "Iteration 78, loss = 1.39785047\n",
      "Iteration 79, loss = 1.39634180\n",
      "Iteration 80, loss = 1.39506969\n",
      "Iteration 81, loss = 1.39399501\n",
      "Iteration 82, loss = 1.39239301\n",
      "Iteration 83, loss = 1.39134105\n",
      "Iteration 84, loss = 1.38999607\n",
      "Iteration 85, loss = 1.38872580\n",
      "Iteration 86, loss = 1.38750278\n",
      "Iteration 87, loss = 1.38633409\n",
      "Iteration 88, loss = 1.38501755\n",
      "Iteration 89, loss = 1.38375315\n",
      "Iteration 90, loss = 1.38271938\n",
      "Iteration 91, loss = 1.38136878\n",
      "Iteration 92, loss = 1.38031997\n",
      "Iteration 93, loss = 1.37906008\n",
      "Iteration 94, loss = 1.37785167\n",
      "Iteration 95, loss = 1.37663110\n",
      "Iteration 96, loss = 1.37546574\n",
      "Iteration 97, loss = 1.37416657\n",
      "Iteration 98, loss = 1.37304926\n",
      "Iteration 99, loss = 1.37143225\n",
      "Iteration 100, loss = 1.36993249\n",
      "Iteration 101, loss = 1.36843885\n",
      "Iteration 102, loss = 1.36696423\n",
      "Iteration 103, loss = 1.36568295\n",
      "Iteration 104, loss = 1.36433571\n",
      "Iteration 105, loss = 1.36335145\n",
      "Iteration 106, loss = 1.36229379\n",
      "Iteration 107, loss = 1.36125681\n",
      "Iteration 108, loss = 1.36017341\n",
      "Iteration 109, loss = 1.35937478\n",
      "Iteration 110, loss = 1.35827838\n",
      "Iteration 111, loss = 1.35725862\n",
      "Iteration 112, loss = 1.35647638\n",
      "Iteration 113, loss = 1.35545687\n",
      "Iteration 114, loss = 1.35455733\n",
      "Iteration 115, loss = 1.35356766\n",
      "Iteration 116, loss = 1.35267249\n",
      "Iteration 117, loss = 1.35160998\n",
      "Iteration 118, loss = 1.35069151\n",
      "Iteration 119, loss = 1.34975171\n",
      "Iteration 120, loss = 1.34894633\n",
      "Iteration 121, loss = 1.34796059\n",
      "Iteration 122, loss = 1.34704258\n",
      "Iteration 123, loss = 1.34616304\n",
      "Iteration 124, loss = 1.34518469\n",
      "Iteration 125, loss = 1.34440337\n",
      "Iteration 126, loss = 1.34354543\n",
      "Iteration 127, loss = 1.34247016\n",
      "Iteration 128, loss = 1.34171089\n",
      "Iteration 129, loss = 1.34078664\n",
      "Iteration 130, loss = 1.33992521\n",
      "Iteration 131, loss = 1.33906508\n",
      "Iteration 132, loss = 1.33833768\n",
      "Iteration 133, loss = 1.33732573\n",
      "Iteration 134, loss = 1.33656454\n",
      "Iteration 135, loss = 1.33576013\n",
      "Iteration 136, loss = 1.33485252\n",
      "Iteration 137, loss = 1.33417505\n",
      "Iteration 138, loss = 1.33338774\n",
      "Iteration 139, loss = 1.33242669\n",
      "Iteration 140, loss = 1.33173971\n",
      "Iteration 141, loss = 1.33090030\n",
      "Iteration 142, loss = 1.33022851\n",
      "Iteration 143, loss = 1.32918216\n",
      "Iteration 144, loss = 1.32856757\n",
      "Iteration 145, loss = 1.32768775\n",
      "Iteration 146, loss = 1.32687843\n",
      "Iteration 147, loss = 1.32617741\n",
      "Iteration 148, loss = 1.32548749\n",
      "Iteration 149, loss = 1.32464662\n",
      "Iteration 150, loss = 1.32386797\n",
      "Iteration 151, loss = 1.32314104\n",
      "Iteration 152, loss = 1.32240652\n",
      "Iteration 153, loss = 1.32164077\n",
      "Iteration 154, loss = 1.32086659\n",
      "Iteration 155, loss = 1.32020338\n",
      "Iteration 156, loss = 1.31954365\n",
      "Iteration 157, loss = 1.31885991\n",
      "Iteration 158, loss = 1.31817266\n",
      "Iteration 159, loss = 1.31741920\n",
      "Iteration 160, loss = 1.31668272\n",
      "Iteration 161, loss = 1.31593915\n",
      "Iteration 162, loss = 1.31541233\n",
      "Iteration 163, loss = 1.31459949\n",
      "Iteration 164, loss = 1.31400998\n",
      "Iteration 165, loss = 1.31335182\n",
      "Iteration 166, loss = 1.31267124\n",
      "Iteration 167, loss = 1.31211081\n",
      "Iteration 168, loss = 1.31156111\n",
      "Iteration 169, loss = 1.31084288\n",
      "Iteration 170, loss = 1.31020090\n",
      "Iteration 171, loss = 1.30961450\n",
      "Iteration 172, loss = 1.30906030\n",
      "Iteration 173, loss = 1.30835378\n",
      "Iteration 174, loss = 1.30766840\n",
      "Iteration 175, loss = 1.30709876\n",
      "Iteration 176, loss = 1.30639927\n",
      "Iteration 177, loss = 1.30585432\n",
      "Iteration 178, loss = 1.30512082\n",
      "Iteration 179, loss = 1.30444335\n",
      "Iteration 180, loss = 1.30393471\n",
      "Iteration 181, loss = 1.30314467\n",
      "Iteration 182, loss = 1.30267994\n",
      "Iteration 183, loss = 1.30192006\n",
      "Iteration 184, loss = 1.30150521\n",
      "Iteration 185, loss = 1.30087172\n",
      "Iteration 186, loss = 1.30032524\n",
      "Iteration 187, loss = 1.29973709\n",
      "Iteration 188, loss = 1.29918189\n",
      "Iteration 189, loss = 1.29872718\n",
      "Iteration 190, loss = 1.29810464\n",
      "Iteration 191, loss = 1.29750493\n",
      "Iteration 192, loss = 1.29700841\n",
      "Iteration 193, loss = 1.29648546\n",
      "Iteration 194, loss = 1.29609380\n",
      "Iteration 195, loss = 1.29542251\n",
      "Iteration 196, loss = 1.29494602\n",
      "Iteration 197, loss = 1.29449558\n",
      "Iteration 198, loss = 1.29397618\n",
      "Iteration 199, loss = 1.29344037\n",
      "Iteration 200, loss = 1.29309081\n",
      "Iteration 1, loss = 1.44682617\n",
      "Iteration 2, loss = 1.28907068\n",
      "Iteration 3, loss = 1.26450679\n",
      "Iteration 4, loss = 1.25637794\n",
      "Iteration 5, loss = 1.25067807\n",
      "Iteration 6, loss = 1.25617137\n",
      "Iteration 7, loss = 1.24734454\n",
      "Iteration 8, loss = 1.25250816\n",
      "Iteration 9, loss = 1.25231653\n",
      "Iteration 10, loss = 1.24763121\n",
      "Iteration 11, loss = 1.24979690\n",
      "Iteration 12, loss = 1.24891909\n",
      "Iteration 13, loss = 1.24661454\n",
      "Iteration 14, loss = 1.25684192\n",
      "Iteration 15, loss = 1.25201572\n",
      "Iteration 16, loss = 1.25353279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 1.25420503\n",
      "Iteration 18, loss = 1.24791374\n",
      "Iteration 19, loss = 1.24749993\n",
      "Iteration 20, loss = 1.25539302\n",
      "Iteration 21, loss = 1.25261089\n",
      "Iteration 22, loss = 1.25299758\n",
      "Iteration 23, loss = 1.25151386\n",
      "Iteration 24, loss = 1.25193794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70111865\n",
      "Iteration 2, loss = 1.69915981\n",
      "Iteration 3, loss = 1.69711827\n",
      "Iteration 4, loss = 1.69492432\n",
      "Iteration 5, loss = 1.69277345\n",
      "Iteration 6, loss = 1.69037171\n",
      "Iteration 7, loss = 1.68823796\n",
      "Iteration 8, loss = 1.68607452\n",
      "Iteration 9, loss = 1.68352083\n",
      "Iteration 10, loss = 1.68127821\n",
      "Iteration 11, loss = 1.67912098\n",
      "Iteration 12, loss = 1.67677268\n",
      "Iteration 13, loss = 1.67463567\n",
      "Iteration 14, loss = 1.67243032\n",
      "Iteration 15, loss = 1.67021086\n",
      "Iteration 16, loss = 1.66808510\n",
      "Iteration 17, loss = 1.66601422\n",
      "Iteration 18, loss = 1.66406087\n",
      "Iteration 19, loss = 1.66193801\n",
      "Iteration 20, loss = 1.65983816\n",
      "Iteration 21, loss = 1.65807158\n",
      "Iteration 22, loss = 1.65605084\n",
      "Iteration 23, loss = 1.65422554\n",
      "Iteration 24, loss = 1.65229602\n",
      "Iteration 25, loss = 1.65056008\n",
      "Iteration 26, loss = 1.64876128\n",
      "Iteration 27, loss = 1.64692968\n",
      "Iteration 28, loss = 1.64519211\n",
      "Iteration 29, loss = 1.64345588\n",
      "Iteration 30, loss = 1.64180169\n",
      "Iteration 31, loss = 1.64004923\n",
      "Iteration 32, loss = 1.63831241\n",
      "Iteration 33, loss = 1.63665149\n",
      "Iteration 34, loss = 1.63498587\n",
      "Iteration 35, loss = 1.63325995\n",
      "Iteration 36, loss = 1.63169606\n",
      "Iteration 37, loss = 1.62992287\n",
      "Iteration 38, loss = 1.62827748\n",
      "Iteration 39, loss = 1.62663148\n",
      "Iteration 40, loss = 1.62498592\n",
      "Iteration 41, loss = 1.62319144\n",
      "Iteration 42, loss = 1.62162402\n",
      "Iteration 43, loss = 1.61998765\n",
      "Iteration 44, loss = 1.61834651\n",
      "Iteration 45, loss = 1.61658887\n",
      "Iteration 46, loss = 1.61475781\n",
      "Iteration 47, loss = 1.61314669\n",
      "Iteration 48, loss = 1.61130262\n",
      "Iteration 49, loss = 1.60975273\n",
      "Iteration 50, loss = 1.60776646\n",
      "Iteration 51, loss = 1.60606026\n",
      "Iteration 52, loss = 1.60427450\n",
      "Iteration 53, loss = 1.60257827\n",
      "Iteration 54, loss = 1.60085985\n",
      "Iteration 55, loss = 1.59917790\n",
      "Iteration 56, loss = 1.59750686\n",
      "Iteration 57, loss = 1.59588567\n",
      "Iteration 58, loss = 1.59426185\n",
      "Iteration 59, loss = 1.59261370\n",
      "Iteration 60, loss = 1.59103239\n",
      "Iteration 61, loss = 1.58931361\n",
      "Iteration 62, loss = 1.58773976\n",
      "Iteration 63, loss = 1.58607849\n",
      "Iteration 64, loss = 1.58461244\n",
      "Iteration 65, loss = 1.58279543\n",
      "Iteration 66, loss = 1.58129255\n",
      "Iteration 67, loss = 1.57972874\n",
      "Iteration 68, loss = 1.57822407\n",
      "Iteration 69, loss = 1.57677538\n",
      "Iteration 70, loss = 1.57508911\n",
      "Iteration 71, loss = 1.57354679\n",
      "Iteration 72, loss = 1.57206249\n",
      "Iteration 73, loss = 1.57053878\n",
      "Iteration 74, loss = 1.56901334\n",
      "Iteration 75, loss = 1.56759612\n",
      "Iteration 76, loss = 1.56610014\n",
      "Iteration 77, loss = 1.56459491\n",
      "Iteration 78, loss = 1.56304164\n",
      "Iteration 79, loss = 1.56172141\n",
      "Iteration 80, loss = 1.56027492\n",
      "Iteration 81, loss = 1.55883680\n",
      "Iteration 82, loss = 1.55735830\n",
      "Iteration 83, loss = 1.55585412\n",
      "Iteration 84, loss = 1.55445338\n",
      "Iteration 85, loss = 1.55299108\n",
      "Iteration 86, loss = 1.55165758\n",
      "Iteration 87, loss = 1.55027235\n",
      "Iteration 88, loss = 1.54879124\n",
      "Iteration 89, loss = 1.54744054\n",
      "Iteration 90, loss = 1.54622282\n",
      "Iteration 91, loss = 1.54475572\n",
      "Iteration 92, loss = 1.54336729\n",
      "Iteration 93, loss = 1.54183588\n",
      "Iteration 94, loss = 1.54055318\n",
      "Iteration 95, loss = 1.53917974\n",
      "Iteration 96, loss = 1.53772611\n",
      "Iteration 97, loss = 1.53647016\n",
      "Iteration 98, loss = 1.53502544\n",
      "Iteration 99, loss = 1.53372403\n",
      "Iteration 100, loss = 1.53220043\n",
      "Iteration 101, loss = 1.53088854\n",
      "Iteration 102, loss = 1.52973140\n",
      "Iteration 103, loss = 1.52830551\n",
      "Iteration 104, loss = 1.52699244\n",
      "Iteration 105, loss = 1.52580968\n",
      "Iteration 106, loss = 1.52442870\n",
      "Iteration 107, loss = 1.52311496\n",
      "Iteration 108, loss = 1.52181688\n",
      "Iteration 109, loss = 1.52057420\n",
      "Iteration 110, loss = 1.51927153\n",
      "Iteration 111, loss = 1.51804610\n",
      "Iteration 112, loss = 1.51678099\n",
      "Iteration 113, loss = 1.51537948\n",
      "Iteration 114, loss = 1.51405622\n",
      "Iteration 115, loss = 1.51286297\n",
      "Iteration 116, loss = 1.51164562\n",
      "Iteration 117, loss = 1.51033655\n",
      "Iteration 118, loss = 1.50909991\n",
      "Iteration 119, loss = 1.50783493\n",
      "Iteration 120, loss = 1.50663947\n",
      "Iteration 121, loss = 1.50530901\n",
      "Iteration 122, loss = 1.50401433\n",
      "Iteration 123, loss = 1.50271515\n",
      "Iteration 124, loss = 1.50146544\n",
      "Iteration 125, loss = 1.50018650\n",
      "Iteration 126, loss = 1.49897047\n",
      "Iteration 127, loss = 1.49773357\n",
      "Iteration 128, loss = 1.49649444\n",
      "Iteration 129, loss = 1.49523994\n",
      "Iteration 130, loss = 1.49391064\n",
      "Iteration 131, loss = 1.49250994\n",
      "Iteration 132, loss = 1.49124624\n",
      "Iteration 133, loss = 1.48967903\n",
      "Iteration 134, loss = 1.48828954\n",
      "Iteration 135, loss = 1.48677508\n",
      "Iteration 136, loss = 1.48532302\n",
      "Iteration 137, loss = 1.48397882\n",
      "Iteration 138, loss = 1.48264130\n",
      "Iteration 139, loss = 1.48118903\n",
      "Iteration 140, loss = 1.47987634\n",
      "Iteration 141, loss = 1.47873015\n",
      "Iteration 142, loss = 1.47726182\n",
      "Iteration 143, loss = 1.47604500\n",
      "Iteration 144, loss = 1.47479008\n",
      "Iteration 145, loss = 1.47357591\n",
      "Iteration 146, loss = 1.47226336\n",
      "Iteration 147, loss = 1.47123757\n",
      "Iteration 148, loss = 1.47003319\n",
      "Iteration 149, loss = 1.46897558\n",
      "Iteration 150, loss = 1.46785823\n",
      "Iteration 151, loss = 1.46668734\n",
      "Iteration 152, loss = 1.46567962\n",
      "Iteration 153, loss = 1.46456463\n",
      "Iteration 154, loss = 1.46343496\n",
      "Iteration 155, loss = 1.46244041\n",
      "Iteration 156, loss = 1.46131998\n",
      "Iteration 157, loss = 1.46030530\n",
      "Iteration 158, loss = 1.45928039\n",
      "Iteration 159, loss = 1.45821277\n",
      "Iteration 160, loss = 1.45711718\n",
      "Iteration 161, loss = 1.45613001\n",
      "Iteration 162, loss = 1.45505071\n",
      "Iteration 163, loss = 1.45402417\n",
      "Iteration 164, loss = 1.45290254\n",
      "Iteration 165, loss = 1.45185892\n",
      "Iteration 166, loss = 1.45084805\n",
      "Iteration 167, loss = 1.44979505\n",
      "Iteration 168, loss = 1.44880564\n",
      "Iteration 169, loss = 1.44777005\n",
      "Iteration 170, loss = 1.44676020\n",
      "Iteration 171, loss = 1.44567122\n",
      "Iteration 172, loss = 1.44454967\n",
      "Iteration 173, loss = 1.44323824\n",
      "Iteration 174, loss = 1.44201700\n",
      "Iteration 175, loss = 1.44097340\n",
      "Iteration 176, loss = 1.43989528\n",
      "Iteration 177, loss = 1.43884077\n",
      "Iteration 178, loss = 1.43780685\n",
      "Iteration 179, loss = 1.43676142\n",
      "Iteration 180, loss = 1.43586614\n",
      "Iteration 181, loss = 1.43483769\n",
      "Iteration 182, loss = 1.43385437\n",
      "Iteration 183, loss = 1.43284108\n",
      "Iteration 184, loss = 1.43185920\n",
      "Iteration 185, loss = 1.43093878\n",
      "Iteration 186, loss = 1.43001300\n",
      "Iteration 187, loss = 1.42908116\n",
      "Iteration 188, loss = 1.42800006\n",
      "Iteration 189, loss = 1.42704798\n",
      "Iteration 190, loss = 1.42603819\n",
      "Iteration 191, loss = 1.42505987\n",
      "Iteration 192, loss = 1.42409128\n",
      "Iteration 193, loss = 1.42309008\n",
      "Iteration 194, loss = 1.42220521\n",
      "Iteration 195, loss = 1.42129436\n",
      "Iteration 196, loss = 1.42036246\n",
      "Iteration 197, loss = 1.41948978\n",
      "Iteration 198, loss = 1.41855830\n",
      "Iteration 199, loss = 1.41773389\n",
      "Iteration 200, loss = 1.41682864\n",
      "Iteration 1, loss = 1.37886156\n",
      "Iteration 2, loss = 1.32713338\n",
      "Iteration 3, loss = 1.27860313\n",
      "Iteration 4, loss = 1.28944065\n",
      "Iteration 5, loss = 1.26350804\n",
      "Iteration 6, loss = 1.24176395\n",
      "Iteration 7, loss = 1.24490534\n",
      "Iteration 8, loss = 1.24775675\n",
      "Iteration 9, loss = 1.24246647\n",
      "Iteration 10, loss = 1.25078487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 1.23983707\n",
      "Iteration 12, loss = 1.23954406\n",
      "Iteration 13, loss = 1.24495927\n",
      "Iteration 14, loss = 1.23858960\n",
      "Iteration 15, loss = 1.25036186\n",
      "Iteration 16, loss = 1.25430397\n",
      "Iteration 17, loss = 1.24953837\n",
      "Iteration 18, loss = 1.24085923\n",
      "Iteration 19, loss = 1.24924893\n",
      "Iteration 20, loss = 1.24207741\n",
      "Iteration 21, loss = 1.24775197\n",
      "Iteration 22, loss = 1.23983304\n",
      "Iteration 23, loss = 1.26668854\n",
      "Iteration 24, loss = 1.24686169\n",
      "Iteration 25, loss = 1.24792886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51707291\n",
      "Iteration 2, loss = 1.51489951\n",
      "Iteration 3, loss = 1.51289061\n",
      "Iteration 4, loss = 1.51071077\n",
      "Iteration 5, loss = 1.50872093\n",
      "Iteration 6, loss = 1.50653622\n",
      "Iteration 7, loss = 1.50461853\n",
      "Iteration 8, loss = 1.50253393\n",
      "Iteration 9, loss = 1.50059848\n",
      "Iteration 10, loss = 1.49884866\n",
      "Iteration 11, loss = 1.49702390\n",
      "Iteration 12, loss = 1.49541149\n",
      "Iteration 13, loss = 1.49368414\n",
      "Iteration 14, loss = 1.49216043\n",
      "Iteration 15, loss = 1.49052740\n",
      "Iteration 16, loss = 1.48903042\n",
      "Iteration 17, loss = 1.48759888\n",
      "Iteration 18, loss = 1.48609683\n",
      "Iteration 19, loss = 1.48477737\n",
      "Iteration 20, loss = 1.48352713\n",
      "Iteration 21, loss = 1.48206631\n",
      "Iteration 22, loss = 1.48079804\n",
      "Iteration 23, loss = 1.47957046\n",
      "Iteration 24, loss = 1.47821691\n",
      "Iteration 25, loss = 1.47708891\n",
      "Iteration 26, loss = 1.47572738\n",
      "Iteration 27, loss = 1.47453552\n",
      "Iteration 28, loss = 1.47332389\n",
      "Iteration 29, loss = 1.47208256\n",
      "Iteration 30, loss = 1.47097112\n",
      "Iteration 31, loss = 1.46974868\n",
      "Iteration 32, loss = 1.46847400\n",
      "Iteration 33, loss = 1.46733083\n",
      "Iteration 34, loss = 1.46610864\n",
      "Iteration 35, loss = 1.46505526\n",
      "Iteration 36, loss = 1.46385840\n",
      "Iteration 37, loss = 1.46272840\n",
      "Iteration 38, loss = 1.46149361\n",
      "Iteration 39, loss = 1.46046267\n",
      "Iteration 40, loss = 1.45938987\n",
      "Iteration 41, loss = 1.45825115\n",
      "Iteration 42, loss = 1.45736067\n",
      "Iteration 43, loss = 1.45615207\n",
      "Iteration 44, loss = 1.45517361\n",
      "Iteration 45, loss = 1.45413791\n",
      "Iteration 46, loss = 1.45313101\n",
      "Iteration 47, loss = 1.45202424\n",
      "Iteration 48, loss = 1.45106146\n",
      "Iteration 49, loss = 1.45004774\n",
      "Iteration 50, loss = 1.44892038\n",
      "Iteration 51, loss = 1.44795898\n",
      "Iteration 52, loss = 1.44690584\n",
      "Iteration 53, loss = 1.44584006\n",
      "Iteration 54, loss = 1.44501103\n",
      "Iteration 55, loss = 1.44382819\n",
      "Iteration 56, loss = 1.44286572\n",
      "Iteration 57, loss = 1.44186257\n",
      "Iteration 58, loss = 1.44079889\n",
      "Iteration 59, loss = 1.43980291\n",
      "Iteration 60, loss = 1.43883015\n",
      "Iteration 61, loss = 1.43785926\n",
      "Iteration 62, loss = 1.43682282\n",
      "Iteration 63, loss = 1.43577583\n",
      "Iteration 64, loss = 1.43483325\n",
      "Iteration 65, loss = 1.43376160\n",
      "Iteration 66, loss = 1.43292260\n",
      "Iteration 67, loss = 1.43189155\n",
      "Iteration 68, loss = 1.43091847\n",
      "Iteration 69, loss = 1.43001236\n",
      "Iteration 70, loss = 1.42895445\n",
      "Iteration 71, loss = 1.42814323\n",
      "Iteration 72, loss = 1.42718980\n",
      "Iteration 73, loss = 1.42620763\n",
      "Iteration 74, loss = 1.42530860\n",
      "Iteration 75, loss = 1.42440731\n",
      "Iteration 76, loss = 1.42351441\n",
      "Iteration 77, loss = 1.42254348\n",
      "Iteration 78, loss = 1.42166636\n",
      "Iteration 79, loss = 1.42086555\n",
      "Iteration 80, loss = 1.42004707\n",
      "Iteration 81, loss = 1.41912768\n",
      "Iteration 82, loss = 1.41822811\n",
      "Iteration 83, loss = 1.41737393\n",
      "Iteration 84, loss = 1.41648874\n",
      "Iteration 85, loss = 1.41567591\n",
      "Iteration 86, loss = 1.41483317\n",
      "Iteration 87, loss = 1.41400896\n",
      "Iteration 88, loss = 1.41313004\n",
      "Iteration 89, loss = 1.41245073\n",
      "Iteration 90, loss = 1.41152533\n",
      "Iteration 91, loss = 1.41078469\n",
      "Iteration 92, loss = 1.40987491\n",
      "Iteration 93, loss = 1.40911452\n",
      "Iteration 94, loss = 1.40818984\n",
      "Iteration 95, loss = 1.40744840\n",
      "Iteration 96, loss = 1.40653083\n",
      "Iteration 97, loss = 1.40568976\n",
      "Iteration 98, loss = 1.40493490\n",
      "Iteration 99, loss = 1.40410730\n",
      "Iteration 100, loss = 1.40331844\n",
      "Iteration 101, loss = 1.40247627\n",
      "Iteration 102, loss = 1.40173412\n",
      "Iteration 103, loss = 1.40092063\n",
      "Iteration 104, loss = 1.40024678\n",
      "Iteration 105, loss = 1.39944318\n",
      "Iteration 106, loss = 1.39856084\n",
      "Iteration 107, loss = 1.39782271\n",
      "Iteration 108, loss = 1.39701420\n",
      "Iteration 109, loss = 1.39622118\n",
      "Iteration 110, loss = 1.39546061\n",
      "Iteration 111, loss = 1.39466202\n",
      "Iteration 112, loss = 1.39400519\n",
      "Iteration 113, loss = 1.39314825\n",
      "Iteration 114, loss = 1.39240501\n",
      "Iteration 115, loss = 1.39172085\n",
      "Iteration 116, loss = 1.39101821\n",
      "Iteration 117, loss = 1.39035620\n",
      "Iteration 118, loss = 1.38961679\n",
      "Iteration 119, loss = 1.38888769\n",
      "Iteration 120, loss = 1.38818340\n",
      "Iteration 121, loss = 1.38743693\n",
      "Iteration 122, loss = 1.38676098\n",
      "Iteration 123, loss = 1.38605546\n",
      "Iteration 124, loss = 1.38533685\n",
      "Iteration 125, loss = 1.38448934\n",
      "Iteration 126, loss = 1.38378929\n",
      "Iteration 127, loss = 1.38319125\n",
      "Iteration 128, loss = 1.38236758\n",
      "Iteration 129, loss = 1.38172075\n",
      "Iteration 130, loss = 1.38097744\n",
      "Iteration 131, loss = 1.38033986\n",
      "Iteration 132, loss = 1.37957453\n",
      "Iteration 133, loss = 1.37888410\n",
      "Iteration 134, loss = 1.37807903\n",
      "Iteration 135, loss = 1.37750252\n",
      "Iteration 136, loss = 1.37678033\n",
      "Iteration 137, loss = 1.37613449\n",
      "Iteration 138, loss = 1.37543105\n",
      "Iteration 139, loss = 1.37472597\n",
      "Iteration 140, loss = 1.37405344\n",
      "Iteration 141, loss = 1.37335466\n",
      "Iteration 142, loss = 1.37272165\n",
      "Iteration 143, loss = 1.37211196\n",
      "Iteration 144, loss = 1.37135340\n",
      "Iteration 145, loss = 1.37070479\n",
      "Iteration 146, loss = 1.37007191\n",
      "Iteration 147, loss = 1.36938551\n",
      "Iteration 148, loss = 1.36872473\n",
      "Iteration 149, loss = 1.36811244\n",
      "Iteration 150, loss = 1.36751735\n",
      "Iteration 151, loss = 1.36676982\n",
      "Iteration 152, loss = 1.36623403\n",
      "Iteration 153, loss = 1.36563957\n",
      "Iteration 154, loss = 1.36485450\n",
      "Iteration 155, loss = 1.36427092\n",
      "Iteration 156, loss = 1.36361717\n",
      "Iteration 157, loss = 1.36319867\n",
      "Iteration 158, loss = 1.36229073\n",
      "Iteration 159, loss = 1.36173156\n",
      "Iteration 160, loss = 1.36117702\n",
      "Iteration 161, loss = 1.36052397\n",
      "Iteration 162, loss = 1.35987105\n",
      "Iteration 163, loss = 1.35926883\n",
      "Iteration 164, loss = 1.35874026\n",
      "Iteration 165, loss = 1.35805957\n",
      "Iteration 166, loss = 1.35755095\n",
      "Iteration 167, loss = 1.35694937\n",
      "Iteration 168, loss = 1.35635634\n",
      "Iteration 169, loss = 1.35580445\n",
      "Iteration 170, loss = 1.35523759\n",
      "Iteration 171, loss = 1.35470047\n",
      "Iteration 172, loss = 1.35413141\n",
      "Iteration 173, loss = 1.35349727\n",
      "Iteration 174, loss = 1.35299103\n",
      "Iteration 175, loss = 1.35238629\n",
      "Iteration 176, loss = 1.35179557\n",
      "Iteration 177, loss = 1.35123048\n",
      "Iteration 178, loss = 1.35069756\n",
      "Iteration 179, loss = 1.35011212\n",
      "Iteration 180, loss = 1.34949624\n",
      "Iteration 181, loss = 1.34891567\n",
      "Iteration 182, loss = 1.34827761\n",
      "Iteration 183, loss = 1.34766654\n",
      "Iteration 184, loss = 1.34713659\n",
      "Iteration 185, loss = 1.34656263\n",
      "Iteration 186, loss = 1.34603632\n",
      "Iteration 187, loss = 1.34537249\n",
      "Iteration 188, loss = 1.34491672\n",
      "Iteration 189, loss = 1.34426740\n",
      "Iteration 190, loss = 1.34375882\n",
      "Iteration 191, loss = 1.34321169\n",
      "Iteration 192, loss = 1.34266210\n",
      "Iteration 193, loss = 1.34213397\n",
      "Iteration 194, loss = 1.34158436\n",
      "Iteration 195, loss = 1.34105784\n",
      "Iteration 196, loss = 1.34053098\n",
      "Iteration 197, loss = 1.33994418\n",
      "Iteration 198, loss = 1.33943024\n",
      "Iteration 199, loss = 1.33887789\n",
      "Iteration 200, loss = 1.33834617\n",
      "Iteration 1, loss = 1.54151581\n",
      "Iteration 2, loss = 1.30151843\n",
      "Iteration 3, loss = 1.27525884\n",
      "Iteration 4, loss = 1.27605059\n",
      "Iteration 5, loss = 1.26154349\n",
      "Iteration 6, loss = 1.28142297\n",
      "Iteration 7, loss = 1.23847650\n",
      "Iteration 8, loss = 1.25921805\n",
      "Iteration 9, loss = 1.24439184\n",
      "Iteration 10, loss = 1.24714149\n",
      "Iteration 11, loss = 1.24485289\n",
      "Iteration 12, loss = 1.24304136\n",
      "Iteration 13, loss = 1.25615493\n",
      "Iteration 14, loss = 1.24820447\n",
      "Iteration 15, loss = 1.24189499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 1.24937543\n",
      "Iteration 17, loss = 1.24185167\n",
      "Iteration 18, loss = 1.24858534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.80578303\n",
      "Iteration 2, loss = 1.80313940\n",
      "Iteration 3, loss = 1.80056531\n",
      "Iteration 4, loss = 1.79799555\n",
      "Iteration 5, loss = 1.79549362\n",
      "Iteration 6, loss = 1.79297595\n",
      "Iteration 7, loss = 1.79039713\n",
      "Iteration 8, loss = 1.78791487\n",
      "Iteration 9, loss = 1.78525792\n",
      "Iteration 10, loss = 1.78278841\n",
      "Iteration 11, loss = 1.78009294\n",
      "Iteration 12, loss = 1.77765262\n",
      "Iteration 13, loss = 1.77506596\n",
      "Iteration 14, loss = 1.77256906\n",
      "Iteration 15, loss = 1.77008444\n",
      "Iteration 16, loss = 1.76758248\n",
      "Iteration 17, loss = 1.76512110\n",
      "Iteration 18, loss = 1.76246463\n",
      "Iteration 19, loss = 1.75979129\n",
      "Iteration 20, loss = 1.75731176\n",
      "Iteration 21, loss = 1.75461758\n",
      "Iteration 22, loss = 1.75189150\n",
      "Iteration 23, loss = 1.74918757\n",
      "Iteration 24, loss = 1.74664388\n",
      "Iteration 25, loss = 1.74421910\n",
      "Iteration 26, loss = 1.74156237\n",
      "Iteration 27, loss = 1.73925271\n",
      "Iteration 28, loss = 1.73676010\n",
      "Iteration 29, loss = 1.73456967\n",
      "Iteration 30, loss = 1.73250013\n",
      "Iteration 31, loss = 1.73014326\n",
      "Iteration 32, loss = 1.72799226\n",
      "Iteration 33, loss = 1.72603395\n",
      "Iteration 34, loss = 1.72374191\n",
      "Iteration 35, loss = 1.72173499\n",
      "Iteration 36, loss = 1.71958911\n",
      "Iteration 37, loss = 1.71759699\n",
      "Iteration 38, loss = 1.71554886\n",
      "Iteration 39, loss = 1.71345354\n",
      "Iteration 40, loss = 1.71145406\n",
      "Iteration 41, loss = 1.70945500\n",
      "Iteration 42, loss = 1.70739081\n",
      "Iteration 43, loss = 1.70544643\n",
      "Iteration 44, loss = 1.70330080\n",
      "Iteration 45, loss = 1.70127815\n",
      "Iteration 46, loss = 1.69913526\n",
      "Iteration 47, loss = 1.69685409\n",
      "Iteration 48, loss = 1.69470132\n",
      "Iteration 49, loss = 1.69225082\n",
      "Iteration 50, loss = 1.68941156\n",
      "Iteration 51, loss = 1.68611887\n",
      "Iteration 52, loss = 1.68207929\n",
      "Iteration 53, loss = 1.67730745\n",
      "Iteration 54, loss = 1.67241591\n",
      "Iteration 55, loss = 1.66778842\n",
      "Iteration 56, loss = 1.66375509\n",
      "Iteration 57, loss = 1.66025363\n",
      "Iteration 58, loss = 1.65641749\n",
      "Iteration 59, loss = 1.65322722\n",
      "Iteration 60, loss = 1.65001785\n",
      "Iteration 61, loss = 1.64701328\n",
      "Iteration 62, loss = 1.64376563\n",
      "Iteration 63, loss = 1.64093183\n",
      "Iteration 64, loss = 1.63779405\n",
      "Iteration 65, loss = 1.63519350\n",
      "Iteration 66, loss = 1.63264436\n",
      "Iteration 67, loss = 1.63031442\n",
      "Iteration 68, loss = 1.62769135\n",
      "Iteration 69, loss = 1.62545472\n",
      "Iteration 70, loss = 1.62322756\n",
      "Iteration 71, loss = 1.62082776\n",
      "Iteration 72, loss = 1.61854912\n",
      "Iteration 73, loss = 1.61602646\n",
      "Iteration 74, loss = 1.61351698\n",
      "Iteration 75, loss = 1.61076472\n",
      "Iteration 76, loss = 1.60848465\n",
      "Iteration 77, loss = 1.60589592\n",
      "Iteration 78, loss = 1.60372776\n",
      "Iteration 79, loss = 1.60162971\n",
      "Iteration 80, loss = 1.59953940\n",
      "Iteration 81, loss = 1.59767994\n",
      "Iteration 82, loss = 1.59597217\n",
      "Iteration 83, loss = 1.59415843\n",
      "Iteration 84, loss = 1.59253058\n",
      "Iteration 85, loss = 1.59088289\n",
      "Iteration 86, loss = 1.58931849\n",
      "Iteration 87, loss = 1.58768124\n",
      "Iteration 88, loss = 1.58609740\n",
      "Iteration 89, loss = 1.58449961\n",
      "Iteration 90, loss = 1.58292654\n",
      "Iteration 91, loss = 1.58138138\n",
      "Iteration 92, loss = 1.57998013\n",
      "Iteration 93, loss = 1.57821797\n",
      "Iteration 94, loss = 1.57660834\n",
      "Iteration 95, loss = 1.57510799\n",
      "Iteration 96, loss = 1.57345864\n",
      "Iteration 97, loss = 1.57173122\n",
      "Iteration 98, loss = 1.57016474\n",
      "Iteration 99, loss = 1.56851135\n",
      "Iteration 100, loss = 1.56696765\n",
      "Iteration 101, loss = 1.56524579\n",
      "Iteration 102, loss = 1.56374545\n",
      "Iteration 103, loss = 1.56213736\n",
      "Iteration 104, loss = 1.56052623\n",
      "Iteration 105, loss = 1.55899779\n",
      "Iteration 106, loss = 1.55762652\n",
      "Iteration 107, loss = 1.55602831\n",
      "Iteration 108, loss = 1.55465832\n",
      "Iteration 109, loss = 1.55321372\n",
      "Iteration 110, loss = 1.55182123\n",
      "Iteration 111, loss = 1.55035012\n",
      "Iteration 112, loss = 1.54899843\n",
      "Iteration 113, loss = 1.54751146\n",
      "Iteration 114, loss = 1.54611566\n",
      "Iteration 115, loss = 1.54475630\n",
      "Iteration 116, loss = 1.54347327\n",
      "Iteration 117, loss = 1.54202666\n",
      "Iteration 118, loss = 1.54071379\n",
      "Iteration 119, loss = 1.53945145\n",
      "Iteration 120, loss = 1.53804241\n",
      "Iteration 121, loss = 1.53671618\n",
      "Iteration 122, loss = 1.53541477\n",
      "Iteration 123, loss = 1.53410043\n",
      "Iteration 124, loss = 1.53273433\n",
      "Iteration 125, loss = 1.53147859\n",
      "Iteration 126, loss = 1.53007513\n",
      "Iteration 127, loss = 1.52889840\n",
      "Iteration 128, loss = 1.52750343\n",
      "Iteration 129, loss = 1.52619280\n",
      "Iteration 130, loss = 1.52488960\n",
      "Iteration 131, loss = 1.52362710\n",
      "Iteration 132, loss = 1.52229979\n",
      "Iteration 133, loss = 1.52095384\n",
      "Iteration 134, loss = 1.51972639\n",
      "Iteration 135, loss = 1.51852873\n",
      "Iteration 136, loss = 1.51724083\n",
      "Iteration 137, loss = 1.51591281\n",
      "Iteration 138, loss = 1.51464570\n",
      "Iteration 139, loss = 1.51348957\n",
      "Iteration 140, loss = 1.51209900\n",
      "Iteration 141, loss = 1.51091694\n",
      "Iteration 142, loss = 1.50965554\n",
      "Iteration 143, loss = 1.50843411\n",
      "Iteration 144, loss = 1.50711390\n",
      "Iteration 145, loss = 1.50603423\n",
      "Iteration 146, loss = 1.50471303\n",
      "Iteration 147, loss = 1.50359055\n",
      "Iteration 148, loss = 1.50235960\n",
      "Iteration 149, loss = 1.50112743\n",
      "Iteration 150, loss = 1.49995350\n",
      "Iteration 151, loss = 1.49871310\n",
      "Iteration 152, loss = 1.49751110\n",
      "Iteration 153, loss = 1.49636349\n",
      "Iteration 154, loss = 1.49513582\n",
      "Iteration 155, loss = 1.49399529\n",
      "Iteration 156, loss = 1.49281719\n",
      "Iteration 157, loss = 1.49162009\n",
      "Iteration 158, loss = 1.49057200\n",
      "Iteration 159, loss = 1.48925922\n",
      "Iteration 160, loss = 1.48816977\n",
      "Iteration 161, loss = 1.48694307\n",
      "Iteration 162, loss = 1.48586884\n",
      "Iteration 163, loss = 1.48463986\n",
      "Iteration 164, loss = 1.48357810\n",
      "Iteration 165, loss = 1.48242436\n",
      "Iteration 166, loss = 1.48113628\n",
      "Iteration 167, loss = 1.48001613\n",
      "Iteration 168, loss = 1.47879310\n",
      "Iteration 169, loss = 1.47760415\n",
      "Iteration 170, loss = 1.47641012\n",
      "Iteration 171, loss = 1.47534695\n",
      "Iteration 172, loss = 1.47412618\n",
      "Iteration 173, loss = 1.47302508\n",
      "Iteration 174, loss = 1.47181653\n",
      "Iteration 175, loss = 1.47074523\n",
      "Iteration 176, loss = 1.46966603\n",
      "Iteration 177, loss = 1.46850452\n",
      "Iteration 178, loss = 1.46739203\n",
      "Iteration 179, loss = 1.46618748\n",
      "Iteration 180, loss = 1.46508837\n",
      "Iteration 181, loss = 1.46390524\n",
      "Iteration 182, loss = 1.46284437\n",
      "Iteration 183, loss = 1.46173389\n",
      "Iteration 184, loss = 1.46062675\n",
      "Iteration 185, loss = 1.45953927\n",
      "Iteration 186, loss = 1.45843063\n",
      "Iteration 187, loss = 1.45740277\n",
      "Iteration 188, loss = 1.45633255\n",
      "Iteration 189, loss = 1.45524827\n",
      "Iteration 190, loss = 1.45421187\n",
      "Iteration 191, loss = 1.45314986\n",
      "Iteration 192, loss = 1.45212677\n",
      "Iteration 193, loss = 1.45103670\n",
      "Iteration 194, loss = 1.44996110\n",
      "Iteration 195, loss = 1.44890360\n",
      "Iteration 196, loss = 1.44782279\n",
      "Iteration 197, loss = 1.44677890\n",
      "Iteration 198, loss = 1.44573169\n",
      "Iteration 199, loss = 1.44460965\n",
      "Iteration 200, loss = 1.44359735\n",
      "Iteration 1, loss = 1.38837365\n",
      "Iteration 2, loss = 1.25934701\n",
      "Iteration 3, loss = 1.29075985\n",
      "Iteration 4, loss = 1.25243025\n",
      "Iteration 5, loss = 1.25284432\n",
      "Iteration 6, loss = 1.24853946\n",
      "Iteration 7, loss = 1.25200736\n",
      "Iteration 8, loss = 1.24320049\n",
      "Iteration 9, loss = 1.25167498\n",
      "Iteration 10, loss = 1.23461979\n",
      "Iteration 11, loss = 1.25572635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.24820812\n",
      "Iteration 13, loss = 1.26154908\n",
      "Iteration 14, loss = 1.25330789\n",
      "Iteration 15, loss = 1.24272951\n",
      "Iteration 16, loss = 1.24144983\n",
      "Iteration 17, loss = 1.26705607\n",
      "Iteration 18, loss = 1.24658067\n",
      "Iteration 19, loss = 1.24387069\n",
      "Iteration 20, loss = 1.24529975\n",
      "Iteration 21, loss = 1.24487293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.36329865\n",
      "Iteration 2, loss = 9.84146572\n",
      "Iteration 3, loss = 7.19975851\n",
      "Iteration 4, loss = 5.34905162\n",
      "Iteration 5, loss = 3.97536897\n",
      "Iteration 6, loss = 2.93922639\n",
      "Iteration 7, loss = 2.19079615\n",
      "Iteration 8, loss = 1.80961109\n",
      "Iteration 9, loss = 1.63707336\n",
      "Iteration 10, loss = 1.57027162\n",
      "Iteration 11, loss = 1.53952485\n",
      "Iteration 12, loss = 1.52594649\n",
      "Iteration 13, loss = 1.50914603\n",
      "Iteration 14, loss = 1.49824638\n",
      "Iteration 15, loss = 1.48340500\n",
      "Iteration 16, loss = 1.47311576\n",
      "Iteration 17, loss = 1.46234183\n",
      "Iteration 18, loss = 1.45529518\n",
      "Iteration 19, loss = 1.44176518\n",
      "Iteration 20, loss = 1.43193132\n",
      "Iteration 21, loss = 1.42224946\n",
      "Iteration 22, loss = 1.41353267\n",
      "Iteration 23, loss = 1.40447494\n",
      "Iteration 24, loss = 1.39629493\n",
      "Iteration 25, loss = 1.38819761\n",
      "Iteration 26, loss = 1.38113558\n",
      "Iteration 27, loss = 1.37481184\n",
      "Iteration 28, loss = 1.36898785\n",
      "Iteration 29, loss = 1.36405990\n",
      "Iteration 30, loss = 1.35911949\n",
      "Iteration 31, loss = 1.35395105\n",
      "Iteration 32, loss = 1.34916472\n",
      "Iteration 33, loss = 1.34461655\n",
      "Iteration 34, loss = 1.34020684\n",
      "Iteration 35, loss = 1.33631268\n",
      "Iteration 36, loss = 1.33251959\n",
      "Iteration 37, loss = 1.32920549\n",
      "Iteration 38, loss = 1.32585575\n",
      "Iteration 39, loss = 1.32238234\n",
      "Iteration 40, loss = 1.31948225\n",
      "Iteration 41, loss = 1.31658997\n",
      "Iteration 42, loss = 1.31384774\n",
      "Iteration 43, loss = 1.31126357\n",
      "Iteration 44, loss = 1.30895602\n",
      "Iteration 45, loss = 1.30639460\n",
      "Iteration 46, loss = 1.30432864\n",
      "Iteration 47, loss = 1.30249563\n",
      "Iteration 48, loss = 1.30050891\n",
      "Iteration 49, loss = 1.29902546\n",
      "Iteration 50, loss = 1.29728545\n",
      "Iteration 51, loss = 1.29583567\n",
      "Iteration 52, loss = 1.29429177\n",
      "Iteration 53, loss = 1.29279961\n",
      "Iteration 54, loss = 1.29143319\n",
      "Iteration 55, loss = 1.29033255\n",
      "Iteration 56, loss = 1.28885328\n",
      "Iteration 57, loss = 1.28795606\n",
      "Iteration 58, loss = 1.28708384\n",
      "Iteration 59, loss = 1.28595104\n",
      "Iteration 60, loss = 1.28509397\n",
      "Iteration 61, loss = 1.28443896\n",
      "Iteration 62, loss = 1.28362111\n",
      "Iteration 63, loss = 1.28287595\n",
      "Iteration 64, loss = 1.28223407\n",
      "Iteration 65, loss = 1.28131181\n",
      "Iteration 66, loss = 1.28070770\n",
      "Iteration 67, loss = 1.28026304\n",
      "Iteration 68, loss = 1.27982624\n",
      "Iteration 69, loss = 1.27888730\n",
      "Iteration 70, loss = 1.27839893\n",
      "Iteration 71, loss = 1.27780609\n",
      "Iteration 72, loss = 1.27726078\n",
      "Iteration 73, loss = 1.27687890\n",
      "Iteration 74, loss = 1.27657802\n",
      "Iteration 75, loss = 1.27616729\n",
      "Iteration 76, loss = 1.27582470\n",
      "Iteration 77, loss = 1.27548432\n",
      "Iteration 78, loss = 1.27527016\n",
      "Iteration 79, loss = 1.27493625\n",
      "Iteration 80, loss = 1.27463796\n",
      "Iteration 81, loss = 1.27440986\n",
      "Iteration 82, loss = 1.27427372\n",
      "Iteration 83, loss = 1.27392550\n",
      "Iteration 84, loss = 1.27375722\n",
      "Iteration 85, loss = 1.27355688\n",
      "Iteration 86, loss = 1.27344950\n",
      "Iteration 87, loss = 1.27324744\n",
      "Iteration 88, loss = 1.27311297\n",
      "Iteration 89, loss = 1.27311079\n",
      "Iteration 90, loss = 1.27293375\n",
      "Iteration 91, loss = 1.27276415\n",
      "Iteration 92, loss = 1.27258805\n",
      "Iteration 93, loss = 1.27245797\n",
      "Iteration 94, loss = 1.27228801\n",
      "Iteration 95, loss = 1.27214209\n",
      "Iteration 96, loss = 1.27212085\n",
      "Iteration 97, loss = 1.27209344\n",
      "Iteration 98, loss = 1.27187552\n",
      "Iteration 99, loss = 1.27171790\n",
      "Iteration 100, loss = 1.27158606\n",
      "Iteration 101, loss = 1.27149933\n",
      "Iteration 102, loss = 1.27138247\n",
      "Iteration 103, loss = 1.27126963\n",
      "Iteration 104, loss = 1.27118912\n",
      "Iteration 105, loss = 1.27107449\n",
      "Iteration 106, loss = 1.27117576\n",
      "Iteration 107, loss = 1.27116000\n",
      "Iteration 108, loss = 1.27108541\n",
      "Iteration 109, loss = 1.27100093\n",
      "Iteration 110, loss = 1.27096879\n",
      "Iteration 111, loss = 1.27085937\n",
      "Iteration 112, loss = 1.27077864\n",
      "Iteration 113, loss = 1.27068858\n",
      "Iteration 114, loss = 1.27064691\n",
      "Iteration 115, loss = 1.27061649\n",
      "Iteration 116, loss = 1.27054456\n",
      "Iteration 117, loss = 1.27048562\n",
      "Iteration 118, loss = 1.27044832\n",
      "Iteration 119, loss = 1.27040567\n",
      "Iteration 120, loss = 1.27043562\n",
      "Iteration 121, loss = 1.27038658\n",
      "Iteration 122, loss = 1.27027343\n",
      "Iteration 123, loss = 1.27027970\n",
      "Iteration 124, loss = 1.27025800\n",
      "Iteration 125, loss = 1.27020366\n",
      "Iteration 126, loss = 1.27021022\n",
      "Iteration 127, loss = 1.27023905\n",
      "Iteration 128, loss = 1.27012061\n",
      "Iteration 129, loss = 1.27009504\n",
      "Iteration 130, loss = 1.27007871\n",
      "Iteration 131, loss = 1.27005611\n",
      "Iteration 132, loss = 1.27006494\n",
      "Iteration 133, loss = 1.27014301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.69653878\n",
      "Iteration 2, loss = 10.30744783\n",
      "Iteration 3, loss = 9.89194727\n",
      "Iteration 4, loss = 9.49353062\n",
      "Iteration 5, loss = 9.09390868\n",
      "Iteration 6, loss = 8.70529379\n",
      "Iteration 7, loss = 8.37160782\n",
      "Iteration 8, loss = 8.01404632\n",
      "Iteration 9, loss = 7.67450608\n",
      "Iteration 10, loss = 7.37855207\n",
      "Iteration 11, loss = 7.06029504\n",
      "Iteration 12, loss = 6.80371937\n",
      "Iteration 13, loss = 6.52861255\n",
      "Iteration 14, loss = 6.29462887\n",
      "Iteration 15, loss = 6.05986106\n",
      "Iteration 16, loss = 5.80270327\n",
      "Iteration 17, loss = 5.56036770\n",
      "Iteration 18, loss = 5.36204693\n",
      "Iteration 19, loss = 5.13127111\n",
      "Iteration 20, loss = 4.92571938\n",
      "Iteration 21, loss = 4.70729208\n",
      "Iteration 22, loss = 4.51397525\n",
      "Iteration 23, loss = 4.32051038\n",
      "Iteration 24, loss = 4.15560181\n",
      "Iteration 25, loss = 3.97692608\n",
      "Iteration 26, loss = 3.81375983\n",
      "Iteration 27, loss = 3.67717048\n",
      "Iteration 28, loss = 3.51977637\n",
      "Iteration 29, loss = 3.40273530\n",
      "Iteration 30, loss = 3.25563889\n",
      "Iteration 31, loss = 3.11151969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 2.99963090\n",
      "Iteration 33, loss = 2.88947037\n",
      "Iteration 34, loss = 2.79319923\n",
      "Iteration 35, loss = 2.70263828\n",
      "Iteration 36, loss = 2.63628183\n",
      "Iteration 37, loss = 2.57633043\n",
      "Iteration 38, loss = 2.51750612\n",
      "Iteration 39, loss = 2.47281452\n",
      "Iteration 40, loss = 2.41519778\n",
      "Iteration 41, loss = 2.37680603\n",
      "Iteration 42, loss = 2.32218692\n",
      "Iteration 43, loss = 2.28747558\n",
      "Iteration 44, loss = 2.24187736\n",
      "Iteration 45, loss = 2.19942825\n",
      "Iteration 46, loss = 2.16299773\n",
      "Iteration 47, loss = 2.13048293\n",
      "Iteration 48, loss = 2.09442546\n",
      "Iteration 49, loss = 2.06847724\n",
      "Iteration 50, loss = 2.03682253\n",
      "Iteration 51, loss = 2.00856682\n",
      "Iteration 52, loss = 1.98781697\n",
      "Iteration 53, loss = 1.96332377\n",
      "Iteration 54, loss = 1.94565596\n",
      "Iteration 55, loss = 1.92684800\n",
      "Iteration 56, loss = 1.90725646\n",
      "Iteration 57, loss = 1.89320358\n",
      "Iteration 58, loss = 1.87658865\n",
      "Iteration 59, loss = 1.86237009\n",
      "Iteration 60, loss = 1.84633121\n",
      "Iteration 61, loss = 1.83466315\n",
      "Iteration 62, loss = 1.81984475\n",
      "Iteration 63, loss = 1.80843614\n",
      "Iteration 64, loss = 1.79345044\n",
      "Iteration 65, loss = 1.78345750\n",
      "Iteration 66, loss = 1.77690173\n",
      "Iteration 67, loss = 1.76800401\n",
      "Iteration 68, loss = 1.75930422\n",
      "Iteration 69, loss = 1.75164825\n",
      "Iteration 70, loss = 1.74326804\n",
      "Iteration 71, loss = 1.73610015\n",
      "Iteration 72, loss = 1.73051052\n",
      "Iteration 73, loss = 1.72321611\n",
      "Iteration 74, loss = 1.71870181\n",
      "Iteration 75, loss = 1.71386403\n",
      "Iteration 76, loss = 1.70882317\n",
      "Iteration 77, loss = 1.70243103\n",
      "Iteration 78, loss = 1.69997295\n",
      "Iteration 79, loss = 1.69235499\n",
      "Iteration 80, loss = 1.68917193\n",
      "Iteration 81, loss = 1.68423132\n",
      "Iteration 82, loss = 1.67924186\n",
      "Iteration 83, loss = 1.67453108\n",
      "Iteration 84, loss = 1.67089299\n",
      "Iteration 85, loss = 1.66790548\n",
      "Iteration 86, loss = 1.66424748\n",
      "Iteration 87, loss = 1.66105310\n",
      "Iteration 88, loss = 1.65881623\n",
      "Iteration 89, loss = 1.65529537\n",
      "Iteration 90, loss = 1.65227999\n",
      "Iteration 91, loss = 1.64895767\n",
      "Iteration 92, loss = 1.64704929\n",
      "Iteration 93, loss = 1.64338378\n",
      "Iteration 94, loss = 1.64024112\n",
      "Iteration 95, loss = 1.63816889\n",
      "Iteration 96, loss = 1.63377017\n",
      "Iteration 97, loss = 1.62999340\n",
      "Iteration 98, loss = 1.62539539\n",
      "Iteration 99, loss = 1.62133738\n",
      "Iteration 100, loss = 1.61604155\n",
      "Iteration 101, loss = 1.61042994\n",
      "Iteration 102, loss = 1.60817946\n",
      "Iteration 103, loss = 1.60230034\n",
      "Iteration 104, loss = 1.59675174\n",
      "Iteration 105, loss = 1.59269468\n",
      "Iteration 106, loss = 1.58928257\n",
      "Iteration 107, loss = 1.58621568\n",
      "Iteration 108, loss = 1.58259445\n",
      "Iteration 109, loss = 1.57987033\n",
      "Iteration 110, loss = 1.57662992\n",
      "Iteration 111, loss = 1.57347521\n",
      "Iteration 112, loss = 1.56944124\n",
      "Iteration 113, loss = 1.56564167\n",
      "Iteration 114, loss = 1.56268789\n",
      "Iteration 115, loss = 1.55961462\n",
      "Iteration 116, loss = 1.55526933\n",
      "Iteration 117, loss = 1.55231276\n",
      "Iteration 118, loss = 1.54944559\n",
      "Iteration 119, loss = 1.54703237\n",
      "Iteration 120, loss = 1.54425552\n",
      "Iteration 121, loss = 1.53967059\n",
      "Iteration 122, loss = 1.53743201\n",
      "Iteration 123, loss = 1.53341634\n",
      "Iteration 124, loss = 1.53090118\n",
      "Iteration 125, loss = 1.52741080\n",
      "Iteration 126, loss = 1.52490870\n",
      "Iteration 127, loss = 1.52171983\n",
      "Iteration 128, loss = 1.51958971\n",
      "Iteration 129, loss = 1.51806713\n",
      "Iteration 130, loss = 1.51547387\n",
      "Iteration 131, loss = 1.51367839\n",
      "Iteration 132, loss = 1.51126073\n",
      "Iteration 133, loss = 1.50802185\n",
      "Iteration 134, loss = 1.50406342\n",
      "Iteration 135, loss = 1.50103265\n",
      "Iteration 136, loss = 1.49690060\n",
      "Iteration 137, loss = 1.49360804\n",
      "Iteration 138, loss = 1.49188361\n",
      "Iteration 139, loss = 1.48672460\n",
      "Iteration 140, loss = 1.48418375\n",
      "Iteration 141, loss = 1.48078716\n",
      "Iteration 142, loss = 1.47913773\n",
      "Iteration 143, loss = 1.47612699\n",
      "Iteration 144, loss = 1.47438551\n",
      "Iteration 145, loss = 1.47344630\n",
      "Iteration 146, loss = 1.47150644\n",
      "Iteration 147, loss = 1.47048305\n",
      "Iteration 148, loss = 1.46915947\n",
      "Iteration 149, loss = 1.46786295\n",
      "Iteration 150, loss = 1.46707287\n",
      "Iteration 151, loss = 1.46615714\n",
      "Iteration 152, loss = 1.46507183\n",
      "Iteration 153, loss = 1.46447093\n",
      "Iteration 154, loss = 1.46336701\n",
      "Iteration 155, loss = 1.46199019\n",
      "Iteration 156, loss = 1.46173183\n",
      "Iteration 157, loss = 1.45958682\n",
      "Iteration 158, loss = 1.45895510\n",
      "Iteration 159, loss = 1.45794555\n",
      "Iteration 160, loss = 1.45672379\n",
      "Iteration 161, loss = 1.45587140\n",
      "Iteration 162, loss = 1.45489222\n",
      "Iteration 163, loss = 1.45409738\n",
      "Iteration 164, loss = 1.45354953\n",
      "Iteration 165, loss = 1.45225807\n",
      "Iteration 166, loss = 1.45157851\n",
      "Iteration 167, loss = 1.45091784\n",
      "Iteration 168, loss = 1.45006875\n",
      "Iteration 169, loss = 1.44895903\n",
      "Iteration 170, loss = 1.44818661\n",
      "Iteration 171, loss = 1.44742998\n",
      "Iteration 172, loss = 1.44671072\n",
      "Iteration 173, loss = 1.44576775\n",
      "Iteration 174, loss = 1.44508874\n",
      "Iteration 175, loss = 1.44439806\n",
      "Iteration 176, loss = 1.44362494\n",
      "Iteration 177, loss = 1.44306856\n",
      "Iteration 178, loss = 1.44256864\n",
      "Iteration 179, loss = 1.44181617\n",
      "Iteration 180, loss = 1.44099576\n",
      "Iteration 181, loss = 1.44046572\n",
      "Iteration 182, loss = 1.43924647\n",
      "Iteration 183, loss = 1.43837651\n",
      "Iteration 184, loss = 1.43782584\n",
      "Iteration 185, loss = 1.43680674\n",
      "Iteration 186, loss = 1.43591635\n",
      "Iteration 187, loss = 1.43524050\n",
      "Iteration 188, loss = 1.43478409\n",
      "Iteration 189, loss = 1.43399612\n",
      "Iteration 190, loss = 1.43341180\n",
      "Iteration 191, loss = 1.43273604\n",
      "Iteration 192, loss = 1.43230989\n",
      "Iteration 193, loss = 1.43154550\n",
      "Iteration 194, loss = 1.43096578\n",
      "Iteration 195, loss = 1.43044833\n",
      "Iteration 196, loss = 1.42996688\n",
      "Iteration 197, loss = 1.42877927\n",
      "Iteration 198, loss = 1.42804272\n",
      "Iteration 199, loss = 1.42754937\n",
      "Iteration 200, loss = 1.42717430\n",
      "Iteration 1, loss = 4.10458032\n",
      "Iteration 2, loss = 1.31073249\n",
      "Iteration 3, loss = 1.25788077\n",
      "Iteration 4, loss = 1.25611847\n",
      "Iteration 5, loss = 1.26370398\n",
      "Iteration 6, loss = 1.26457686\n",
      "Iteration 7, loss = 1.25009213\n",
      "Iteration 8, loss = 1.25108878\n",
      "Iteration 9, loss = 1.25068892\n",
      "Iteration 10, loss = 1.24978458\n",
      "Iteration 11, loss = 1.24875216\n",
      "Iteration 12, loss = 1.25062678\n",
      "Iteration 13, loss = 1.24975378\n",
      "Iteration 14, loss = 1.25072659\n",
      "Iteration 15, loss = 1.25063562\n",
      "Iteration 16, loss = 1.25264712\n",
      "Iteration 17, loss = 1.24930258\n",
      "Iteration 18, loss = 1.24930032\n",
      "Iteration 19, loss = 1.25272231\n",
      "Iteration 20, loss = 1.25148685\n",
      "Iteration 21, loss = 1.24967534\n",
      "Iteration 22, loss = 1.25274614\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.93442012\n",
      "Iteration 2, loss = 8.67497303\n",
      "Iteration 3, loss = 8.44839626\n",
      "Iteration 4, loss = 8.22487856\n",
      "Iteration 5, loss = 7.98273562\n",
      "Iteration 6, loss = 7.77598413\n",
      "Iteration 7, loss = 7.57815274\n",
      "Iteration 8, loss = 7.35781171\n",
      "Iteration 9, loss = 7.16698959\n",
      "Iteration 10, loss = 6.96689482\n",
      "Iteration 11, loss = 6.76255733\n",
      "Iteration 12, loss = 6.58626295\n",
      "Iteration 13, loss = 6.38742450\n",
      "Iteration 14, loss = 6.18392106\n",
      "Iteration 15, loss = 6.01140954\n",
      "Iteration 16, loss = 5.82290423\n",
      "Iteration 17, loss = 5.65860673\n",
      "Iteration 18, loss = 5.46915535\n",
      "Iteration 19, loss = 5.28823675\n",
      "Iteration 20, loss = 5.08947477\n",
      "Iteration 21, loss = 4.92401047\n",
      "Iteration 22, loss = 4.74274388\n",
      "Iteration 23, loss = 4.56461096\n",
      "Iteration 24, loss = 4.43991646\n",
      "Iteration 25, loss = 4.28600315\n",
      "Iteration 26, loss = 4.17165475\n",
      "Iteration 27, loss = 4.03697208\n",
      "Iteration 28, loss = 3.93543166\n",
      "Iteration 29, loss = 3.83786647\n",
      "Iteration 30, loss = 3.72401242\n",
      "Iteration 31, loss = 3.64367732\n",
      "Iteration 32, loss = 3.53749232\n",
      "Iteration 33, loss = 3.44853110\n",
      "Iteration 34, loss = 3.37041221\n",
      "Iteration 35, loss = 3.27460035\n",
      "Iteration 36, loss = 3.18259517\n",
      "Iteration 37, loss = 3.11580991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 3.05204856\n",
      "Iteration 39, loss = 2.99293761\n",
      "Iteration 40, loss = 2.96259704\n",
      "Iteration 41, loss = 2.92530719\n",
      "Iteration 42, loss = 2.88813700\n",
      "Iteration 43, loss = 2.85977099\n",
      "Iteration 44, loss = 2.83302780\n",
      "Iteration 45, loss = 2.81032075\n",
      "Iteration 46, loss = 2.78968036\n",
      "Iteration 47, loss = 2.77343843\n",
      "Iteration 48, loss = 2.74637273\n",
      "Iteration 49, loss = 2.73057570\n",
      "Iteration 50, loss = 2.71528769\n",
      "Iteration 51, loss = 2.70342880\n",
      "Iteration 52, loss = 2.68636507\n",
      "Iteration 53, loss = 2.68069535\n",
      "Iteration 54, loss = 2.66013625\n",
      "Iteration 55, loss = 2.64601283\n",
      "Iteration 56, loss = 2.62858017\n",
      "Iteration 57, loss = 2.60652378\n",
      "Iteration 58, loss = 2.59011576\n",
      "Iteration 59, loss = 2.57393269\n",
      "Iteration 60, loss = 2.55479427\n",
      "Iteration 61, loss = 2.53397051\n",
      "Iteration 62, loss = 2.51744077\n",
      "Iteration 63, loss = 2.49976720\n",
      "Iteration 64, loss = 2.48322905\n",
      "Iteration 65, loss = 2.46967293\n",
      "Iteration 66, loss = 2.44794767\n",
      "Iteration 67, loss = 2.43713610\n",
      "Iteration 68, loss = 2.41925280\n",
      "Iteration 69, loss = 2.40511907\n",
      "Iteration 70, loss = 2.39388572\n",
      "Iteration 71, loss = 2.38038929\n",
      "Iteration 72, loss = 2.37176868\n",
      "Iteration 73, loss = 2.36117123\n",
      "Iteration 74, loss = 2.35289864\n",
      "Iteration 75, loss = 2.34214439\n",
      "Iteration 76, loss = 2.33484060\n",
      "Iteration 77, loss = 2.32631753\n",
      "Iteration 78, loss = 2.31898478\n",
      "Iteration 79, loss = 2.31099024\n",
      "Iteration 80, loss = 2.30155493\n",
      "Iteration 81, loss = 2.29059560\n",
      "Iteration 82, loss = 2.28696547\n",
      "Iteration 83, loss = 2.27830859\n",
      "Iteration 84, loss = 2.26702503\n",
      "Iteration 85, loss = 2.26035013\n",
      "Iteration 86, loss = 2.25161660\n",
      "Iteration 87, loss = 2.24597198\n",
      "Iteration 88, loss = 2.23572056\n",
      "Iteration 89, loss = 2.22612147\n",
      "Iteration 90, loss = 2.22118863\n",
      "Iteration 91, loss = 2.21254158\n",
      "Iteration 92, loss = 2.20378981\n",
      "Iteration 93, loss = 2.19751349\n",
      "Iteration 94, loss = 2.18914913\n",
      "Iteration 95, loss = 2.18460108\n",
      "Iteration 96, loss = 2.17597034\n",
      "Iteration 97, loss = 2.17121948\n",
      "Iteration 98, loss = 2.16451684\n",
      "Iteration 99, loss = 2.15710214\n",
      "Iteration 100, loss = 2.14563856\n",
      "Iteration 101, loss = 2.13947693\n",
      "Iteration 102, loss = 2.13365001\n",
      "Iteration 103, loss = 2.11978242\n",
      "Iteration 104, loss = 2.11104156\n",
      "Iteration 105, loss = 2.10343578\n",
      "Iteration 106, loss = 2.09453193\n",
      "Iteration 107, loss = 2.08606455\n",
      "Iteration 108, loss = 2.07761811\n",
      "Iteration 109, loss = 2.07120917\n",
      "Iteration 110, loss = 2.06850992\n",
      "Iteration 111, loss = 2.05585595\n",
      "Iteration 112, loss = 2.04848717\n",
      "Iteration 113, loss = 2.04039721\n",
      "Iteration 114, loss = 2.03133679\n",
      "Iteration 115, loss = 2.02310547\n",
      "Iteration 116, loss = 2.01548615\n",
      "Iteration 117, loss = 2.00787075\n",
      "Iteration 118, loss = 1.99685714\n",
      "Iteration 119, loss = 1.99011618\n",
      "Iteration 120, loss = 1.98094104\n",
      "Iteration 121, loss = 1.97634977\n",
      "Iteration 122, loss = 1.96504091\n",
      "Iteration 123, loss = 1.95964616\n",
      "Iteration 124, loss = 1.95263667\n",
      "Iteration 125, loss = 1.94558517\n",
      "Iteration 126, loss = 1.93975907\n",
      "Iteration 127, loss = 1.93058264\n",
      "Iteration 128, loss = 1.92444373\n",
      "Iteration 129, loss = 1.91711030\n",
      "Iteration 130, loss = 1.91091296\n",
      "Iteration 131, loss = 1.90418546\n",
      "Iteration 132, loss = 1.89747577\n",
      "Iteration 133, loss = 1.89115324\n",
      "Iteration 134, loss = 1.88493706\n",
      "Iteration 135, loss = 1.87899601\n",
      "Iteration 136, loss = 1.87505053\n",
      "Iteration 137, loss = 1.86821726\n",
      "Iteration 138, loss = 1.86047686\n",
      "Iteration 139, loss = 1.85184231\n",
      "Iteration 140, loss = 1.84596891\n",
      "Iteration 141, loss = 1.84131244\n",
      "Iteration 142, loss = 1.83288814\n",
      "Iteration 143, loss = 1.82525188\n",
      "Iteration 144, loss = 1.82045604\n",
      "Iteration 145, loss = 1.81511250\n",
      "Iteration 146, loss = 1.80719259\n",
      "Iteration 147, loss = 1.79963119\n",
      "Iteration 148, loss = 1.79278190\n",
      "Iteration 149, loss = 1.78484266\n",
      "Iteration 150, loss = 1.77970001\n",
      "Iteration 151, loss = 1.76900246\n",
      "Iteration 152, loss = 1.75953198\n",
      "Iteration 153, loss = 1.74952146\n",
      "Iteration 154, loss = 1.74172201\n",
      "Iteration 155, loss = 1.73283342\n",
      "Iteration 156, loss = 1.72420187\n",
      "Iteration 157, loss = 1.71518949\n",
      "Iteration 158, loss = 1.70625248\n",
      "Iteration 159, loss = 1.69855212\n",
      "Iteration 160, loss = 1.68942588\n",
      "Iteration 161, loss = 1.68222771\n",
      "Iteration 162, loss = 1.66914592\n",
      "Iteration 163, loss = 1.65782288\n",
      "Iteration 164, loss = 1.64949030\n",
      "Iteration 165, loss = 1.64156002\n",
      "Iteration 166, loss = 1.63493600\n",
      "Iteration 167, loss = 1.62881470\n",
      "Iteration 168, loss = 1.61637022\n",
      "Iteration 169, loss = 1.60668763\n",
      "Iteration 170, loss = 1.59416491\n",
      "Iteration 171, loss = 1.58584850\n",
      "Iteration 172, loss = 1.57555957\n",
      "Iteration 173, loss = 1.56513642\n",
      "Iteration 174, loss = 1.56010630\n",
      "Iteration 175, loss = 1.54885601\n",
      "Iteration 176, loss = 1.54076761\n",
      "Iteration 177, loss = 1.53486620\n",
      "Iteration 178, loss = 1.52644535\n",
      "Iteration 179, loss = 1.52091223\n",
      "Iteration 180, loss = 1.51616330\n",
      "Iteration 181, loss = 1.50833913\n",
      "Iteration 182, loss = 1.50513235\n",
      "Iteration 183, loss = 1.49732337\n",
      "Iteration 184, loss = 1.49148241\n",
      "Iteration 185, loss = 1.48577730\n",
      "Iteration 186, loss = 1.48325097\n",
      "Iteration 187, loss = 1.47650254\n",
      "Iteration 188, loss = 1.47201109\n",
      "Iteration 189, loss = 1.46609739\n",
      "Iteration 190, loss = 1.46247309\n",
      "Iteration 191, loss = 1.45815933\n",
      "Iteration 192, loss = 1.45507637\n",
      "Iteration 193, loss = 1.45162764\n",
      "Iteration 194, loss = 1.44900563\n",
      "Iteration 195, loss = 1.44390568\n",
      "Iteration 196, loss = 1.44036284\n",
      "Iteration 197, loss = 1.43656238\n",
      "Iteration 198, loss = 1.43585429\n",
      "Iteration 199, loss = 1.43039189\n",
      "Iteration 200, loss = 1.42753087\n",
      "Iteration 1, loss = 11.10990277\n",
      "Iteration 2, loss = 1.85678333\n",
      "Iteration 3, loss = 1.43942362\n",
      "Iteration 4, loss = 1.35868446\n",
      "Iteration 5, loss = 1.29301436\n",
      "Iteration 6, loss = 1.27463970\n",
      "Iteration 7, loss = 1.28146058\n",
      "Iteration 8, loss = 1.26960481\n",
      "Iteration 9, loss = 1.27391303\n",
      "Iteration 10, loss = 1.27142723\n",
      "Iteration 11, loss = 1.26330464\n",
      "Iteration 12, loss = 1.27579936\n",
      "Iteration 13, loss = 1.26758916\n",
      "Iteration 14, loss = 1.27080689\n",
      "Iteration 15, loss = 1.27030441\n",
      "Iteration 16, loss = 1.27070670\n",
      "Iteration 17, loss = 1.26355899\n",
      "Iteration 18, loss = 1.28775533\n",
      "Iteration 19, loss = 1.27109039\n",
      "Iteration 20, loss = 1.26629308\n",
      "Iteration 21, loss = 1.27300780\n",
      "Iteration 22, loss = 1.26817462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.38359558\n",
      "Iteration 2, loss = 16.38359542\n",
      "Iteration 3, loss = 16.38359525\n",
      "Iteration 4, loss = 16.38359509\n",
      "Iteration 5, loss = 16.38359492\n",
      "Iteration 6, loss = 16.38359476\n",
      "Iteration 7, loss = 16.38359460\n",
      "Iteration 8, loss = 16.38359444\n",
      "Iteration 9, loss = 16.38359428\n",
      "Iteration 10, loss = 16.38359412\n",
      "Iteration 11, loss = 16.38359396\n",
      "Iteration 12, loss = 16.38359380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.66078976\n",
      "Iteration 2, loss = 1.74435607\n",
      "Iteration 3, loss = 1.31132953\n",
      "Iteration 4, loss = 1.31727853\n",
      "Iteration 5, loss = 1.27571615\n",
      "Iteration 6, loss = 1.26738781\n",
      "Iteration 7, loss = 1.26631385\n",
      "Iteration 8, loss = 1.26225485\n",
      "Iteration 9, loss = 1.26523258\n",
      "Iteration 10, loss = 1.26834445\n",
      "Iteration 11, loss = 1.26855799\n",
      "Iteration 12, loss = 1.26720301\n",
      "Iteration 13, loss = 1.26352941\n",
      "Iteration 14, loss = 1.26595151\n",
      "Iteration 15, loss = 1.26362818\n",
      "Iteration 16, loss = 1.26418483\n",
      "Iteration 17, loss = 1.26528192\n",
      "Iteration 18, loss = 1.26391899\n",
      "Iteration 19, loss = 1.26203845\n",
      "Iteration 20, loss = 1.26381966\n",
      "Iteration 21, loss = 1.26306046\n",
      "Iteration 22, loss = 1.26361368\n",
      "Iteration 23, loss = 1.26581205\n",
      "Iteration 24, loss = 1.27235877\n",
      "Iteration 25, loss = 1.26368773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 26, loss = 1.26510504\n",
      "Iteration 27, loss = 1.26754363\n",
      "Iteration 28, loss = 1.26872063\n",
      "Iteration 29, loss = 1.26442369\n",
      "Iteration 30, loss = 1.26226405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.38358656\n",
      "Iteration 2, loss = 16.38358639\n",
      "Iteration 3, loss = 16.38358622\n",
      "Iteration 4, loss = 16.38358605\n",
      "Iteration 5, loss = 16.38358588\n",
      "Iteration 6, loss = 16.38358572\n",
      "Iteration 7, loss = 16.38358555\n",
      "Iteration 8, loss = 16.38358539\n",
      "Iteration 9, loss = 16.38358523\n",
      "Iteration 10, loss = 16.38358507\n",
      "Iteration 11, loss = 16.38358491\n",
      "Iteration 12, loss = 16.38358475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.60393271\n",
      "Iteration 2, loss = 1.61152447\n",
      "Iteration 3, loss = 1.31783202\n",
      "Iteration 4, loss = 1.29848334\n",
      "Iteration 5, loss = 1.28531034\n",
      "Iteration 6, loss = 1.28426493\n",
      "Iteration 7, loss = 1.27620011\n",
      "Iteration 8, loss = 1.26955256\n",
      "Iteration 9, loss = 1.27010145\n",
      "Iteration 10, loss = 1.26873706\n",
      "Iteration 11, loss = 1.27014534\n",
      "Iteration 12, loss = 1.27010636\n",
      "Iteration 13, loss = 1.27043802\n",
      "Iteration 14, loss = 1.27237059\n",
      "Iteration 15, loss = 1.26950404\n",
      "Iteration 16, loss = 1.26991925\n",
      "Iteration 17, loss = 1.27004176\n",
      "Iteration 18, loss = 1.27621649\n",
      "Iteration 19, loss = 1.27437266\n",
      "Iteration 20, loss = 1.26888009\n",
      "Iteration 21, loss = 1.27120368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.94699050\n",
      "Iteration 2, loss = 19.42882245\n",
      "Iteration 3, loss = 18.90773638\n",
      "Iteration 4, loss = 18.40436535\n",
      "Iteration 5, loss = 17.90853572\n",
      "Iteration 6, loss = 17.42104617\n",
      "Iteration 7, loss = 16.87843598\n",
      "Iteration 8, loss = 16.38996833\n",
      "Iteration 9, loss = 15.85202662\n",
      "Iteration 10, loss = 15.34044760\n",
      "Iteration 11, loss = 14.83531761\n",
      "Iteration 12, loss = 14.32129213\n",
      "Iteration 13, loss = 13.79520490\n",
      "Iteration 14, loss = 13.28127299\n",
      "Iteration 15, loss = 12.72854358\n",
      "Iteration 16, loss = 12.21949991\n",
      "Iteration 17, loss = 11.72262381\n",
      "Iteration 18, loss = 11.21412790\n",
      "Iteration 19, loss = 10.78138298\n",
      "Iteration 20, loss = 10.32634235\n",
      "Iteration 21, loss = 9.92862052\n",
      "Iteration 22, loss = 9.51191424\n",
      "Iteration 23, loss = 9.14216051\n",
      "Iteration 24, loss = 8.82818353\n",
      "Iteration 25, loss = 8.50446730\n",
      "Iteration 26, loss = 8.21872610\n",
      "Iteration 27, loss = 7.93538997\n",
      "Iteration 28, loss = 7.64943953\n",
      "Iteration 29, loss = 7.40499075\n",
      "Iteration 30, loss = 7.12944336\n",
      "Iteration 31, loss = 6.88323816\n",
      "Iteration 32, loss = 6.63236341\n",
      "Iteration 33, loss = 6.40084915\n",
      "Iteration 34, loss = 6.17548175\n",
      "Iteration 35, loss = 5.95917723\n",
      "Iteration 36, loss = 5.74348231\n",
      "Iteration 37, loss = 5.54286962\n",
      "Iteration 38, loss = 5.32597261\n",
      "Iteration 39, loss = 5.12239930\n",
      "Iteration 40, loss = 4.94213567\n",
      "Iteration 41, loss = 4.76228097\n",
      "Iteration 42, loss = 4.58457825\n",
      "Iteration 43, loss = 4.40712817\n",
      "Iteration 44, loss = 4.25901126\n",
      "Iteration 45, loss = 4.09575954\n",
      "Iteration 46, loss = 3.95269875\n",
      "Iteration 47, loss = 3.83755266\n",
      "Iteration 48, loss = 3.71511880\n",
      "Iteration 49, loss = 3.61290222\n",
      "Iteration 50, loss = 3.53228589\n",
      "Iteration 51, loss = 3.46461744\n",
      "Iteration 52, loss = 3.40236605\n",
      "Iteration 53, loss = 3.34290724\n",
      "Iteration 54, loss = 3.28360606\n",
      "Iteration 55, loss = 3.23263594\n",
      "Iteration 56, loss = 3.18576376\n",
      "Iteration 57, loss = 3.14543439\n",
      "Iteration 58, loss = 3.10402911\n",
      "Iteration 59, loss = 3.07442432\n",
      "Iteration 60, loss = 3.04247281\n",
      "Iteration 61, loss = 3.01306562\n",
      "Iteration 62, loss = 2.98791630\n",
      "Iteration 63, loss = 2.96100718\n",
      "Iteration 64, loss = 2.94088938\n",
      "Iteration 65, loss = 2.91314255\n",
      "Iteration 66, loss = 2.89357931\n",
      "Iteration 67, loss = 2.86957056\n",
      "Iteration 68, loss = 2.84703007\n",
      "Iteration 69, loss = 2.83244680\n",
      "Iteration 70, loss = 2.80785079\n",
      "Iteration 71, loss = 2.79174171\n",
      "Iteration 72, loss = 2.77308213\n",
      "Iteration 73, loss = 2.76044017\n",
      "Iteration 74, loss = 2.74317326\n",
      "Iteration 75, loss = 2.72933051\n",
      "Iteration 76, loss = 2.71663192\n",
      "Iteration 77, loss = 2.70892065\n",
      "Iteration 78, loss = 2.69627077\n",
      "Iteration 79, loss = 2.68389564\n",
      "Iteration 80, loss = 2.67474849\n",
      "Iteration 81, loss = 2.66053466\n",
      "Iteration 82, loss = 2.64642576\n",
      "Iteration 83, loss = 2.63523311\n",
      "Iteration 84, loss = 2.62321381\n",
      "Iteration 85, loss = 2.61026040\n",
      "Iteration 86, loss = 2.59944551\n",
      "Iteration 87, loss = 2.58785808\n",
      "Iteration 88, loss = 2.57676795\n",
      "Iteration 89, loss = 2.56276357\n",
      "Iteration 90, loss = 2.55215884\n",
      "Iteration 91, loss = 2.54003661\n",
      "Iteration 92, loss = 2.52605187\n",
      "Iteration 93, loss = 2.51485781\n",
      "Iteration 94, loss = 2.50281444\n",
      "Iteration 95, loss = 2.49281113\n",
      "Iteration 96, loss = 2.47960477\n",
      "Iteration 97, loss = 2.46631866\n",
      "Iteration 98, loss = 2.45683675\n",
      "Iteration 99, loss = 2.44220762\n",
      "Iteration 100, loss = 2.43005610\n",
      "Iteration 101, loss = 2.41995102\n",
      "Iteration 102, loss = 2.40646496\n",
      "Iteration 103, loss = 2.39811910\n",
      "Iteration 104, loss = 2.38253783\n",
      "Iteration 105, loss = 2.37386378\n",
      "Iteration 106, loss = 2.36262205\n",
      "Iteration 107, loss = 2.35160527\n",
      "Iteration 108, loss = 2.34045080\n",
      "Iteration 109, loss = 2.33030196\n",
      "Iteration 110, loss = 2.32073420\n",
      "Iteration 111, loss = 2.31366220\n",
      "Iteration 112, loss = 2.30174520\n",
      "Iteration 113, loss = 2.29441499\n",
      "Iteration 114, loss = 2.28768750\n",
      "Iteration 115, loss = 2.28032433\n",
      "Iteration 116, loss = 2.27012678\n",
      "Iteration 117, loss = 2.26310690\n",
      "Iteration 118, loss = 2.25511778\n",
      "Iteration 119, loss = 2.24607295\n",
      "Iteration 120, loss = 2.24101468\n",
      "Iteration 121, loss = 2.23010673\n",
      "Iteration 122, loss = 2.22005028\n",
      "Iteration 123, loss = 2.21360979\n",
      "Iteration 124, loss = 2.20338663\n",
      "Iteration 125, loss = 2.19745768\n",
      "Iteration 126, loss = 2.18739964\n",
      "Iteration 127, loss = 2.18220095\n",
      "Iteration 128, loss = 2.17346424\n",
      "Iteration 129, loss = 2.16483308\n",
      "Iteration 130, loss = 2.15894254\n",
      "Iteration 131, loss = 2.15205827\n",
      "Iteration 132, loss = 2.14359897\n",
      "Iteration 133, loss = 2.13809758\n",
      "Iteration 134, loss = 2.13273571\n",
      "Iteration 135, loss = 2.12429374\n",
      "Iteration 136, loss = 2.11858809\n",
      "Iteration 137, loss = 2.11168298\n",
      "Iteration 138, loss = 2.10561782\n",
      "Iteration 139, loss = 2.09980502\n",
      "Iteration 140, loss = 2.09458762\n",
      "Iteration 141, loss = 2.09009211\n",
      "Iteration 142, loss = 2.08290697\n",
      "Iteration 143, loss = 2.07920213\n",
      "Iteration 144, loss = 2.07452537\n",
      "Iteration 145, loss = 2.07081846\n",
      "Iteration 146, loss = 2.06714554\n",
      "Iteration 147, loss = 2.06332864\n",
      "Iteration 148, loss = 2.06076563\n",
      "Iteration 149, loss = 2.05848202\n",
      "Iteration 150, loss = 2.05621572\n",
      "Iteration 151, loss = 2.05315190\n",
      "Iteration 152, loss = 2.05130455\n",
      "Iteration 153, loss = 2.05041236\n",
      "Iteration 154, loss = 2.04891064\n",
      "Iteration 155, loss = 2.04774950\n",
      "Iteration 156, loss = 2.04668284\n",
      "Iteration 157, loss = 2.04570875\n",
      "Iteration 158, loss = 2.04500093\n",
      "Iteration 159, loss = 2.04367229\n",
      "Iteration 160, loss = 2.04294547\n",
      "Iteration 161, loss = 2.04160325\n",
      "Iteration 162, loss = 2.04067658\n",
      "Iteration 163, loss = 2.04005183\n",
      "Iteration 164, loss = 2.03888085\n",
      "Iteration 165, loss = 2.03767706\n",
      "Iteration 166, loss = 2.03677676\n",
      "Iteration 167, loss = 2.03554624\n",
      "Iteration 168, loss = 2.03453465\n",
      "Iteration 169, loss = 2.03389474\n",
      "Iteration 170, loss = 2.03274450\n",
      "Iteration 171, loss = 2.03175024\n",
      "Iteration 172, loss = 2.03091003\n",
      "Iteration 173, loss = 2.02978557\n",
      "Iteration 174, loss = 2.02894996\n",
      "Iteration 175, loss = 2.02804774\n",
      "Iteration 176, loss = 2.02719524\n",
      "Iteration 177, loss = 2.02639720\n",
      "Iteration 178, loss = 2.02545987\n",
      "Iteration 179, loss = 2.02469570\n",
      "Iteration 180, loss = 2.02391602\n",
      "Iteration 181, loss = 2.02288230\n",
      "Iteration 182, loss = 2.02204155\n",
      "Iteration 183, loss = 2.02123633\n",
      "Iteration 184, loss = 2.02029918\n",
      "Iteration 185, loss = 2.01935177\n",
      "Iteration 186, loss = 2.01847862\n",
      "Iteration 187, loss = 2.01774036\n",
      "Iteration 188, loss = 2.01651833\n",
      "Iteration 189, loss = 2.01595184\n",
      "Iteration 190, loss = 2.01490861\n",
      "Iteration 191, loss = 2.01435674\n",
      "Iteration 192, loss = 2.01338213\n",
      "Iteration 193, loss = 2.01270459\n",
      "Iteration 194, loss = 2.01194076\n",
      "Iteration 195, loss = 2.01129249\n",
      "Iteration 196, loss = 2.01053034\n",
      "Iteration 197, loss = 2.00976338\n",
      "Iteration 198, loss = 2.00902559\n",
      "Iteration 199, loss = 2.00825927\n",
      "Iteration 200, loss = 2.00752324\n",
      "Iteration 1, loss = 8.39807560\n",
      "Iteration 2, loss = 1.90494241\n",
      "Iteration 3, loss = 1.31258435\n",
      "Iteration 4, loss = 1.25247330\n",
      "Iteration 5, loss = 1.23633275\n",
      "Iteration 6, loss = 1.23527097\n",
      "Iteration 7, loss = 1.22776912\n",
      "Iteration 8, loss = 1.22234104\n",
      "Iteration 9, loss = 1.22464273\n",
      "Iteration 10, loss = 1.21882737\n",
      "Iteration 11, loss = 1.21843647\n",
      "Iteration 12, loss = 1.21748009\n",
      "Iteration 13, loss = 1.21393939\n",
      "Iteration 14, loss = 1.21733039\n",
      "Iteration 15, loss = 1.21538930\n",
      "Iteration 16, loss = 1.21343420\n",
      "Iteration 17, loss = 1.21369212\n",
      "Iteration 18, loss = 1.21432419\n",
      "Iteration 19, loss = 1.21462580\n",
      "Iteration 20, loss = 1.21262509\n",
      "Iteration 21, loss = 1.21515478\n",
      "Iteration 22, loss = 1.21679945\n",
      "Iteration 23, loss = 1.21515184\n",
      "Iteration 24, loss = 1.21255657\n",
      "Iteration 25, loss = 1.21350682\n",
      "Iteration 26, loss = 1.21796211\n",
      "Iteration 27, loss = 1.21497387\n",
      "Iteration 28, loss = 1.21421857\n",
      "Iteration 29, loss = 1.21296084\n",
      "Iteration 30, loss = 1.21504189\n",
      "Iteration 31, loss = 1.21364020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 13.60025488\n",
      "Iteration 2, loss = 13.35298462\n",
      "Iteration 3, loss = 13.10046252\n",
      "Iteration 4, loss = 12.86972382\n",
      "Iteration 5, loss = 12.56029102\n",
      "Iteration 6, loss = 12.31690550\n",
      "Iteration 7, loss = 12.02175907\n",
      "Iteration 8, loss = 11.76219007\n",
      "Iteration 9, loss = 11.49771729\n",
      "Iteration 10, loss = 11.22540412\n",
      "Iteration 11, loss = 10.95582575\n",
      "Iteration 12, loss = 10.69073035\n",
      "Iteration 13, loss = 10.46866365\n",
      "Iteration 14, loss = 10.22789236\n",
      "Iteration 15, loss = 10.00745835\n",
      "Iteration 16, loss = 9.78457371\n",
      "Iteration 17, loss = 9.59717113\n",
      "Iteration 18, loss = 9.37127405\n",
      "Iteration 19, loss = 9.19720669\n",
      "Iteration 20, loss = 9.00122191\n",
      "Iteration 21, loss = 8.80868414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 8.61897954\n",
      "Iteration 23, loss = 8.45148171\n",
      "Iteration 24, loss = 8.27514131\n",
      "Iteration 25, loss = 8.10935464\n",
      "Iteration 26, loss = 7.92307745\n",
      "Iteration 27, loss = 7.77697960\n",
      "Iteration 28, loss = 7.61169921\n",
      "Iteration 29, loss = 7.46884873\n",
      "Iteration 30, loss = 7.32201383\n",
      "Iteration 31, loss = 7.17077968\n",
      "Iteration 32, loss = 7.02359658\n",
      "Iteration 33, loss = 6.88296617\n",
      "Iteration 34, loss = 6.73392688\n",
      "Iteration 35, loss = 6.60685084\n",
      "Iteration 36, loss = 6.45536642\n",
      "Iteration 37, loss = 6.32534194\n",
      "Iteration 38, loss = 6.19836262\n",
      "Iteration 39, loss = 6.05683418\n",
      "Iteration 40, loss = 5.92280805\n",
      "Iteration 41, loss = 5.79254047\n",
      "Iteration 42, loss = 5.67046413\n",
      "Iteration 43, loss = 5.53398383\n",
      "Iteration 44, loss = 5.41109336\n",
      "Iteration 45, loss = 5.29508677\n",
      "Iteration 46, loss = 5.16128661\n",
      "Iteration 47, loss = 5.04294510\n",
      "Iteration 48, loss = 4.91465534\n",
      "Iteration 49, loss = 4.80347651\n",
      "Iteration 50, loss = 4.67927624\n",
      "Iteration 51, loss = 4.57292840\n",
      "Iteration 52, loss = 4.44357075\n",
      "Iteration 53, loss = 4.33771507\n",
      "Iteration 54, loss = 4.21982317\n",
      "Iteration 55, loss = 4.11845700\n",
      "Iteration 56, loss = 3.99955995\n",
      "Iteration 57, loss = 3.89712739\n",
      "Iteration 58, loss = 3.77250984\n",
      "Iteration 59, loss = 3.67616049\n",
      "Iteration 60, loss = 3.56030850\n",
      "Iteration 61, loss = 3.47200096\n",
      "Iteration 62, loss = 3.36326375\n",
      "Iteration 63, loss = 3.27677629\n",
      "Iteration 64, loss = 3.18386629\n",
      "Iteration 65, loss = 3.09012622\n",
      "Iteration 66, loss = 3.01100441\n",
      "Iteration 67, loss = 2.92801408\n",
      "Iteration 68, loss = 2.85829017\n",
      "Iteration 69, loss = 2.78948703\n",
      "Iteration 70, loss = 2.72652617\n",
      "Iteration 71, loss = 2.66848002\n",
      "Iteration 72, loss = 2.61095893\n",
      "Iteration 73, loss = 2.55177374\n",
      "Iteration 74, loss = 2.50347958\n",
      "Iteration 75, loss = 2.46074924\n",
      "Iteration 76, loss = 2.41514916\n",
      "Iteration 77, loss = 2.37671288\n",
      "Iteration 78, loss = 2.34494755\n",
      "Iteration 79, loss = 2.31482229\n",
      "Iteration 80, loss = 2.28299093\n",
      "Iteration 81, loss = 2.25416101\n",
      "Iteration 82, loss = 2.22794745\n",
      "Iteration 83, loss = 2.20288580\n",
      "Iteration 84, loss = 2.17609758\n",
      "Iteration 85, loss = 2.15442205\n",
      "Iteration 86, loss = 2.13029918\n",
      "Iteration 87, loss = 2.11252001\n",
      "Iteration 88, loss = 2.08996019\n",
      "Iteration 89, loss = 2.07077132\n",
      "Iteration 90, loss = 2.05162848\n",
      "Iteration 91, loss = 2.03178586\n",
      "Iteration 92, loss = 2.01374956\n",
      "Iteration 93, loss = 1.99359678\n",
      "Iteration 94, loss = 1.97992823\n",
      "Iteration 95, loss = 1.96114726\n",
      "Iteration 96, loss = 1.94598791\n",
      "Iteration 97, loss = 1.93083547\n",
      "Iteration 98, loss = 1.91414969\n",
      "Iteration 99, loss = 1.89899030\n",
      "Iteration 100, loss = 1.88632128\n",
      "Iteration 101, loss = 1.86991010\n",
      "Iteration 102, loss = 1.85982364\n",
      "Iteration 103, loss = 1.84618908\n",
      "Iteration 104, loss = 1.83314231\n",
      "Iteration 105, loss = 1.82222283\n",
      "Iteration 106, loss = 1.81246539\n",
      "Iteration 107, loss = 1.80342846\n",
      "Iteration 108, loss = 1.79328184\n",
      "Iteration 109, loss = 1.78453913\n",
      "Iteration 110, loss = 1.77642306\n",
      "Iteration 111, loss = 1.76603208\n",
      "Iteration 112, loss = 1.75735976\n",
      "Iteration 113, loss = 1.74804808\n",
      "Iteration 114, loss = 1.73908948\n",
      "Iteration 115, loss = 1.73071677\n",
      "Iteration 116, loss = 1.72083751\n",
      "Iteration 117, loss = 1.71516873\n",
      "Iteration 118, loss = 1.70459424\n",
      "Iteration 119, loss = 1.69916754\n",
      "Iteration 120, loss = 1.69008226\n",
      "Iteration 121, loss = 1.68275857\n",
      "Iteration 122, loss = 1.67755977\n",
      "Iteration 123, loss = 1.67076829\n",
      "Iteration 124, loss = 1.66513329\n",
      "Iteration 125, loss = 1.65954385\n",
      "Iteration 126, loss = 1.65301168\n",
      "Iteration 127, loss = 1.64844930\n",
      "Iteration 128, loss = 1.64233471\n",
      "Iteration 129, loss = 1.63733193\n",
      "Iteration 130, loss = 1.63167725\n",
      "Iteration 131, loss = 1.62646465\n",
      "Iteration 132, loss = 1.62235191\n",
      "Iteration 133, loss = 1.61717538\n",
      "Iteration 134, loss = 1.61217421\n",
      "Iteration 135, loss = 1.60825416\n",
      "Iteration 136, loss = 1.60311466\n",
      "Iteration 137, loss = 1.59801259\n",
      "Iteration 138, loss = 1.59515444\n",
      "Iteration 139, loss = 1.59105346\n",
      "Iteration 140, loss = 1.58698472\n",
      "Iteration 141, loss = 1.58153449\n",
      "Iteration 142, loss = 1.57597669\n",
      "Iteration 143, loss = 1.57129575\n",
      "Iteration 144, loss = 1.56727843\n",
      "Iteration 145, loss = 1.56306508\n",
      "Iteration 146, loss = 1.55886823\n",
      "Iteration 147, loss = 1.55690819\n",
      "Iteration 148, loss = 1.55059078\n",
      "Iteration 149, loss = 1.54749806\n",
      "Iteration 150, loss = 1.54459743\n",
      "Iteration 151, loss = 1.54016534\n",
      "Iteration 152, loss = 1.53765969\n",
      "Iteration 153, loss = 1.53384301\n",
      "Iteration 154, loss = 1.53072596\n",
      "Iteration 155, loss = 1.52756438\n",
      "Iteration 156, loss = 1.52534495\n",
      "Iteration 157, loss = 1.52188353\n",
      "Iteration 158, loss = 1.51926546\n",
      "Iteration 159, loss = 1.51578323\n",
      "Iteration 160, loss = 1.51367835\n",
      "Iteration 161, loss = 1.50986934\n",
      "Iteration 162, loss = 1.50607560\n",
      "Iteration 163, loss = 1.50270510\n",
      "Iteration 164, loss = 1.50036832\n",
      "Iteration 165, loss = 1.49554742\n",
      "Iteration 166, loss = 1.49165599\n",
      "Iteration 167, loss = 1.48881244\n",
      "Iteration 168, loss = 1.48546533\n",
      "Iteration 169, loss = 1.48090348\n",
      "Iteration 170, loss = 1.47805909\n",
      "Iteration 171, loss = 1.47402602\n",
      "Iteration 172, loss = 1.47070191\n",
      "Iteration 173, loss = 1.46769524\n",
      "Iteration 174, loss = 1.46523827\n",
      "Iteration 175, loss = 1.46082471\n",
      "Iteration 176, loss = 1.45736062\n",
      "Iteration 177, loss = 1.45388590\n",
      "Iteration 178, loss = 1.45157883\n",
      "Iteration 179, loss = 1.44742178\n",
      "Iteration 180, loss = 1.44292706\n",
      "Iteration 181, loss = 1.43882823\n",
      "Iteration 182, loss = 1.43615030\n",
      "Iteration 183, loss = 1.43049170\n",
      "Iteration 184, loss = 1.42578652\n",
      "Iteration 185, loss = 1.42185313\n",
      "Iteration 186, loss = 1.41775121\n",
      "Iteration 187, loss = 1.41416314\n",
      "Iteration 188, loss = 1.41091585\n",
      "Iteration 189, loss = 1.40810908\n",
      "Iteration 190, loss = 1.40407745\n",
      "Iteration 191, loss = 1.40188949\n",
      "Iteration 192, loss = 1.39891480\n",
      "Iteration 193, loss = 1.39604850\n",
      "Iteration 194, loss = 1.39337018\n",
      "Iteration 195, loss = 1.39066960\n",
      "Iteration 196, loss = 1.38876782\n",
      "Iteration 197, loss = 1.38617672\n",
      "Iteration 198, loss = 1.38384272\n",
      "Iteration 199, loss = 1.38079589\n",
      "Iteration 200, loss = 1.37844338\n",
      "Iteration 1, loss = 15.74203059\n",
      "Iteration 2, loss = 1.44052293\n",
      "Iteration 3, loss = 1.34537658\n",
      "Iteration 4, loss = 1.30416510\n",
      "Iteration 5, loss = 1.26621984\n",
      "Iteration 6, loss = 1.23763505\n",
      "Iteration 7, loss = 1.22645058\n",
      "Iteration 8, loss = 1.22386300\n",
      "Iteration 9, loss = 1.22431128\n",
      "Iteration 10, loss = 1.21901055\n",
      "Iteration 11, loss = 1.21914360\n",
      "Iteration 12, loss = 1.21830544\n",
      "Iteration 13, loss = 1.21517313\n",
      "Iteration 14, loss = 1.21914180\n",
      "Iteration 15, loss = 1.21643588\n",
      "Iteration 16, loss = 1.21907171\n",
      "Iteration 17, loss = 1.21394432\n",
      "Iteration 18, loss = 1.21190454\n",
      "Iteration 19, loss = 1.21356690\n",
      "Iteration 20, loss = 1.21789502\n",
      "Iteration 21, loss = 1.21813221\n",
      "Iteration 22, loss = 1.21383742\n",
      "Iteration 23, loss = 1.21177876\n",
      "Iteration 24, loss = 1.21467467\n",
      "Iteration 25, loss = 1.21399100\n",
      "Iteration 26, loss = 1.21589731\n",
      "Iteration 27, loss = 1.21626073\n",
      "Iteration 28, loss = 1.21429280\n",
      "Iteration 29, loss = 1.21662905\n",
      "Iteration 30, loss = 1.21574622\n",
      "Iteration 31, loss = 1.21382182\n",
      "Iteration 32, loss = 1.21534395\n",
      "Iteration 33, loss = 1.21391045\n",
      "Iteration 34, loss = 1.21396226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 13.86432556\n",
      "Iteration 2, loss = 13.76934563\n",
      "Iteration 3, loss = 13.69238055\n",
      "Iteration 4, loss = 13.57249780\n",
      "Iteration 5, loss = 13.48876638\n",
      "Iteration 6, loss = 13.36726565\n",
      "Iteration 7, loss = 13.22659146\n",
      "Iteration 8, loss = 13.11401872\n",
      "Iteration 9, loss = 13.00771725\n",
      "Iteration 10, loss = 12.88240316\n",
      "Iteration 11, loss = 12.78343648\n",
      "Iteration 12, loss = 12.66088395\n",
      "Iteration 13, loss = 12.53662591\n",
      "Iteration 14, loss = 12.41030713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 12.32108629\n",
      "Iteration 16, loss = 12.21866972\n",
      "Iteration 17, loss = 12.09899867\n",
      "Iteration 18, loss = 11.99592561\n",
      "Iteration 19, loss = 11.91196855\n",
      "Iteration 20, loss = 11.85155678\n",
      "Iteration 21, loss = 11.77613491\n",
      "Iteration 22, loss = 11.70664413\n",
      "Iteration 23, loss = 11.68123299\n",
      "Iteration 24, loss = 11.62977719\n",
      "Iteration 25, loss = 11.59146526\n",
      "Iteration 26, loss = 11.58635490\n",
      "Iteration 27, loss = 11.56519255\n",
      "Iteration 28, loss = 11.54513803\n",
      "Iteration 29, loss = 11.54736456\n",
      "Iteration 30, loss = 11.54778732\n",
      "Iteration 31, loss = 11.54022193\n",
      "Iteration 32, loss = 11.53576071\n",
      "Iteration 33, loss = 11.53644900\n",
      "Iteration 34, loss = 11.53818968\n",
      "Iteration 35, loss = 11.52871366\n",
      "Iteration 36, loss = 11.52504241\n",
      "Iteration 37, loss = 11.52074055\n",
      "Iteration 38, loss = 11.51840130\n",
      "Iteration 39, loss = 11.51463581\n",
      "Iteration 40, loss = 11.50700464\n",
      "Iteration 41, loss = 11.50732413\n",
      "Iteration 42, loss = 11.50227920\n",
      "Iteration 43, loss = 11.50068370\n",
      "Iteration 44, loss = 11.49898527\n",
      "Iteration 45, loss = 11.49630416\n",
      "Iteration 46, loss = 11.49068270\n",
      "Iteration 47, loss = 11.48838333\n",
      "Iteration 48, loss = 11.48707585\n",
      "Iteration 49, loss = 11.48467389\n",
      "Iteration 50, loss = 11.48135656\n",
      "Iteration 51, loss = 11.47881912\n",
      "Iteration 52, loss = 11.47541960\n",
      "Iteration 53, loss = 11.43480655\n",
      "Iteration 54, loss = 11.38279611\n",
      "Iteration 55, loss = 11.36531592\n",
      "Iteration 56, loss = 11.30795137\n",
      "Iteration 57, loss = 11.22824539\n",
      "Iteration 58, loss = 11.13385565\n",
      "Iteration 59, loss = 11.03966234\n",
      "Iteration 60, loss = 10.91768704\n",
      "Iteration 61, loss = 10.82247708\n",
      "Iteration 62, loss = 10.72909853\n",
      "Iteration 63, loss = 10.62471444\n",
      "Iteration 64, loss = 10.52438591\n",
      "Iteration 65, loss = 10.40940885\n",
      "Iteration 66, loss = 10.31407893\n",
      "Iteration 67, loss = 10.19632886\n",
      "Iteration 68, loss = 10.12097621\n",
      "Iteration 69, loss = 10.01143430\n",
      "Iteration 70, loss = 9.89080298\n",
      "Iteration 71, loss = 9.79823575\n",
      "Iteration 72, loss = 9.69052727\n",
      "Iteration 73, loss = 9.60518837\n",
      "Iteration 74, loss = 9.50201500\n",
      "Iteration 75, loss = 9.37681354\n",
      "Iteration 76, loss = 9.28908360\n",
      "Iteration 77, loss = 9.20770038\n",
      "Iteration 78, loss = 9.09312325\n",
      "Iteration 79, loss = 9.00587139\n",
      "Iteration 80, loss = 8.92541813\n",
      "Iteration 81, loss = 8.85657643\n",
      "Iteration 82, loss = 8.76444458\n",
      "Iteration 83, loss = 8.67230085\n",
      "Iteration 84, loss = 8.62218644\n",
      "Iteration 85, loss = 8.52295664\n",
      "Iteration 86, loss = 8.44648478\n",
      "Iteration 87, loss = 8.39155920\n",
      "Iteration 88, loss = 8.32137766\n",
      "Iteration 89, loss = 8.26686768\n",
      "Iteration 90, loss = 8.23469970\n",
      "Iteration 91, loss = 8.20348329\n",
      "Iteration 92, loss = 8.19756026\n",
      "Iteration 93, loss = 8.19102512\n",
      "Iteration 94, loss = 8.18924697\n",
      "Iteration 95, loss = 8.18849778\n",
      "Iteration 96, loss = 8.19781454\n",
      "Iteration 97, loss = 8.20120541\n",
      "Iteration 98, loss = 8.19563941\n",
      "Iteration 99, loss = 8.19256072\n",
      "Iteration 100, loss = 8.19877344\n",
      "Iteration 101, loss = 8.19465208\n",
      "Iteration 102, loss = 8.19220728\n",
      "Iteration 103, loss = 8.18710732\n",
      "Iteration 104, loss = 8.17824123\n",
      "Iteration 105, loss = 8.17639083\n",
      "Iteration 106, loss = 8.17779475\n",
      "Iteration 107, loss = 8.17890906\n",
      "Iteration 108, loss = 8.17864174\n",
      "Iteration 109, loss = 8.18623693\n",
      "Iteration 110, loss = 8.18895915\n",
      "Iteration 111, loss = 8.19311081\n",
      "Iteration 112, loss = 8.19261364\n",
      "Iteration 113, loss = 8.18806858\n",
      "Iteration 114, loss = 8.17781470\n",
      "Iteration 115, loss = 8.18023095\n",
      "Iteration 116, loss = 8.17922195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.18371603\n",
      "Iteration 2, loss = 1.60728761\n",
      "Iteration 3, loss = 1.32181001\n",
      "Iteration 4, loss = 1.27636436\n",
      "Iteration 5, loss = 1.28386375\n",
      "Iteration 6, loss = 1.28125447\n",
      "Iteration 7, loss = 1.26436003\n",
      "Iteration 8, loss = 1.25066537\n",
      "Iteration 9, loss = 1.25566301\n",
      "Iteration 10, loss = 1.25128775\n",
      "Iteration 11, loss = 1.24830087\n",
      "Iteration 12, loss = 1.25137232\n",
      "Iteration 13, loss = 1.24898258\n",
      "Iteration 14, loss = 1.24658984\n",
      "Iteration 15, loss = 1.25224692\n",
      "Iteration 16, loss = 1.25005654\n",
      "Iteration 17, loss = 1.24808909\n",
      "Iteration 18, loss = 1.24508842\n",
      "Iteration 19, loss = 1.24942782\n",
      "Iteration 20, loss = 1.24876289\n",
      "Iteration 21, loss = 1.24954119\n",
      "Iteration 22, loss = 1.25087022\n",
      "Iteration 23, loss = 1.24984507\n",
      "Iteration 24, loss = 1.25067644\n",
      "Iteration 25, loss = 1.24918418\n",
      "Iteration 26, loss = 1.24709398\n",
      "Iteration 27, loss = 1.24664580\n",
      "Iteration 28, loss = 1.25182667\n",
      "Iteration 29, loss = 1.24814602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.79076865\n",
      "Iteration 2, loss = 1.79030853\n",
      "Iteration 3, loss = 1.78991604\n",
      "Iteration 4, loss = 1.78944438\n",
      "Iteration 5, loss = 1.78904200\n",
      "Iteration 6, loss = 1.78855906\n",
      "Iteration 7, loss = 1.78814271\n",
      "Iteration 8, loss = 1.78767910\n",
      "Iteration 9, loss = 1.78726155\n",
      "Iteration 10, loss = 1.78679795\n",
      "Iteration 11, loss = 1.78635381\n",
      "Iteration 12, loss = 1.78589592\n",
      "Iteration 13, loss = 1.78546992\n",
      "Iteration 14, loss = 1.78501929\n",
      "Iteration 15, loss = 1.78454306\n",
      "Iteration 16, loss = 1.78413677\n",
      "Iteration 17, loss = 1.78370258\n",
      "Iteration 18, loss = 1.78323429\n",
      "Iteration 19, loss = 1.78280117\n",
      "Iteration 20, loss = 1.78235422\n",
      "Iteration 21, loss = 1.78191948\n",
      "Iteration 22, loss = 1.78145261\n",
      "Iteration 23, loss = 1.78102286\n",
      "Iteration 24, loss = 1.78058688\n",
      "Iteration 25, loss = 1.78018328\n",
      "Iteration 26, loss = 1.77972162\n",
      "Iteration 27, loss = 1.77929590\n",
      "Iteration 28, loss = 1.77887350\n",
      "Iteration 29, loss = 1.77847170\n",
      "Iteration 30, loss = 1.77798176\n",
      "Iteration 31, loss = 1.77755586\n",
      "Iteration 32, loss = 1.77712477\n",
      "Iteration 33, loss = 1.77669158\n",
      "Iteration 34, loss = 1.77623557\n",
      "Iteration 35, loss = 1.77581374\n",
      "Iteration 36, loss = 1.77537073\n",
      "Iteration 37, loss = 1.77493212\n",
      "Iteration 38, loss = 1.77448915\n",
      "Iteration 39, loss = 1.77404652\n",
      "Iteration 40, loss = 1.77362024\n",
      "Iteration 41, loss = 1.77319378\n",
      "Iteration 42, loss = 1.77274114\n",
      "Iteration 43, loss = 1.77230488\n",
      "Iteration 44, loss = 1.77184483\n",
      "Iteration 45, loss = 1.77141200\n",
      "Iteration 46, loss = 1.77098582\n",
      "Iteration 47, loss = 1.77050580\n",
      "Iteration 48, loss = 1.77008997\n",
      "Iteration 49, loss = 1.76962858\n",
      "Iteration 50, loss = 1.76922420\n",
      "Iteration 51, loss = 1.76873821\n",
      "Iteration 52, loss = 1.76833655\n",
      "Iteration 53, loss = 1.76788963\n",
      "Iteration 54, loss = 1.76744342\n",
      "Iteration 55, loss = 1.76700903\n",
      "Iteration 56, loss = 1.76656773\n",
      "Iteration 57, loss = 1.76614272\n",
      "Iteration 58, loss = 1.76571215\n",
      "Iteration 59, loss = 1.76526092\n",
      "Iteration 60, loss = 1.76485924\n",
      "Iteration 61, loss = 1.76441902\n",
      "Iteration 62, loss = 1.76398668\n",
      "Iteration 63, loss = 1.76357411\n",
      "Iteration 64, loss = 1.76311063\n",
      "Iteration 65, loss = 1.76271307\n",
      "Iteration 66, loss = 1.76227409\n",
      "Iteration 67, loss = 1.76182561\n",
      "Iteration 68, loss = 1.76141752\n",
      "Iteration 69, loss = 1.76100035\n",
      "Iteration 70, loss = 1.76054222\n",
      "Iteration 71, loss = 1.76013549\n",
      "Iteration 72, loss = 1.75969441\n",
      "Iteration 73, loss = 1.75926969\n",
      "Iteration 74, loss = 1.75880946\n",
      "Iteration 75, loss = 1.75841452\n",
      "Iteration 76, loss = 1.75797236\n",
      "Iteration 77, loss = 1.75752002\n",
      "Iteration 78, loss = 1.75707971\n",
      "Iteration 79, loss = 1.75663379\n",
      "Iteration 80, loss = 1.75622295\n",
      "Iteration 81, loss = 1.75579760\n",
      "Iteration 82, loss = 1.75536626\n",
      "Iteration 83, loss = 1.75494524\n",
      "Iteration 84, loss = 1.75451076\n",
      "Iteration 85, loss = 1.75409815\n",
      "Iteration 86, loss = 1.75365534\n",
      "Iteration 87, loss = 1.75324639\n",
      "Iteration 88, loss = 1.75282737\n",
      "Iteration 89, loss = 1.75238700\n",
      "Iteration 90, loss = 1.75196019\n",
      "Iteration 91, loss = 1.75151413\n",
      "Iteration 92, loss = 1.75112569\n",
      "Iteration 93, loss = 1.75068741\n",
      "Iteration 94, loss = 1.75027302\n",
      "Iteration 95, loss = 1.74981828\n",
      "Iteration 96, loss = 1.74938927\n",
      "Iteration 97, loss = 1.74893958\n",
      "Iteration 98, loss = 1.74851532\n",
      "Iteration 99, loss = 1.74809916\n",
      "Iteration 100, loss = 1.74764636\n",
      "Iteration 101, loss = 1.74723857\n",
      "Iteration 102, loss = 1.74680557\n",
      "Iteration 103, loss = 1.74637527\n",
      "Iteration 104, loss = 1.74596736\n",
      "Iteration 105, loss = 1.74553842\n",
      "Iteration 106, loss = 1.74511335\n",
      "Iteration 107, loss = 1.74469869\n",
      "Iteration 108, loss = 1.74429744\n",
      "Iteration 109, loss = 1.74385512\n",
      "Iteration 110, loss = 1.74343687\n",
      "Iteration 111, loss = 1.74302203\n",
      "Iteration 112, loss = 1.74261721\n",
      "Iteration 113, loss = 1.74220833\n",
      "Iteration 114, loss = 1.74179859\n",
      "Iteration 115, loss = 1.74136079\n",
      "Iteration 116, loss = 1.74095830\n",
      "Iteration 117, loss = 1.74056771\n",
      "Iteration 118, loss = 1.74014200\n",
      "Iteration 119, loss = 1.73973954\n",
      "Iteration 120, loss = 1.73935144\n",
      "Iteration 121, loss = 1.73894874\n",
      "Iteration 122, loss = 1.73851321\n",
      "Iteration 123, loss = 1.73809070\n",
      "Iteration 124, loss = 1.73770493\n",
      "Iteration 125, loss = 1.73731014\n",
      "Iteration 126, loss = 1.73688093\n",
      "Iteration 127, loss = 1.73648000\n",
      "Iteration 128, loss = 1.73605940\n",
      "Iteration 129, loss = 1.73564398\n",
      "Iteration 130, loss = 1.73526608\n",
      "Iteration 131, loss = 1.73481495\n",
      "Iteration 132, loss = 1.73441586\n",
      "Iteration 133, loss = 1.73398606\n",
      "Iteration 134, loss = 1.73357105\n",
      "Iteration 135, loss = 1.73315139\n",
      "Iteration 136, loss = 1.73273547\n",
      "Iteration 137, loss = 1.73230979\n",
      "Iteration 138, loss = 1.73193467\n",
      "Iteration 139, loss = 1.73149970\n",
      "Iteration 140, loss = 1.73111701\n",
      "Iteration 141, loss = 1.73067341\n",
      "Iteration 142, loss = 1.73030568\n",
      "Iteration 143, loss = 1.72989008\n",
      "Iteration 144, loss = 1.72949295\n",
      "Iteration 145, loss = 1.72906287\n",
      "Iteration 146, loss = 1.72866674\n",
      "Iteration 147, loss = 1.72826086\n",
      "Iteration 148, loss = 1.72783621\n",
      "Iteration 149, loss = 1.72742789\n",
      "Iteration 150, loss = 1.72707401\n",
      "Iteration 151, loss = 1.72664083\n",
      "Iteration 152, loss = 1.72621847\n",
      "Iteration 153, loss = 1.72582494\n",
      "Iteration 154, loss = 1.72541736\n",
      "Iteration 155, loss = 1.72500643\n",
      "Iteration 156, loss = 1.72460747\n",
      "Iteration 157, loss = 1.72419029\n",
      "Iteration 158, loss = 1.72379143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 159, loss = 1.72337535\n",
      "Iteration 160, loss = 1.72298035\n",
      "Iteration 161, loss = 1.72256415\n",
      "Iteration 162, loss = 1.72213865\n",
      "Iteration 163, loss = 1.72175907\n",
      "Iteration 164, loss = 1.72133640\n",
      "Iteration 165, loss = 1.72089288\n",
      "Iteration 166, loss = 1.72050551\n",
      "Iteration 167, loss = 1.72010972\n",
      "Iteration 168, loss = 1.71969912\n",
      "Iteration 169, loss = 1.71930480\n",
      "Iteration 170, loss = 1.71888889\n",
      "Iteration 171, loss = 1.71849599\n",
      "Iteration 172, loss = 1.71809230\n",
      "Iteration 173, loss = 1.71769529\n",
      "Iteration 174, loss = 1.71728555\n",
      "Iteration 175, loss = 1.71684924\n",
      "Iteration 176, loss = 1.71644839\n",
      "Iteration 177, loss = 1.71606664\n",
      "Iteration 178, loss = 1.71567993\n",
      "Iteration 179, loss = 1.71528084\n",
      "Iteration 180, loss = 1.71486885\n",
      "Iteration 181, loss = 1.71445695\n",
      "Iteration 182, loss = 1.71404855\n",
      "Iteration 183, loss = 1.71365304\n",
      "Iteration 184, loss = 1.71325065\n",
      "Iteration 185, loss = 1.71283203\n",
      "Iteration 186, loss = 1.71243879\n",
      "Iteration 187, loss = 1.71201178\n",
      "Iteration 188, loss = 1.71163029\n",
      "Iteration 189, loss = 1.71124930\n",
      "Iteration 190, loss = 1.71082570\n",
      "Iteration 191, loss = 1.71042360\n",
      "Iteration 192, loss = 1.71004288\n",
      "Iteration 193, loss = 1.70963297\n",
      "Iteration 194, loss = 1.70923419\n",
      "Iteration 195, loss = 1.70885318\n",
      "Iteration 196, loss = 1.70846049\n",
      "Iteration 197, loss = 1.70805625\n",
      "Iteration 198, loss = 1.70765318\n",
      "Iteration 199, loss = 1.70726220\n",
      "Iteration 200, loss = 1.70685598\n",
      "Iteration 1, loss = 9.60710926\n",
      "Iteration 2, loss = 1.78965890\n",
      "Iteration 3, loss = 1.50306401\n",
      "Iteration 4, loss = 1.30560225\n",
      "Iteration 5, loss = 1.23715518\n",
      "Iteration 6, loss = 1.24863721\n",
      "Iteration 7, loss = 1.25802122\n",
      "Iteration 8, loss = 1.25127028\n",
      "Iteration 9, loss = 1.24520978\n",
      "Iteration 10, loss = 1.23928488\n",
      "Iteration 11, loss = 1.24177957\n",
      "Iteration 12, loss = 1.24448480\n",
      "Iteration 13, loss = 1.23855020\n",
      "Iteration 14, loss = 1.23800527\n",
      "Iteration 15, loss = 1.23664814\n",
      "Iteration 16, loss = 1.23640741\n",
      "Iteration 17, loss = 1.23858207\n",
      "Iteration 18, loss = 1.23730925\n",
      "Iteration 19, loss = 1.23786402\n",
      "Iteration 20, loss = 1.23934997\n",
      "Iteration 21, loss = 1.23904080\n",
      "Iteration 22, loss = 1.23738579\n",
      "Iteration 23, loss = 1.24130909\n",
      "Iteration 24, loss = 1.23866877\n",
      "Iteration 25, loss = 1.24143635\n",
      "Iteration 26, loss = 1.24490100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 1.24118082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.72826243\n",
      "Iteration 2, loss = 15.72826218\n",
      "Iteration 3, loss = 15.72826194\n",
      "Iteration 4, loss = 15.72826169\n",
      "Iteration 5, loss = 15.72826145\n",
      "Iteration 6, loss = 15.72826121\n",
      "Iteration 7, loss = 15.72826097\n",
      "Iteration 8, loss = 15.72826073\n",
      "Iteration 9, loss = 15.72826049\n",
      "Iteration 10, loss = 15.72826025\n",
      "Iteration 11, loss = 15.72826002\n",
      "Iteration 12, loss = 15.72825978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.95220313\n",
      "Iteration 2, loss = 1.51783853\n",
      "Iteration 3, loss = 1.26684665\n",
      "Iteration 4, loss = 1.25779108\n",
      "Iteration 5, loss = 1.25248463\n",
      "Iteration 6, loss = 1.25225954\n",
      "Iteration 7, loss = 1.24151364\n",
      "Iteration 8, loss = 1.23789719\n",
      "Iteration 9, loss = 1.23882570\n",
      "Iteration 10, loss = 1.23761446\n",
      "Iteration 11, loss = 1.23557375\n",
      "Iteration 12, loss = 1.23924218\n",
      "Iteration 13, loss = 1.23623853\n",
      "Iteration 14, loss = 1.23987546\n",
      "Iteration 15, loss = 1.23940126\n",
      "Iteration 16, loss = 1.23674385\n",
      "Iteration 17, loss = 1.23607091\n",
      "Iteration 18, loss = 1.23852289\n",
      "Iteration 19, loss = 1.23998321\n",
      "Iteration 20, loss = 1.23939022\n",
      "Iteration 21, loss = 1.24120511\n",
      "Iteration 22, loss = 1.23798865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.46932574\n",
      "Iteration 2, loss = 15.41512997\n",
      "Iteration 3, loss = 15.36153660\n",
      "Iteration 4, loss = 15.32410098\n",
      "Iteration 5, loss = 15.29194092\n",
      "Iteration 6, loss = 15.22765788\n",
      "Iteration 7, loss = 15.18652478\n",
      "Iteration 8, loss = 15.16011315\n",
      "Iteration 9, loss = 15.10093532\n",
      "Iteration 10, loss = 15.05840177\n",
      "Iteration 11, loss = 15.02395393\n",
      "Iteration 12, loss = 14.95765085\n",
      "Iteration 13, loss = 14.91482716\n",
      "Iteration 14, loss = 14.85893497\n",
      "Iteration 15, loss = 14.81289601\n",
      "Iteration 16, loss = 14.73205733\n",
      "Iteration 17, loss = 14.67430702\n",
      "Iteration 18, loss = 14.62468818\n",
      "Iteration 19, loss = 14.55085258\n",
      "Iteration 20, loss = 14.50326282\n",
      "Iteration 21, loss = 14.41956752\n",
      "Iteration 22, loss = 14.29044375\n",
      "Iteration 23, loss = 14.21761529\n",
      "Iteration 24, loss = 14.15292604\n",
      "Iteration 25, loss = 14.08220034\n",
      "Iteration 26, loss = 14.04830342\n",
      "Iteration 27, loss = 13.99502385\n",
      "Iteration 28, loss = 13.95667405\n",
      "Iteration 29, loss = 13.91652217\n",
      "Iteration 30, loss = 13.88190232\n",
      "Iteration 31, loss = 13.82462647\n",
      "Iteration 32, loss = 13.77792861\n",
      "Iteration 33, loss = 13.73157322\n",
      "Iteration 34, loss = 13.69311913\n",
      "Iteration 35, loss = 13.65636174\n",
      "Iteration 36, loss = 13.59826366\n",
      "Iteration 37, loss = 13.54260044\n",
      "Iteration 38, loss = 13.45253020\n",
      "Iteration 39, loss = 13.38606884\n",
      "Iteration 40, loss = 13.32922728\n",
      "Iteration 41, loss = 13.23967051\n",
      "Iteration 42, loss = 13.17533862\n",
      "Iteration 43, loss = 13.08919660\n",
      "Iteration 44, loss = 12.97685225\n",
      "Iteration 45, loss = 12.85913950\n",
      "Iteration 46, loss = 12.75245549\n",
      "Iteration 47, loss = 12.61310775\n",
      "Iteration 48, loss = 12.47881015\n",
      "Iteration 49, loss = 12.32504306\n",
      "Iteration 50, loss = 12.19369375\n",
      "Iteration 51, loss = 12.02990837\n",
      "Iteration 52, loss = 11.87670230\n",
      "Iteration 53, loss = 11.69447922\n",
      "Iteration 54, loss = 11.51705273\n",
      "Iteration 55, loss = 11.37608342\n",
      "Iteration 56, loss = 11.18377131\n",
      "Iteration 57, loss = 10.98445310\n",
      "Iteration 58, loss = 10.80493918\n",
      "Iteration 59, loss = 10.57038584\n",
      "Iteration 60, loss = 10.35116142\n",
      "Iteration 61, loss = 10.15019989\n",
      "Iteration 62, loss = 9.93198252\n",
      "Iteration 63, loss = 9.67249245\n",
      "Iteration 64, loss = 9.43217376\n",
      "Iteration 65, loss = 9.19749004\n",
      "Iteration 66, loss = 8.94847927\n",
      "Iteration 67, loss = 8.70565831\n",
      "Iteration 68, loss = 8.46064522\n",
      "Iteration 69, loss = 8.20829587\n",
      "Iteration 70, loss = 7.97584101\n",
      "Iteration 71, loss = 7.75804520\n",
      "Iteration 72, loss = 7.51749796\n",
      "Iteration 73, loss = 7.31832084\n",
      "Iteration 74, loss = 7.08529716\n",
      "Iteration 75, loss = 6.90829755\n",
      "Iteration 76, loss = 6.70733341\n",
      "Iteration 77, loss = 6.58595832\n",
      "Iteration 78, loss = 6.42273838\n",
      "Iteration 79, loss = 6.29473711\n",
      "Iteration 80, loss = 6.19543112\n",
      "Iteration 81, loss = 6.09753517\n",
      "Iteration 82, loss = 6.01686651\n",
      "Iteration 83, loss = 5.95098958\n",
      "Iteration 84, loss = 5.86758237\n",
      "Iteration 85, loss = 5.79505863\n",
      "Iteration 86, loss = 5.73033626\n",
      "Iteration 87, loss = 5.66056108\n",
      "Iteration 88, loss = 5.59245038\n",
      "Iteration 89, loss = 5.52840503\n",
      "Iteration 90, loss = 5.46799353\n",
      "Iteration 91, loss = 5.39689044\n",
      "Iteration 92, loss = 5.33684011\n",
      "Iteration 93, loss = 5.26594035\n",
      "Iteration 94, loss = 5.20692587\n",
      "Iteration 95, loss = 5.14478564\n",
      "Iteration 96, loss = 5.07891798\n",
      "Iteration 97, loss = 5.01931394\n",
      "Iteration 98, loss = 4.96160547\n",
      "Iteration 99, loss = 4.89620354\n",
      "Iteration 100, loss = 4.83619639\n",
      "Iteration 101, loss = 4.77548948\n",
      "Iteration 102, loss = 4.71636249\n",
      "Iteration 103, loss = 4.65381394\n",
      "Iteration 104, loss = 4.59852029\n",
      "Iteration 105, loss = 4.53175905\n",
      "Iteration 106, loss = 4.47749242\n",
      "Iteration 107, loss = 4.41902976\n",
      "Iteration 108, loss = 4.35907918\n",
      "Iteration 109, loss = 4.30336100\n",
      "Iteration 110, loss = 4.24559909\n",
      "Iteration 111, loss = 4.19495221\n",
      "Iteration 112, loss = 4.13580131\n",
      "Iteration 113, loss = 4.08320854\n",
      "Iteration 114, loss = 4.02321814\n",
      "Iteration 115, loss = 3.97112422\n",
      "Iteration 116, loss = 3.91804156\n",
      "Iteration 117, loss = 3.86544290\n",
      "Iteration 118, loss = 3.81911884\n",
      "Iteration 119, loss = 3.77171688\n",
      "Iteration 120, loss = 3.70678929\n",
      "Iteration 121, loss = 3.65929641\n",
      "Iteration 122, loss = 3.60694525\n",
      "Iteration 123, loss = 3.55283290\n",
      "Iteration 124, loss = 3.49909652\n",
      "Iteration 125, loss = 3.44904011\n",
      "Iteration 126, loss = 3.39529352\n",
      "Iteration 127, loss = 3.34206809\n",
      "Iteration 128, loss = 3.29309053\n",
      "Iteration 129, loss = 3.24351189\n",
      "Iteration 130, loss = 3.19450843\n",
      "Iteration 131, loss = 3.14215094\n",
      "Iteration 132, loss = 3.09346875\n",
      "Iteration 133, loss = 3.04933345\n",
      "Iteration 134, loss = 3.00091457\n",
      "Iteration 135, loss = 2.95358323\n",
      "Iteration 136, loss = 2.91439171\n",
      "Iteration 137, loss = 2.86785141\n",
      "Iteration 138, loss = 2.82345913\n",
      "Iteration 139, loss = 2.77926072\n",
      "Iteration 140, loss = 2.73680314\n",
      "Iteration 141, loss = 2.69474032\n",
      "Iteration 142, loss = 2.64982544\n",
      "Iteration 143, loss = 2.60959837\n",
      "Iteration 144, loss = 2.56897973\n",
      "Iteration 145, loss = 2.53749964\n",
      "Iteration 146, loss = 2.49777630\n",
      "Iteration 147, loss = 2.46272827\n",
      "Iteration 148, loss = 2.42754309\n",
      "Iteration 149, loss = 2.38869873\n",
      "Iteration 150, loss = 2.35796625\n",
      "Iteration 151, loss = 2.32258597\n",
      "Iteration 152, loss = 2.29000257\n",
      "Iteration 153, loss = 2.26808907\n",
      "Iteration 154, loss = 2.23470171\n",
      "Iteration 155, loss = 2.20677503\n",
      "Iteration 156, loss = 2.18169436\n",
      "Iteration 157, loss = 2.15405778\n",
      "Iteration 158, loss = 2.12565740\n",
      "Iteration 159, loss = 2.10328719\n",
      "Iteration 160, loss = 2.07891846\n",
      "Iteration 161, loss = 2.06033346\n",
      "Iteration 162, loss = 2.03787492\n",
      "Iteration 163, loss = 2.01931559\n",
      "Iteration 164, loss = 2.00003976\n",
      "Iteration 165, loss = 1.98278409\n",
      "Iteration 166, loss = 1.96264671\n",
      "Iteration 167, loss = 1.95190270\n",
      "Iteration 168, loss = 1.93239422\n",
      "Iteration 169, loss = 1.91208192\n",
      "Iteration 170, loss = 1.89747459\n",
      "Iteration 171, loss = 1.88606641\n",
      "Iteration 172, loss = 1.87171057\n",
      "Iteration 173, loss = 1.85809655\n",
      "Iteration 174, loss = 1.84311842\n",
      "Iteration 175, loss = 1.83290142\n",
      "Iteration 176, loss = 1.81793724\n",
      "Iteration 177, loss = 1.80732906\n",
      "Iteration 178, loss = 1.79812858\n",
      "Iteration 179, loss = 1.78436526\n",
      "Iteration 180, loss = 1.77443069\n",
      "Iteration 181, loss = 1.76502046\n",
      "Iteration 182, loss = 1.75898064\n",
      "Iteration 183, loss = 1.74570789\n",
      "Iteration 184, loss = 1.73786333\n",
      "Iteration 185, loss = 1.73192813\n",
      "Iteration 186, loss = 1.72077916\n",
      "Iteration 187, loss = 1.71344761\n",
      "Iteration 188, loss = 1.70431765\n",
      "Iteration 189, loss = 1.69733947\n",
      "Iteration 190, loss = 1.69987725\n",
      "Iteration 191, loss = 1.68782188\n",
      "Iteration 192, loss = 1.67925675\n",
      "Iteration 193, loss = 1.66705075\n",
      "Iteration 194, loss = 1.66166915\n",
      "Iteration 195, loss = 1.65455807\n",
      "Iteration 196, loss = 1.65160785\n",
      "Iteration 197, loss = 1.64730967\n",
      "Iteration 198, loss = 1.63548231\n",
      "Iteration 199, loss = 1.63374482\n",
      "Iteration 200, loss = 1.62753676\n",
      "Iteration 1, loss = 10.88078157\n",
      "Iteration 2, loss = 2.00162690\n",
      "Iteration 3, loss = 1.46815876\n",
      "Iteration 4, loss = 1.29672967\n",
      "Iteration 5, loss = 1.25985468\n",
      "Iteration 6, loss = 1.23970937\n",
      "Iteration 7, loss = 1.24042114\n",
      "Iteration 8, loss = 1.24017204\n",
      "Iteration 9, loss = 1.23866398\n",
      "Iteration 10, loss = 1.23663837\n",
      "Iteration 11, loss = 1.23894521\n",
      "Iteration 12, loss = 1.24061898\n",
      "Iteration 13, loss = 1.24112506\n",
      "Iteration 14, loss = 1.24115390\n",
      "Iteration 15, loss = 1.23482102\n",
      "Iteration 16, loss = 1.23892594\n",
      "Iteration 17, loss = 1.23800851\n",
      "Iteration 18, loss = 1.23843943\n",
      "Iteration 19, loss = 1.23809784\n",
      "Iteration 20, loss = 1.23699907\n",
      "Iteration 21, loss = 1.23698731\n",
      "Iteration 22, loss = 1.23998162\n",
      "Iteration 23, loss = 1.24066047\n",
      "Iteration 24, loss = 1.23625388\n",
      "Iteration 25, loss = 1.23821106\n",
      "Iteration 26, loss = 1.23657860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for KeysName in hyper:\n",
    "    \n",
    "    if KeysName == 'act':\n",
    "        for value in hyper[str(KeysName)]:            \n",
    "            model = ANN( activation= value, hidden_layer_sizes=(10,5), shuffle=True, \n",
    "                batch_size=10, learning_rate='adaptive', tol= 1e-4 , verbose = True)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            param_range =  np.logspace(-4, -1, 2)\n",
    "            train_scores, test_scores = validation_curve( model,X_test,y_test , param_name=\"learning_rate_init\", param_range=param_range,\n",
    "                                                cv=10, scoring=\"accuracy\", n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5516b084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy with different learning rate')          \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cdf922f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16444/2687446806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m'Train Test Data split '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'Train Test Data split '\n",
    "\n",
    "X = df.iloc[:,2:-1]\n",
    "y = df.iloc[:,10].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state = 101, shuffle=True) \n",
    "\n",
    "'Data Normalization '\n",
    "\n",
    "\n",
    "# Normalization\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train_scaled = scale.transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "           \n",
    "            \n",
    "    \n",
    "'''        \n",
    "\n",
    "#ANN()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "from sklearn.learning_curve import validation_curve\n",
    "param_range =  np.logspace(-4, -1, 5)\n",
    "\n",
    "train_scores, test_scores = validation_curve( model,X_test,y_test , param_name=\"learning_rate_init\", param_range=param_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "lw=2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=2)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\")\n",
    "\n",
    "#plt.semilogx(param_range, test_scores_mean, label=\"Test score\",\n",
    "#             color=\"navy\", lw=lw)\n",
    "#plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                 color=\"navy\", lw=lw)\n",
    "\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy with different learning rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "data1 = data.data\n",
    "df = pd.read_csv(\"bcdata.txt\" , header=None)\n",
    "\n",
    "\n",
    "'Filling the missing data'\n",
    "\n",
    "for j in range (len(df.columns)):\n",
    "    for i in range (len(df)):\n",
    "        if df.iloc[i,j] == \"?\":\n",
    "            df.iloc[i,j] = np.int64 ( (df.iloc[i-1,j-1] + df.iloc[i+1,j+1] ) / 2 )\n",
    "            \n",
    "       # df.iloc[i,j] = np.int64(df.iloc[i,j])\n",
    "\n",
    "#plt.hist(df)\n",
    "'Train Test Data split '\n",
    "\n",
    "X = df.iloc[:,2:-1]\n",
    "y = df.iloc[:,10].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state = 101, shuffle=True) \n",
    "\n",
    "'Data Normalization '\n",
    "\n",
    "\n",
    "# Normalization\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train_scaled = scale.transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "hyper = {\n",
    "        'act' : {'relu', 'logistic', 'tanh'},\n",
    "        'lr' : {0.0001,0.001,0.01,.1},\n",
    "        'max_iter' : {200,500,1000}\n",
    "        }\n",
    "\n",
    "for KeysName in hyper:\n",
    "    \n",
    "    if KeysName == 'act':\n",
    "        for value in hyper[str(KeysName)]:            \n",
    "            model = ANN( activation= value, hidden_layer_sizes=(10,5), shuffle=True, \n",
    "                batch_size=10, learning_rate='adaptive', tol= 1e-4 , verbose = True)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            param_range =  np.logspace(-4, -1, 2)\n",
    "            train_scores, test_scores = validation_curve( model,X_test,y_test , param_name=\"learning_rate_init\", param_range=param_range,\n",
    "                                                cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "            \n",
    "        \n",
    "           \n",
    "            \n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy with different learning rate')          \n",
    "plt.show()      \n",
    "'''        \n",
    "\n",
    "#ANN()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "from sklearn.learning_curve import validation_curve\n",
    "param_range =  np.logspace(-4, -1, 5)\n",
    "\n",
    "train_scores, test_scores = validation_curve( model,X_test,y_test , param_name=\"learning_rate_init\", param_range=param_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "lw=2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=2)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\")\n",
    "\n",
    "#plt.semilogx(param_range, test_scores_mean, label=\"Test score\",\n",
    "#             color=\"navy\", lw=lw)\n",
    "#plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                 color=\"navy\", lw=lw)\n",
    "\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy with different learning rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc457e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6060aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier as ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dff06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import load_breast_cancer\n",
    "#data = load_breast_cancer()\n",
    "#data1 = data.data\n",
    "#df = pd.read_csv(\"bcdata.txt\" , header=None)\n",
    "#dataset = pd.read_csv(\"heart.csv\")\n",
    "dataset = pd.read_csv('processed.cleveland.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b5e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,2:-1]\n",
    "y = dataset.iloc[:,13].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state = 101, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c37478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "#array = dataset.values\n",
    "#X = array[:,0:13]\n",
    "#y = array[:,13]\n",
    "#X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "709ff847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train_scaled = scale.transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a1a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN( activation= 'relu', hidden_layer_sizes=(10,5), shuffle=True, \n",
    "    batch_size=10, learning_rate='adaptive', tol= 1e-4 , verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53222734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.89324642\n",
      "Iteration 2, loss = 9.41358030\n",
      "Iteration 3, loss = 7.43525603\n",
      "Iteration 4, loss = 5.75717194\n",
      "Iteration 5, loss = 4.45625165\n",
      "Iteration 6, loss = 3.86097820\n",
      "Iteration 7, loss = 3.73224991\n",
      "Iteration 8, loss = 3.40784407\n",
      "Iteration 9, loss = 3.03734892\n",
      "Iteration 10, loss = 2.69615582\n",
      "Iteration 11, loss = 2.61538873\n",
      "Iteration 12, loss = 2.29038034\n",
      "Iteration 13, loss = 2.00923910\n",
      "Iteration 14, loss = 1.87013459\n",
      "Iteration 15, loss = 1.65003826\n",
      "Iteration 16, loss = 1.49422398\n",
      "Iteration 17, loss = 1.43010109\n",
      "Iteration 18, loss = 1.34916895\n",
      "Iteration 19, loss = 1.38198461\n",
      "Iteration 20, loss = 1.27608544\n",
      "Iteration 21, loss = 1.25662499\n",
      "Iteration 22, loss = 1.23123719\n",
      "Iteration 23, loss = 1.18303841\n",
      "Iteration 24, loss = 1.20107866\n",
      "Iteration 25, loss = 1.19797849\n",
      "Iteration 26, loss = 1.21147789\n",
      "Iteration 27, loss = 1.16270266\n",
      "Iteration 28, loss = 1.15749709\n",
      "Iteration 29, loss = 1.14979018\n",
      "Iteration 30, loss = 1.15174184\n",
      "Iteration 31, loss = 1.23087777\n",
      "Iteration 32, loss = 1.17890188\n",
      "Iteration 33, loss = 1.26665961\n",
      "Iteration 34, loss = 1.27276690\n",
      "Iteration 35, loss = 1.16566959\n",
      "Iteration 36, loss = 1.17533281\n",
      "Iteration 37, loss = 1.15000829\n",
      "Iteration 38, loss = 1.20338287\n",
      "Iteration 39, loss = 1.14494139\n",
      "Iteration 40, loss = 1.11424013\n",
      "Iteration 41, loss = 1.13195971\n",
      "Iteration 42, loss = 1.11913815\n",
      "Iteration 43, loss = 1.10276915\n",
      "Iteration 44, loss = 1.09352882\n",
      "Iteration 45, loss = 1.18081753\n",
      "Iteration 46, loss = 1.27227461\n",
      "Iteration 47, loss = 1.27356221\n",
      "Iteration 48, loss = 1.09147320\n",
      "Iteration 49, loss = 1.12737341\n",
      "Iteration 50, loss = 1.10189926\n",
      "Iteration 51, loss = 1.07533860\n",
      "Iteration 52, loss = 1.08438735\n",
      "Iteration 53, loss = 1.08627626\n",
      "Iteration 54, loss = 1.08040365\n",
      "Iteration 55, loss = 1.08308321\n",
      "Iteration 56, loss = 1.12001396\n",
      "Iteration 57, loss = 1.07197309\n",
      "Iteration 58, loss = 1.07963833\n",
      "Iteration 59, loss = 1.05228665\n",
      "Iteration 60, loss = 1.04426941\n",
      "Iteration 61, loss = 1.04536265\n",
      "Iteration 62, loss = 1.06004921\n",
      "Iteration 63, loss = 1.05619705\n",
      "Iteration 64, loss = 1.04636757\n",
      "Iteration 65, loss = 1.09851139\n",
      "Iteration 66, loss = 1.04808829\n",
      "Iteration 67, loss = 1.04265726\n",
      "Iteration 68, loss = 1.02823316\n",
      "Iteration 69, loss = 1.06449717\n",
      "Iteration 70, loss = 1.10078961\n",
      "Iteration 71, loss = 1.09369267\n",
      "Iteration 72, loss = 1.04905609\n",
      "Iteration 73, loss = 1.04936982\n",
      "Iteration 74, loss = 1.06324291\n",
      "Iteration 75, loss = 1.00707695\n",
      "Iteration 76, loss = 1.06745009\n",
      "Iteration 77, loss = 1.02218016\n",
      "Iteration 78, loss = 1.01856360\n",
      "Iteration 79, loss = 1.05225114\n",
      "Iteration 80, loss = 1.06513886\n",
      "Iteration 81, loss = 0.99403654\n",
      "Iteration 82, loss = 0.99786567\n",
      "Iteration 83, loss = 1.16416849\n",
      "Iteration 84, loss = 1.13059736\n",
      "Iteration 85, loss = 1.07741306\n",
      "Iteration 86, loss = 1.07226084\n",
      "Iteration 87, loss = 1.06021706\n",
      "Iteration 88, loss = 1.02305079\n",
      "Iteration 89, loss = 1.19099466\n",
      "Iteration 90, loss = 1.04238239\n",
      "Iteration 91, loss = 0.99436721\n",
      "Iteration 92, loss = 0.98678481\n",
      "Iteration 93, loss = 0.99191913\n",
      "Iteration 94, loss = 1.07380798\n",
      "Iteration 95, loss = 1.05914123\n",
      "Iteration 96, loss = 1.02269200\n",
      "Iteration 97, loss = 1.22325147\n",
      "Iteration 98, loss = 1.07759644\n",
      "Iteration 99, loss = 0.96300283\n",
      "Iteration 100, loss = 1.00184921\n",
      "Iteration 101, loss = 1.00921984\n",
      "Iteration 102, loss = 0.98951039\n",
      "Iteration 103, loss = 1.04788714\n",
      "Iteration 104, loss = 0.96639346\n",
      "Iteration 105, loss = 1.03867457\n",
      "Iteration 106, loss = 0.99276918\n",
      "Iteration 107, loss = 1.03597276\n",
      "Iteration 108, loss = 0.98671651\n",
      "Iteration 109, loss = 0.98759020\n",
      "Iteration 110, loss = 0.95073383\n",
      "Iteration 111, loss = 0.99511134\n",
      "Iteration 112, loss = 0.94045552\n",
      "Iteration 113, loss = 1.05333678\n",
      "Iteration 114, loss = 0.99557755\n",
      "Iteration 115, loss = 0.96976527\n",
      "Iteration 116, loss = 0.99158066\n",
      "Iteration 117, loss = 1.00695988\n",
      "Iteration 118, loss = 1.03256198\n",
      "Iteration 119, loss = 0.99741757\n",
      "Iteration 120, loss = 0.97206361\n",
      "Iteration 121, loss = 1.25158912\n",
      "Iteration 122, loss = 1.07755479\n",
      "Iteration 123, loss = 0.96329645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=10, hidden_layer_sizes=(10, 5),\n",
       "              learning_rate='adaptive', verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c49f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 27.72113801\n",
      "Iteration 2, loss = 18.07393088\n",
      "Iteration 3, loss = 5.38105189\n",
      "Iteration 4, loss = 2.66078093\n",
      "Iteration 5, loss = 2.17550898\n",
      "Iteration 6, loss = 2.07845007\n",
      "Iteration 7, loss = 2.02940471\n",
      "Iteration 8, loss = 1.99927740\n",
      "Iteration 9, loss = 1.96626164\n",
      "Iteration 10, loss = 1.93666251\n",
      "Iteration 11, loss = 1.91219842\n",
      "Iteration 12, loss = 1.89055610\n",
      "Iteration 13, loss = 1.86935688\n",
      "Iteration 14, loss = 1.84710113\n",
      "Iteration 15, loss = 1.82353284\n",
      "Iteration 16, loss = 1.80011410\n",
      "Iteration 17, loss = 1.77799382\n",
      "Iteration 18, loss = 1.75376777\n",
      "Iteration 19, loss = 1.73284181\n",
      "Iteration 20, loss = 1.70995293\n",
      "Iteration 21, loss = 1.68801695\n",
      "Iteration 22, loss = 1.67074818\n",
      "Iteration 23, loss = 1.65627073\n",
      "Iteration 24, loss = 1.64285084\n",
      "Iteration 25, loss = 1.62940752\n",
      "Iteration 26, loss = 1.61611327\n",
      "Iteration 27, loss = 1.60277068\n",
      "Iteration 28, loss = 1.59036571\n",
      "Iteration 29, loss = 1.57807472\n",
      "Iteration 30, loss = 1.56644286\n",
      "Iteration 31, loss = 1.55525015\n",
      "Iteration 32, loss = 1.54431793\n",
      "Iteration 33, loss = 1.53323325\n",
      "Iteration 34, loss = 1.52230834\n",
      "Iteration 35, loss = 1.51216035\n",
      "Iteration 36, loss = 1.50253170\n",
      "Iteration 37, loss = 1.49309247\n",
      "Iteration 38, loss = 1.48408339\n",
      "Iteration 39, loss = 1.47507151\n",
      "Iteration 40, loss = 1.46640239\n",
      "Iteration 41, loss = 1.45830557\n",
      "Iteration 42, loss = 1.45054479\n",
      "Iteration 43, loss = 1.44263965\n",
      "Iteration 44, loss = 1.43524981\n",
      "Iteration 45, loss = 1.42825919\n",
      "Iteration 46, loss = 1.42133121\n",
      "Iteration 47, loss = 1.41491115\n",
      "Iteration 48, loss = 1.40346007\n",
      "Iteration 49, loss = 1.31797363\n",
      "Iteration 50, loss = 1.27894215\n",
      "Iteration 51, loss = 1.26148193\n",
      "Iteration 52, loss = 1.24005804\n",
      "Iteration 53, loss = 1.22490635\n",
      "Iteration 54, loss = 1.21248258\n",
      "Iteration 55, loss = 1.21697177\n",
      "Iteration 56, loss = 1.20479242\n",
      "Iteration 57, loss = 1.19966664\n",
      "Iteration 58, loss = 1.19943923\n",
      "Iteration 59, loss = 1.19404708\n",
      "Iteration 60, loss = 1.19560761\n",
      "Iteration 61, loss = 1.19515722\n",
      "Iteration 62, loss = 1.19567666\n",
      "Iteration 63, loss = 1.21170503\n",
      "Iteration 64, loss = 1.19315096\n",
      "Iteration 65, loss = 1.19247494\n",
      "Iteration 66, loss = 1.19004791\n",
      "Iteration 67, loss = 1.19160209\n",
      "Iteration 68, loss = 1.18756452\n",
      "Iteration 69, loss = 1.18657534\n",
      "Iteration 70, loss = 1.19067782\n",
      "Iteration 71, loss = 1.19599542\n",
      "Iteration 72, loss = 1.18738065\n",
      "Iteration 73, loss = 1.18248266\n",
      "Iteration 74, loss = 1.18419250\n",
      "Iteration 75, loss = 1.18474100\n",
      "Iteration 76, loss = 1.18352792\n",
      "Iteration 77, loss = 1.18331679\n",
      "Iteration 78, loss = 1.19057643\n",
      "Iteration 79, loss = 1.18064000\n",
      "Iteration 80, loss = 1.18412910\n",
      "Iteration 81, loss = 1.17755640\n",
      "Iteration 82, loss = 1.17822604\n",
      "Iteration 83, loss = 1.17834096\n",
      "Iteration 84, loss = 1.17657296\n",
      "Iteration 85, loss = 1.17597212\n",
      "Iteration 86, loss = 1.17413013\n",
      "Iteration 87, loss = 1.17771632\n",
      "Iteration 88, loss = 1.17499035\n",
      "Iteration 89, loss = 1.17418338\n",
      "Iteration 90, loss = 1.17351379\n",
      "Iteration 91, loss = 1.17559277\n",
      "Iteration 92, loss = 1.16895843\n",
      "Iteration 93, loss = 1.16893342\n",
      "Iteration 94, loss = 1.16782570\n",
      "Iteration 95, loss = 1.16561180\n",
      "Iteration 96, loss = 1.16679489\n",
      "Iteration 97, loss = 1.16958261\n",
      "Iteration 98, loss = 1.16850788\n",
      "Iteration 99, loss = 1.16768176\n",
      "Iteration 100, loss = 1.16265107\n",
      "Iteration 101, loss = 1.16097214\n",
      "Iteration 102, loss = 1.16008310\n",
      "Iteration 103, loss = 1.17943807\n",
      "Iteration 104, loss = 1.16653439\n",
      "Iteration 105, loss = 1.16388751\n",
      "Iteration 106, loss = 1.15643493\n",
      "Iteration 107, loss = 1.15708067\n",
      "Iteration 108, loss = 1.15963060\n",
      "Iteration 109, loss = 1.15449737\n",
      "Iteration 110, loss = 1.15615223\n",
      "Iteration 111, loss = 1.15373035\n",
      "Iteration 112, loss = 1.15511963\n",
      "Iteration 113, loss = 1.17030429\n",
      "Iteration 114, loss = 1.15721078\n",
      "Iteration 115, loss = 1.15265350\n",
      "Iteration 116, loss = 1.15513872\n",
      "Iteration 117, loss = 1.15105673\n",
      "Iteration 118, loss = 1.14627796\n",
      "Iteration 119, loss = 1.14376906\n",
      "Iteration 120, loss = 1.15082811\n",
      "Iteration 121, loss = 1.14612937\n",
      "Iteration 122, loss = 1.16267283\n",
      "Iteration 123, loss = 1.14217359\n",
      "Iteration 124, loss = 1.14171467\n",
      "Iteration 125, loss = 1.14204832\n",
      "Iteration 126, loss = 1.14483591\n",
      "Iteration 127, loss = 1.13578246\n",
      "Iteration 128, loss = 1.14198112\n",
      "Iteration 129, loss = 1.14703100\n",
      "Iteration 130, loss = 1.16457887\n",
      "Iteration 131, loss = 1.13060083\n",
      "Iteration 132, loss = 1.13192370\n",
      "Iteration 133, loss = 1.12792850\n",
      "Iteration 134, loss = 1.12856702\n",
      "Iteration 135, loss = 1.13048133\n",
      "Iteration 136, loss = 1.14339851\n",
      "Iteration 137, loss = 1.12544385\n",
      "Iteration 138, loss = 1.13972612\n",
      "Iteration 139, loss = 1.12156098\n",
      "Iteration 140, loss = 1.12115538\n",
      "Iteration 141, loss = 1.12079377\n",
      "Iteration 142, loss = 1.11993432\n",
      "Iteration 143, loss = 1.12094693\n",
      "Iteration 144, loss = 1.11998875\n",
      "Iteration 145, loss = 1.11708132\n",
      "Iteration 146, loss = 1.11145251\n",
      "Iteration 147, loss = 1.12207517\n",
      "Iteration 148, loss = 1.10985723\n",
      "Iteration 149, loss = 1.10675051\n",
      "Iteration 150, loss = 1.10529643\n",
      "Iteration 151, loss = 1.10442885\n",
      "Iteration 152, loss = 1.10325225\n",
      "Iteration 153, loss = 1.10038561\n",
      "Iteration 154, loss = 1.10600819\n",
      "Iteration 155, loss = 1.10622428\n",
      "Iteration 156, loss = 1.10747271\n",
      "Iteration 157, loss = 1.09764720\n",
      "Iteration 158, loss = 1.08810334\n",
      "Iteration 159, loss = 1.08850997\n",
      "Iteration 160, loss = 1.08531089\n",
      "Iteration 161, loss = 1.08999945\n",
      "Iteration 162, loss = 1.07482150\n",
      "Iteration 163, loss = 1.09156932\n",
      "Iteration 164, loss = 1.07614888\n",
      "Iteration 165, loss = 1.07346652\n",
      "Iteration 166, loss = 1.08488845\n",
      "Iteration 167, loss = 1.07781630\n",
      "Iteration 168, loss = 1.06809293\n",
      "Iteration 169, loss = 1.06683910\n",
      "Iteration 170, loss = 1.07518539\n",
      "Iteration 171, loss = 1.06844786\n",
      "Iteration 172, loss = 1.06021603\n",
      "Iteration 173, loss = 1.06275934\n",
      "Iteration 174, loss = 1.05339201\n",
      "Iteration 175, loss = 1.05109922\n",
      "Iteration 176, loss = 1.05627117\n",
      "Iteration 177, loss = 1.06024297\n",
      "Iteration 178, loss = 1.04651605\n",
      "Iteration 179, loss = 1.04156247\n",
      "Iteration 180, loss = 1.04099719\n",
      "Iteration 181, loss = 1.04213972\n",
      "Iteration 182, loss = 1.04033407\n",
      "Iteration 183, loss = 1.04748561\n",
      "Iteration 184, loss = 1.03872955\n",
      "Iteration 185, loss = 1.03716018\n",
      "Iteration 186, loss = 1.02940533\n",
      "Iteration 187, loss = 1.04627407\n",
      "Iteration 188, loss = 1.02919229\n",
      "Iteration 189, loss = 1.01835654\n",
      "Iteration 190, loss = 1.02577625\n",
      "Iteration 191, loss = 1.01505190\n",
      "Iteration 192, loss = 1.01493773\n",
      "Iteration 193, loss = 1.02285790\n",
      "Iteration 194, loss = 1.00805530\n",
      "Iteration 195, loss = 1.00341763\n",
      "Iteration 196, loss = 0.99848208\n",
      "Iteration 197, loss = 1.00086198\n",
      "Iteration 198, loss = 0.99787385\n",
      "Iteration 199, loss = 0.99461610\n",
      "Iteration 200, loss = 0.99511858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#ANN()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece39a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  0  1  0]\n",
      " [10  0  0  2  0]\n",
      " [ 5  0  0  5  0]\n",
      " [ 1  0  0  5  0]\n",
      " [ 1  0  0  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.97      0.77        31\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.36      0.83      0.50         6\n",
      "           4       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.20      0.36      0.25        61\n",
      "weighted avg       0.36      0.57      0.44        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\myneee101\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d73b4052",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'errors_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16444/4140070812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0merrors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'errors_'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "plt.plot(range(1, len(model.errors_) + 1), model. errors_, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42b371eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.47234117\n",
      "Iteration 2, loss = 2.18219491\n",
      "Iteration 3, loss = 1.87221596\n",
      "Iteration 4, loss = 1.82314378\n",
      "Iteration 5, loss = 1.79578875\n",
      "Iteration 6, loss = 1.77222816\n",
      "Iteration 7, loss = 1.75379311\n",
      "Iteration 8, loss = 1.73583275\n",
      "Iteration 9, loss = 1.72017866\n",
      "Iteration 10, loss = 1.70454042\n",
      "Iteration 11, loss = 1.68951701\n",
      "Iteration 12, loss = 1.67491882\n",
      "Iteration 13, loss = 1.66071993\n",
      "Iteration 14, loss = 1.64702378\n",
      "Iteration 15, loss = 1.63333298\n",
      "Iteration 16, loss = 1.62014398\n",
      "Iteration 17, loss = 1.60711187\n",
      "Iteration 18, loss = 1.59418466\n",
      "Iteration 19, loss = 1.58184381\n",
      "Iteration 20, loss = 1.56972725\n",
      "Iteration 21, loss = 1.55818874\n",
      "Iteration 22, loss = 1.54667692\n",
      "Iteration 23, loss = 1.53571355\n",
      "Iteration 24, loss = 1.52495687\n",
      "Iteration 25, loss = 1.51488899\n",
      "Iteration 26, loss = 1.50509316\n",
      "Iteration 27, loss = 1.49573933\n",
      "Iteration 28, loss = 1.48686664\n",
      "Iteration 29, loss = 1.47798743\n",
      "Iteration 30, loss = 1.46941980\n",
      "Iteration 31, loss = 1.46093446\n",
      "Iteration 32, loss = 1.45295234\n",
      "Iteration 33, loss = 1.44500998\n",
      "Iteration 34, loss = 1.43756042\n",
      "Iteration 35, loss = 1.43012429\n",
      "Iteration 36, loss = 1.42295855\n",
      "Iteration 37, loss = 1.41595983\n",
      "Iteration 38, loss = 1.40964909\n",
      "Iteration 39, loss = 1.40309006\n",
      "Iteration 40, loss = 1.39704072\n",
      "Iteration 41, loss = 1.39122996\n",
      "Iteration 42, loss = 1.38559048\n",
      "Iteration 43, loss = 1.38041413\n",
      "Iteration 44, loss = 1.37519770\n",
      "Iteration 45, loss = 1.37017881\n",
      "Iteration 46, loss = 1.36559143\n",
      "Iteration 47, loss = 1.36083688\n",
      "Iteration 48, loss = 1.35656523\n",
      "Iteration 49, loss = 1.35203764\n",
      "Iteration 50, loss = 1.34792618\n",
      "Iteration 51, loss = 1.34373964\n",
      "Iteration 52, loss = 1.33977647\n",
      "Iteration 53, loss = 1.33569804\n",
      "Iteration 54, loss = 1.33225690\n",
      "Iteration 55, loss = 1.32846542\n",
      "Iteration 56, loss = 1.32528652\n",
      "Iteration 57, loss = 1.32309819\n",
      "Iteration 58, loss = 1.31945896\n",
      "Iteration 59, loss = 1.31686358\n",
      "Iteration 60, loss = 1.31450525\n",
      "Iteration 61, loss = 1.31139419\n",
      "Iteration 62, loss = 1.30886356\n",
      "Iteration 63, loss = 1.30693619\n",
      "Iteration 64, loss = 1.30375859\n",
      "Iteration 65, loss = 1.30491478\n",
      "Iteration 66, loss = 1.29959479\n",
      "Iteration 67, loss = 1.29930725\n",
      "Iteration 68, loss = 1.29601973\n",
      "Iteration 69, loss = 1.29420813\n",
      "Iteration 70, loss = 1.29260180\n",
      "Iteration 71, loss = 1.29114936\n",
      "Iteration 72, loss = 1.28939681\n",
      "Iteration 73, loss = 1.28772953\n",
      "Iteration 74, loss = 1.28692546\n",
      "Iteration 75, loss = 1.28462146\n",
      "Iteration 76, loss = 1.28395442\n",
      "Iteration 77, loss = 1.28216144\n",
      "Iteration 78, loss = 1.28304245\n",
      "Iteration 79, loss = 1.27932528\n",
      "Iteration 80, loss = 1.28160830\n",
      "Iteration 81, loss = 1.27699316\n",
      "Iteration 82, loss = 1.27570424\n",
      "Iteration 83, loss = 1.27758419\n",
      "Iteration 84, loss = 1.27644105\n",
      "Iteration 85, loss = 1.27629955\n",
      "Iteration 86, loss = 1.27452080\n",
      "Iteration 87, loss = 1.27336416\n",
      "Iteration 88, loss = 1.27275072\n",
      "Iteration 89, loss = 1.27140788\n",
      "Iteration 90, loss = 1.26973673\n",
      "Iteration 91, loss = 1.26919463\n",
      "Iteration 92, loss = 1.27187171\n",
      "Iteration 93, loss = 1.27549562\n",
      "Iteration 94, loss = 1.27406004\n",
      "Iteration 95, loss = 1.27344752\n",
      "Iteration 96, loss = 1.27281680\n",
      "Iteration 97, loss = 1.27233892\n",
      "Iteration 98, loss = 1.27186254\n",
      "Iteration 99, loss = 1.27144846\n",
      "Iteration 100, loss = 1.27096810\n",
      "Iteration 101, loss = 1.27047466\n",
      "Iteration 102, loss = 1.27011397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.56904392\n",
      "Iteration 2, loss = 15.61009227\n",
      "Iteration 3, loss = 14.18831920\n",
      "Iteration 4, loss = 11.71859360\n",
      "Iteration 5, loss = 9.30222985\n",
      "Iteration 6, loss = 7.33096749\n",
      "Iteration 7, loss = 5.92922636\n",
      "Iteration 8, loss = 4.83577751\n",
      "Iteration 9, loss = 3.92745660\n",
      "Iteration 10, loss = 3.23669903\n",
      "Iteration 11, loss = 2.76522175\n",
      "Iteration 12, loss = 2.42823087\n",
      "Iteration 13, loss = 2.17458944\n",
      "Iteration 14, loss = 1.97758031\n",
      "Iteration 15, loss = 1.81262196\n",
      "Iteration 16, loss = 1.69173356\n",
      "Iteration 17, loss = 1.60645510\n",
      "Iteration 18, loss = 1.54046483\n",
      "Iteration 19, loss = 1.50181811\n",
      "Iteration 20, loss = 1.47656684\n",
      "Iteration 21, loss = 1.46537572\n",
      "Iteration 22, loss = 1.45141014\n",
      "Iteration 23, loss = 1.44183678\n",
      "Iteration 24, loss = 1.43430361\n",
      "Iteration 25, loss = 1.42434235\n",
      "Iteration 26, loss = 1.40404578\n",
      "Iteration 27, loss = 1.39652739\n",
      "Iteration 28, loss = 1.38889259\n",
      "Iteration 29, loss = 1.38038756\n",
      "Iteration 30, loss = 1.37087898\n",
      "Iteration 31, loss = 1.37005049\n",
      "Iteration 32, loss = 1.35775735\n",
      "Iteration 33, loss = 1.35473591\n",
      "Iteration 34, loss = 1.34822960\n",
      "Iteration 35, loss = 1.34598857\n",
      "Iteration 36, loss = 1.34099651\n",
      "Iteration 37, loss = 1.33797069\n",
      "Iteration 38, loss = 1.33121833\n",
      "Iteration 39, loss = 1.32618585\n",
      "Iteration 40, loss = 1.32462811\n",
      "Iteration 41, loss = 1.31984849\n",
      "Iteration 42, loss = 1.31663677\n",
      "Iteration 43, loss = 1.31479570\n",
      "Iteration 44, loss = 1.31900647\n",
      "Iteration 45, loss = 1.32451116\n",
      "Iteration 46, loss = 1.30850911\n",
      "Iteration 47, loss = 1.30824797\n",
      "Iteration 48, loss = 1.30326369\n",
      "Iteration 49, loss = 1.30253158\n",
      "Iteration 50, loss = 1.29671960\n",
      "Iteration 51, loss = 1.30335988\n",
      "Iteration 52, loss = 1.29631992\n",
      "Iteration 53, loss = 1.29521598\n",
      "Iteration 54, loss = 1.29426093\n",
      "Iteration 55, loss = 1.29043828\n",
      "Iteration 56, loss = 1.28692154\n",
      "Iteration 57, loss = 1.29153523\n",
      "Iteration 58, loss = 1.28515240\n",
      "Iteration 59, loss = 1.28715728\n",
      "Iteration 60, loss = 1.28249300\n",
      "Iteration 61, loss = 1.28479754\n",
      "Iteration 62, loss = 1.28113155\n",
      "Iteration 63, loss = 1.27929252\n",
      "Iteration 64, loss = 1.27941221\n",
      "Iteration 65, loss = 1.27714216\n",
      "Iteration 66, loss = 1.27504806\n",
      "Iteration 67, loss = 1.27656052\n",
      "Iteration 68, loss = 1.27548177\n",
      "Iteration 69, loss = 1.27380491\n",
      "Iteration 70, loss = 1.27335356\n",
      "Iteration 71, loss = 1.27544941\n",
      "Iteration 72, loss = 1.27372312\n",
      "Iteration 73, loss = 1.27279167\n",
      "Iteration 74, loss = 1.27120121\n",
      "Iteration 75, loss = 1.27021622\n",
      "Iteration 76, loss = 1.26850727\n",
      "Iteration 77, loss = 1.26655505\n",
      "Iteration 78, loss = 1.26758911\n",
      "Iteration 79, loss = 1.26675206\n",
      "Iteration 80, loss = 1.26504899\n",
      "Iteration 81, loss = 1.26486385\n",
      "Iteration 82, loss = 1.26626418\n",
      "Iteration 83, loss = 1.26558249\n",
      "Iteration 84, loss = 1.26075619\n",
      "Iteration 85, loss = 1.26152632\n",
      "Iteration 86, loss = 1.26257949\n",
      "Iteration 87, loss = 1.26036097\n",
      "Iteration 88, loss = 1.26228484\n",
      "Iteration 89, loss = 1.25875568\n",
      "Iteration 90, loss = 1.25964496\n",
      "Iteration 91, loss = 1.25912745\n",
      "Iteration 92, loss = 1.26049291\n",
      "Iteration 93, loss = 1.25807271\n",
      "Iteration 94, loss = 1.25772674\n",
      "Iteration 95, loss = 1.25599419\n",
      "Iteration 96, loss = 1.25777045\n",
      "Iteration 97, loss = 1.25687404\n",
      "Iteration 98, loss = 1.25498853\n",
      "Iteration 99, loss = 1.25479425\n",
      "Iteration 100, loss = 1.25466625\n",
      "Iteration 101, loss = 1.25520907\n",
      "Iteration 102, loss = 1.25330371\n",
      "Iteration 103, loss = 1.25132004\n",
      "Iteration 104, loss = 1.25242194\n",
      "Iteration 105, loss = 1.25208334\n",
      "Iteration 106, loss = 1.25280236\n",
      "Iteration 107, loss = 1.25246860\n",
      "Iteration 108, loss = 1.25046332\n",
      "Iteration 109, loss = 1.25151291\n",
      "Iteration 110, loss = 1.25419319\n",
      "Iteration 111, loss = 1.25075460\n",
      "Iteration 112, loss = 1.25108445\n",
      "Iteration 113, loss = 1.24979258\n",
      "Iteration 114, loss = 1.26039292\n",
      "Iteration 115, loss = 1.25870974\n",
      "Iteration 116, loss = 1.25845330\n",
      "Iteration 117, loss = 1.25303932\n",
      "Iteration 118, loss = 1.24905698\n",
      "Iteration 119, loss = 1.24918422\n",
      "Iteration 120, loss = 1.24659702\n",
      "Iteration 121, loss = 1.24927412\n",
      "Iteration 122, loss = 1.24796169\n",
      "Iteration 123, loss = 1.24637618\n",
      "Iteration 124, loss = 1.24962866\n",
      "Iteration 125, loss = 1.24690194\n",
      "Iteration 126, loss = 1.24933555\n",
      "Iteration 127, loss = 1.24666429\n",
      "Iteration 128, loss = 1.24803583\n",
      "Iteration 129, loss = 1.24690504\n",
      "Iteration 130, loss = 1.24518178\n",
      "Iteration 131, loss = 1.24679002\n",
      "Iteration 132, loss = 1.24537091\n",
      "Iteration 133, loss = 1.24893734\n",
      "Iteration 134, loss = 1.24511361\n",
      "Iteration 135, loss = 1.24499379\n",
      "Iteration 136, loss = 1.24710858\n",
      "Iteration 137, loss = 1.24550543\n",
      "Iteration 138, loss = 1.24513680\n",
      "Iteration 139, loss = 1.25346629\n",
      "Iteration 140, loss = 1.25429977\n",
      "Iteration 141, loss = 1.24580225\n",
      "Iteration 142, loss = 1.24251973\n",
      "Iteration 143, loss = 1.24599842\n",
      "Iteration 144, loss = 1.24348805\n",
      "Iteration 145, loss = 1.24532916\n",
      "Iteration 146, loss = 1.24396671\n",
      "Iteration 147, loss = 1.24293930\n",
      "Iteration 148, loss = 1.24862745\n",
      "Iteration 149, loss = 1.24585111\n",
      "Iteration 150, loss = 1.24380459\n",
      "Iteration 151, loss = 1.24153886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 152, loss = 1.24384318\n",
      "Iteration 153, loss = 1.24290704\n",
      "Iteration 154, loss = 1.24490320\n",
      "Iteration 155, loss = 1.24280933\n",
      "Iteration 156, loss = 1.24424016\n",
      "Iteration 157, loss = 1.24221090\n",
      "Iteration 158, loss = 1.24593336\n",
      "Iteration 159, loss = 1.24364884\n",
      "Iteration 160, loss = 1.24389356\n",
      "Iteration 161, loss = 1.24599922\n",
      "Iteration 162, loss = 1.24122977\n",
      "Iteration 163, loss = 1.24231958\n",
      "Iteration 164, loss = 1.24818990\n",
      "Iteration 165, loss = 1.24668422\n",
      "Iteration 166, loss = 1.24181810\n",
      "Iteration 167, loss = 1.24156921\n",
      "Iteration 168, loss = 1.24357687\n",
      "Iteration 169, loss = 1.24333225\n",
      "Iteration 170, loss = 1.24713348\n",
      "Iteration 171, loss = 1.24122911\n",
      "Iteration 172, loss = 1.24215226\n",
      "Iteration 173, loss = 1.24253792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16444/2654428808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#ANN()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m   1036\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass_fast\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \"\"\"\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Initialize first layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'age'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier as ANN\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"processed.cleveland.csv\" , header=None)\n",
    "\n",
    "\n",
    "\n",
    "#plt.hist(df)\n",
    "'Train Test Data split '\n",
    "array = data.values\n",
    "X = array[:,0:13]\n",
    "y = array[:,13]\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "\n",
    "#X = data.iloc[:,2:-1]\n",
    "#y = df.iloc[:,13].values\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state = 101, shuffle=True) \n",
    "\n",
    "'Data Normalization '\n",
    "\n",
    "\n",
    "# Normalization\n",
    "#scale = StandardScaler()\n",
    "#scale.fit(X_train)\n",
    "#X_train_scaled = scale.transform(X_train)\n",
    "#X_test_scaled = scale.transform(X_test)\n",
    "\n",
    "          \n",
    "model = ANN( activation= 'relu', hidden_layer_sizes=(10,5), shuffle=True, \n",
    "    batch_size=10, learning_rate='adaptive', tol= 1e-4 , verbose = True)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "            \n",
    "    \n",
    "\n",
    "#ANN()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "plt.plot(range(1, len(model.errors_) + 1), model. errors_, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6736b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
